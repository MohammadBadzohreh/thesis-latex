\chapter{پیشینه پژوهش}


\section*{تحلیل شبکه‌های ترانسفورمر و کانولوشن در پردازش تصاویر}

در شبکه‌های ترانسفورمر، تصاویری که وارد شبکه می‌شوند، به پچ‌هایی با ابعاد مشخص تقسیم می‌شوند (مثل \(8 \times 8\) یا \(16 \times 16\) پیکسل). این پچ‌ها به عنوان ورودی به مدل داده می‌شوند، و در طی پردازش، مدل به طور عمومی این پچ‌ها را به صورت غیرمحلی \footnote{\lr{Global}} و مستقل از مکان‌هایشان در تصویر پردازش می‌کند.


با این حال، در مدل‌های کانولوشن، ویژگی‌های محلی \footnote{\lr{Local Features}} می‌توانند به راحتی شناسایی شوند چون شبکه به طور طبیعی در داخل تصویر حرکت کرده و ویژگی‌های اطراف یک نقطه خاص را تحلیل می‌کند. این ویژگی‌های محلی مثل لبه‌ها، بافت‌ها و اشیاء می‌توانند شناسایی شوند، زیرا هر فیلتر در یک ناحیه محلی از تصویر اعمال می‌شود و اطلاعات محلی را از آن ناحیه استخراج می‌کند.


اما در ترانسفورمرها، چون تصویر به پچ‌های ثابت تقسیم می‌شود و سپس این پچ‌ها به مدل وارد می‌شوند، دید محلی مدل محدود می‌شود. یعنی مدل نمی‌تواند به راحتی ویژگی‌های محلی تصویر را مانند یک شبکه کانولوشنی شناسایی کند. به عبارت دیگر، مدل برای بررسی ارتباطات و ویژگی‌ها فقط با توجه به پچ‌های جداگانه و بدون آگاهی از ساختار کلی تصویر، عمل می‌کند.

در عین حال، ترانسفورمرها به دلیل ساختار توجه خود می‌توانند به روابط کلی  نیز توجه داشته باشند. یعنی تمام پچ‌ها می‌توانند به هم متصل شوند و اطلاعاتی از نقاط دورتر تصویر را دریافت کنند. این ویژگی باعث می‌شود که مدل توانایی پردازش اطلاعات جهانی و تطبیق آن با سایر بخش‌های تصویر را داشته باشد.

اما این موضوع که ترانسفورمر نمی‌تواند به طور طبیعی دید محلی داشته باشد، به این معنی است که برخی از اطلاعات مفیدی که برای تحلیل دقیق تصاویر ضروری است، ممکن است از دست برود یا با مشکل مواجه شود. برای رفع این مشکل، معمولاً روش‌هایی مثل استفاده از لایه‌های کانولوشن در کنار ترانسفورمرها یا تقسیم‌بندی بهتر پچ‌ها به کار می‌رود تا شبکه قادر باشد هم دید محلی و هم دید گلوبال را به طور همزمان در اختیار داشته باشد.

\subsection{ویژگی‌های محلی}
در شبکه‌های کانولوشن، فیلترهای کانولوشنی \footnote{\lr{Convolutional Filters}} برای استخراج ویژگی‌های محلی طراحی شده‌اند. این فیلترها معمولاً روی نواحی کوچک تصویر (مانند \(3 \times 3\) یا \(5 \times 5\) پیکسل) اعمال می‌شوند. فرض کنید تصویری از یک گربه دارید؛ در لایه‌های ابتدایی یک کانولوشن، این فیلترها ممکن است لبه‌ها \footnote{\lr{Edges}}، گوشه‌ها \footnote{\lr{Corners}}، یا بافت‌های کوچک \footnote{\lr{Textures}} در موهای گربه را شناسایی کنند. این پردازش محلی است زیرا هر فیلتر فقط روی ناحیه کوچکی از تصویر تمرکز می‌کند.


\subsection{ویژگی‌های جهانی}
با عمیق‌تر شدن شبکه و افزایش تعداد لایه‌ها، خروجی لایه‌های ابتدایی (ویژگی‌های محلی) به ویژگی‌های بزرگ‌تر و پیچیده‌تر ترکیب می‌شوند. این فرآیند با استفاده از عملیات‌هایی مثل ادغام و فیلترهای بزرگ‌تر انجام می‌شود. برای مثال، پس از چند لایه، کانولوشن ممکن است به جای گوشه‌های گربه، ساختار کل گوش گربه را شناسایی کند. در لایه‌های عمیق‌تر، کانولوشن می‌تواند کل شکل گربه یا حتی دسته‌بندی نهایی (مانند اینکه این یک گربه است) را انجام دهد. این پردازش جهانی است زیرا کل تصویر را برای استنباط ویژگی‌های پیچیده در نظر می‌گیرد.

\subsection{ترانسفورمرها و محدودیت‌های دید محلی}
در ترانسفورمرها، ورودی تصویر به پچ‌های ثابت (مانند $16 \times 16$) تقسیم می‌شود و هر پچ به طور مستقل پردازش می‌شود، بدون آنکه ارتباطات بین پیکسل‌های داخل پچ یا بین پچ‌ها به‌صورت محلی در نظر گرفته شود. به عنوان یک مثال مشکل، اگر یک چشم گربه در مرز دو پچ جدا شود، مدل ممکن است این ارتباط محلی بین دو پچ را درک نکند و ویژگی چشم گربه از دست برود. 

در ترانسفورمرها، ارتباطات بین پچ‌ها با استفاده از مکانیزم \lr{Self-Attention} محاسبه می‌شود که به مدل اجازه می‌دهد ارتباطات گلوبال بین تمام پچ‌ها را بررسی کند، اما اغلب ویژگی‌های محلی نهفته در هر پچ نادیده گرفته می‌شوند.

برای حل مشکل دید local , global  ترانسفورمر ها، چند روش را پیاده کرده ایم 

\subsection{روش اول:}

\subsection{تبدیل تصاویر به دو پچ مجزا:}

فرض کنید تصویری با اندازهٔ $224 \times 224$ پیکسل داریم که اندازه‌ای متداول در دیتاست‌هایی نظیر ImageNet است. این تصویر به دو صورت مختلف به پچ‌هایی با اندازه‌های متفاوت تقسیم می‌شود. 

در روش اول، تصویر به بلوک‌هایی با ابعاد $8 \times 8$ پیکسل تقسیم می‌شود. در این حالت، تعداد پچ‌ها در هر ردیف و ستون به ترتیب برابر با  $28$ است و در مجموع 
\[
28 \times 28 \;=\; 784
\]
پچ از تصویر استخراج می‌شود. هر پچ شامل $8 \times 8$ پیکسل است و اگر تصویر دارای سه کانال رنگی باشد (مانند تصاویر RGB)، هر پچ شامل 
\[
8 \times 8 \times 3 \;=\; 192
\]
مقدار عددی خواهد بود. این پچ‌ها پس از تبدیل به بردار، به عنوان ورودی به یکی از مسیرهای پردازشی در ترانسفورمر وارد می‌شوند.

ویک بار دیگر همان تصویر به بلوک‌هایی با ابعاد $16 \times 16$ پیکسل تقسیم می‌شود. در این حالت، تعداد پچ‌ها در هر ردیف و ستون به ترتیب 
$14$ است و در مجموع
\[
14 \times 14 \;=\; 196
\]
پچ ایجاد می‌شود. هر پچ $16 \times 16$ پیکسل را شامل می‌شود و در صورت RGB بودن تصویر، هر پچ دارای
\[
16 \times 16 \times 3 \;=\; 768
\]
مقدار عددی خواهد بود. این پچ‌ها نیز به بردار تبدیل شده و به مسیر پردازشی جداگانه‌ای در ترانسفورمر وارد می‌شوند.

بنابراین در همان ابتدا ما دو تا لایه موازی را در ترانسفورمر پیش میگیریم
یکی با دید جزئی و یکی هم با دیدگاه جهانی و در ادامه این دید های جزئی و جهانی را با یک دیگر ترکیب میکنیم اما قبل آن باید یک سری کار ها برای انجام این کار صورت گیرد.

\subsection{هماهنگ سازی پچ ها:}

در این مرحله، هدف آن است که پس از لایه‌های اولیهٔ ترانسفورمر (یا هر مرحله‌ای که پچ‌های $8 \times 8$ و $16 \times 16$ جاسازی اولیه شده‌اند)، تعداد و ترتیب پچ‌های هر دو مسیر را هماهنگ کنیم تا امکان ادغام (ترکیب) آن‌ها در لایه‌های بعدی فراهم شود. دو عمل مهم در این بخش اتفاق می‌افتد:
\begin{enumerate}
	\item تکرار (Replication) پچ‌های $16 \times 16$ به تعداد ۴ برابر
	\item تغییر ترتیب (Re-Order) پچ‌های $8 \times 8$
\end{enumerate}

این دو گام باعث می‌شوند در نهایت، هر دو مجموعهٔ پچ، دارای ۷۸۴ ردیف (پچ) باشند و ردیف‌های متقابل در هر دو مجموعه، به ناحیهٔ فضایی یکسانی از تصویر اصلی اشاره کنند. در ادامه، هر یک از این مراحل را با جزئیات بیشتری توضیح می‌دهیم.

\section*{تکرار ۴برابری پچ‌های \texorpdfstring{$16 \times 16$}{16x16}}

چرا باید تعداد پچ‌های $16 \times 16$ را ۴ برابر کنیم؟

اگر تصویر ورودی $224 \times 224$ باشد، پچ‌های $16 \times 16$ در هر بعد 
\[
\frac{224}{16} = 14
\]
قطعه تولید می‌کنند و بنابراین در کل، 
\[
14 \times 14 = 196
\]
پچ خواهیم داشت. در مقابل، پچ‌های $8 \times 8$ به‌خاطر نصف‌بودن ضلع پچ ($8$ به‌جای $16$)، تعداد قطعات در هر بعد دو برابر می‌شود:
\[
\frac{224}{8} = 28.
\]
پس تعداد کل پچ‌ها 
\[
28 \times 28 = 784
\]
خواهد بود. واضح است که
\[
784 = 4 \times 196.
\]
یعنی پچ‌های $8 \times 8$ چهار برابر بیشتر از پچ‌های $16 \times 16$ هستند. 

چون قصد داریم در گامی بعدی (مثلاً یک لایه انکودر مشترک) این دو مجموعهٔ پچ را ادغام یا مقایسه کنیم، باید تعداد پچ‌های هر دو مسیر یکسان باشد.

\section*{مکانیزم تکرار}

برای هم‌اندازه‌کردن این دو مجموعه، هر پچ $16 \times 16$ را دقیقاً چهار بار کپی می‌کنیم. به صورت ریاضی، اگر 
\[
X^{(16\times16)} \in \mathbb{R}^{196 \times D}
\]
ماتریسی در ابعاد $196 \times D$ باشد (یعنی ۱۹۶ پچ، هر کدام برداری با بعد $D$)، عمل تکرار به شکل زیر نوشته می‌شود:
\[
\tilde{X}^{(16\times16)} 
= 
\underbrace{ \bigl[X^{(16\times16)},\, X^{(16\times16)},\, X^{(16\times16)},\, X^{(16\times16)}\bigr] }_{\text{تکرار ۴ مرتبه}}
\, \in\, \mathbb{R}^{784 \times D}.
\]
عملگر $[\cdot]$ در این‌جا به معنای الحاق (Concatenate) در راستای بُعد اول (تعداد پچ‌ها) است. در نتیجه، ۴ نسخهٔ یکسان از $X^{(16\times16)}$ پشت سر هم قرار می‌گیرند و ابعاد نهایی به $784 \times D$ می‌رسد.

از نظر مفهومی، چنین برداشتی وجود دارد که هریک از پچ‌های $16 \times 16$، وقتی روی تصویر اصلی نگاه کنیم، با چهار منطقهٔ کوچک‌تر $8 \times 8$ هم‌پوشانی دارد (چون $16\times16$ ازلحاظ مساحت ۴ برابر $8\times8$ است). اما فعلاً صرفاً از نظر تعداد، آن را ۴ مرتبه تکرار می‌کنیم؛ بعداً در مرحلهٔ «تغییر ترتیب» توضیح می‌دهیم که چگونه می‌توان این تکرار را به بخش‌های تصویر ربط داد.


\section*{تغییر ترتیب (Re-Order) پچ‌های $8 \times 8$}

اکنون که پچ‌های $16 \times 16$ به‌صورت ۴برابر تکرار شده و به ۷۸۴ پچ رسیده‌اند، می‌خواهیم پچ‌های $8 \times 8$ را نیز به شکلی بازآرایی کنیم که هر گروه ۴تایی از پچ‌های $8 \times 8$ دقیقاً متناظر با یک پچ $16 \times 16$ باشد. این متناظر بودن از نظر موقعیت مکانی در تصویر اهمیت دارد.

\subsubsection*{چرا بازآرایی (Re-Order) لازم است؟}
در استخراج اولیهٔ پچ‌های $8 \times 8$، معمولاً طبق یک ترتیب خطی (مانند \textit{Row-Major}) از گوشهٔ بالا-چپ تصویر تا گوشهٔ پایین-راست حرکت می‌کنیم و پچ‌ها را شماره‌گذاری می‌کنیم (۱، ۲، ۳، ... ۷۸۴). در این شماره‌گذاری عادی، پچ‌های ۱، ۲، ۳ و ۴ لزوماً در کنار هم قرار دارند، اما این هم‌جواری ممکن است دقیقاً با پچ اولِ $16 \times 16$ منطبق نباشد.

برای مثال، ممکن است پچ ۱ در $8 \times 8$ با پیکسل‌های ردیف ۰ تا ۷ و ستون ۰ تا ۷ هم‌پوشانی داشته باشد، درحالی‌که پچ ۲ در $8 \times 8$ مربوط به ردیف ۰ تا ۷ و ستون ۸ تا ۱۵ است. اگر بگوییم پچ اولِ $16 \times 16$ (که کل ناحیهٔ صفر تا ۱۵ در سطر و صفر تا ۱۵ در ستون را می‌پوشاند) با ۴ پچ $8 \times 8$ متناظر است، لازم است به‌درستی تشخیص دهیم که آن ۴ پچ در کدام شماره‌های ۱ تا ۷۸۴ قرار گرفته‌اند. برای مثال (در یک چینش فرضی):
\begin{itemize}
	\item پچ‌های (۱، ۲، ۲۹، ۳۰) از میان $8 \times 8$ احتمالاً چهار بخش کوچکی هستند که روی‌هم پیکسل‌های سطر ۰..۱۵ و ستون ۰..۱۵ را می‌پوشانند. پس این ۴ پچ باهم معادل پچ اولِ $16 \times 16$ هستند.
	\item پچ دومِ $16 \times 16$ ممکن است با پچ‌های (۳، ۴، ۳۱، ۳۲) در $8 \times 8$ هم‌پوشانی داشته باشد، و به همین شکل ادامه می‌یابد.
\end{itemize}
بنابراین برای اینکه «ردیف اول تکرارشدهٔ پچ $16 \times 16$» با «۴ ردیف درست از پچ‌های $8 \times 8$» روبه‌رو شود، باید ترتیب پچ‌های $8 \times 8$ دقیقاً طبق این نقشهٔ فضایی بازآرایی (Re-Order) شود.

\subsubsection*{تابع ReOrder}
به‌صورت ریاضی، می‌توان این بازآرایی را به شکل یک تابع
\(\mathrm{ReOrder}(\cdot)\)
نشان داد. اگر 
\[
X^{(8\times8)} \in \mathbb{R}^{784 \times D}
\]
ماتریسی با ابعاد $784 \times D$ باشد (شماره‌گذاری ردیفی عادی)، خروجی زیر را خواهیم داشت:
\[
\hat{X}^{(8\times8)} \;=\; \mathrm{ReOrder}\bigl(X^{(8\times8)}\bigr)\;\in\;\mathbb{R}^{784 \times D}.
\]
وظیفهٔ \(\mathrm{ReOrder}\) آن است که ردیف‌های 
\(X^{(8\times8)}\)
را طوری جابه‌جا کند که ۴ ردیف پشت‌سرهم در 
\(\hat{X}^{(8\times8)}\)
دقیقاً همان چهار بخشی از تصویر باشند که یک پچ خاص $16 \times 16$ (در حالت تکرارشده) روی آن قرار دارد. به‌عبارت دیگر، از ۱، ۲، ۳، ۴ در چینش عادی، ممکن است تبدیل به ۱، ۲، ۲۹، ۳۰ شود (اگر چنین ترتیبی در صفحهٔ تصویر باهم منطبق است).


\subsection{\lr{Positional Embedding}}

در این مرحله که هماهنگ‌سازی پچ‌ها (\textit{Patch Alignment}) به اتمام رسیده و هر دو مجموعهٔ پچ (مسیر $8 \times 8$ و مسیر $16 \times 16$ تکرارشده) دارای ابعاد یکسان \((784 \times D)\) و ترتیب متناظر هستند، می‌توان جاسازی مکانی (\textit{Positional Embedding}) را اعمال کرد. هدف از افزودن \textit{Positional Embedding} آن است که مدل بتواند جایگاه هر پچ در تصویر اصلی را درک کند و صرفاً با بردارهای ویژگی انتزاعی مواجه نباشد.

اغلب در مدل‌های ترانسفورمر بینایی، برای هر پچ (صرف‌نظر از اندازه‌اش) یک بردار مکان (\textit{Position Embedding}) پیش‌بینی می‌شود که در همان ابتدای مسیر با بردار ویژگی پچ جمع می‌گردد. اما در رویکرد فعلی، چون ما ابتدا لازم داشتیم پچ‌های $16 \times 16$ را تکرار کنیم و پچ‌های $8 \times 8$ را تغییر ترتیب بدهیم، بهتر است پس از این بازآرایی، \textit{Positional Embedding} را به‌گونه‌ای اعمال کنیم که دقیقاً منعکس‌کنندهٔ جایگاه نهایی هر پچ در ترتیب هماهنگ‌شده باشد. 

در غیر این صورت، اگر قبل از هماهنگی، \textit{Positional Embedding} اعمال شده بود، تکرار و جابه‌جایی پچ‌ها ممکن است ساختار مکان‌یابی آن‌ها را به‌هم بریزد یا نیاز به به‌روزرسانی مجدد \textit{Position Embedding} باشد.

از آنجا که هر دو مجموعهٔ پچ ($8 \times 8$ و $16 \times 16$ تکرارشده) پس از هماهنگ‌سازی در ابعاد 
\(\mathbb{R}^{784 \times D}\)
هستند، ماتریس جاسازی مکانی (\(E_{\text{pos}}\)) نیز باید ۷۸۴ سطر داشته باشد. در نتیجه:
\[
E_{\text{pos}} \in \mathbb{R}^{784 \times D},
\]
که در آن هر سطر از \(E_{\text{pos}}\) مختص یک پچ (ردیف) در خروجی مرحلهٔ هماهنگ‌سازی است.

در این روش، تنها از یک مجموعهٔ \(E_{\text{pos}}\) مشترک برای هر دو نوع پچ استفاده می‌شود. جون هر دو ابعاد یکسان هستند و positional embedding  پارامتر یادگیرنده ندارد میتوان از یک positional embedding  استفاده کرد.

\subsection{لایه های اول تا هشتم انکودر}

پس از آن‌که \textit{Positional Embedding} به پچ‌های هر دو مسیر اعمال شد، عملاً هر دو مجموعهٔ خروجی دارای شکل و ابعاد یکسان \((784 \times D)\) هستند (در این مرحله فرض گرفته‌ایم \textit{CLS token} وجود نداشته باشد یا در محاسبات فعلی نادیده گرفته شود). این امر باعث می‌شود که در هر دو مسیر، \textit{Query}، \textit{Key} و \textit{Value} در مکانیزم \textit{Self-Attention} نیز ابعاد یکسانی داشته باشند.

\subsection{لایه نهم انکودر}

در لایهٔ نهم، ابتدا \textit{Query} و \textit{Key} را برای هر مسیر به‌صورت جداگانه محاسبه می‌کنیم. طبق روال استاندارد ترانسفورمر، هر ورودی با ماتریس‌های وزنیِ یادگیری‌پذیر (\(W_Q\) و \(W_K\)) ضرب می‌شود تا به فضاهای \(Q\) و \(K\) نگاشت شود. بسته به طراحی، می‌توان از همان وزن‌ها یا وزن‌های جداگانه استفاده کرد؛ اما برای سادگی، فرض کنیم وزن‌ها مشترک هستند:
\[
Q^{(8)} = X^{(8)} W_Q, \quad K^{(8)} = X^{(8)} W_K,
\]
\[
Q^{(16)} = X^{(16)} W_Q, \quad K^{(16)} = X^{(16)} W_K.
\]

هرکدام از \(Q^{(8)}\) و \(Q^{(16)}\) ابعادی معادل \(\mathbb{R}^{784 \times d_k}\) دارند (\(d_k\) معمولاً \(\frac{D}{h}\) در صورت چندسری بودن \textit{Attention} است، یا ممکن است با \(D\) برابر باشد در صورت تک‌سری).

به‌طور مشابه \(K^{(8)}\) و \(K^{(16)}\) نیز ابعادی معادل \(\mathbb{R}^{784 \times d_k}\) دارند.

\subsection{محاسبهٔ ماتریس شباهت (\(QK^T\)) و میانگین‌گیری}

مکانیزم خودتوجهی (\textit{Self-Attention}) معمولاً از ضرب \(Q\) در \(K^T\) برای محاسبهٔ میزان شباهت پچ‌ها استفاده می‌کند. شما می‌خواهید قبل از \textit{Softmax}، میانگین شباهت‌های دو مسیر را بگیرید. بنابراین به این ترتیب عمل می‌کنیم:

\subsection*{شباهت مسیر $8 \times 8$:}
\[
S^{(8)} = Q^{(8)} K^{(8)^T} \; \in \; \mathbb{R}^{784 \times 784}
\]

\subsection*{شباهت مسیر $16 \times 16$:}
\[
S^{(16)} = Q^{(16)} K^{(16)^T} \; \in \; \mathbb{R}^{784 \times 784}
\]

\subsection*{ادغام شباهت‌ها:}
سپس برای ادغام این دو شباهت، از میانگین‌گیری استفاده می‌کنیم:
\[
S_{\text{merged}} = \frac{S^{(8)} + S^{(16)}}{2}.
\]

در هر دوی \(S^{(8)}\) و \(S^{(16)}\) ابعاد \(\mathbb{R}^{784 \times 784}\) دارند و بنابراین جمع‌کردن و میانگین‌گیری آن‌ها بدون مشکل صورت می‌گیرد.

\subsection{اعمال مقیاس‌بندی \( \frac{1}{\sqrt{d_k}} \) و  \textit{Softmax}}

در نسخهٔ کلاسیک \textit{Attention}، ماتریس شباهت \( QK^T \) معمولاً با ضریب \( \frac{1}{\sqrt{d_k}} \) مقیاس (Scaling) می‌شود تا مقادیر بزرگ در ماتریس شباهت کنترل شوند و یادگیری پایدارتر شود:
\[
\tilde{S}_{\text{merged}} = \frac{1}{\sqrt{d_k}} S_{\text{merged}} = \frac{1}{\sqrt{d_k}} \cdot \frac{S^{(8)} + S^{(16)}}{2} = \frac{S^{(8)} + S^{(16)}}{2 \sqrt{d_k}}.
\]

در گام بعدی، باید بر روی هر سطر این ماتریس \(\tilde{S}_{\text{merged}}\) عمل \textit{Softmax} انجام دهیم تا ضرایب توجه (\(A\)) به دست آید:
\[
A = \text{softmax}(\tilde{S}_{\text{merged}}) \; \in \; \mathbb{R}^{784 \times 784}.
\]

به‌عبارت دیگر، برای هر عنصر \(A_{ij}\):
\[
A_{ij} = \frac{\exp(\tilde{S}_{\text{merged}, ij})}{\sum_{k=1}^{784} \exp(\tilde{S}_{\text{merged}, ik})}
\]

این ماتریس \(A\) نشان‌دهندهٔ وزن‌های توجه بین هر دو پچ است. در اینجا:
\begin{itemize}
	\item سطرها نشان‌دهندهٔ پچ‌های \textit{Query} هستند.
	\item ستون‌ها نشان‌دهندهٔ پچ‌های \textit{Key} هستند.
\end{itemize}

در این مرحله، خروجی نهایی مکانیزم توجه (\textit{Attention}) تنها بر اساس بردارهای \(V\) مربوط به پچ‌های \(8 \times 8\) تولید می‌شود. در معماری استاندارد ترانسفورمر، پس از محاسبهٔ نقشهٔ توجه \(\text{softmax}(QK^T)\)، نتیجه در بردارهای ارزش (\(V\)) ضرب می‌گردد تا بردار نهایی توجه به دست آید:
\[
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V.
\]

اما ما در اینجا از بردارهای \(V\) صرفاً متعلق به مسیر پچ‌های \(8 \times 8\) استفاده می‌شود. برای این منظور، ابتدا با استفاده از ماتریس وزنی \(W_V\)، بردار ارزش \(V^{(8)}\) را از \(X^{(8)}\) (بردار ویژگی پچ‌های \(8 \times 8\)) استخراج می‌کنیم:
\[
V^{(8)} = X^{(8)} W_V \; \in \; \mathbb{R}^{784 \times d_v},
\]
که در آن \(W_V\) یک ماتریس یادگیری‌پذیر با ابعاد \(D \times d_v\) است. 

پس از آن‌که نقشهٔ توجه نهایی \(A\) (محاسبه‌شده بر پایهٔ ترکیب میانگین‌شده از شباهت‌های مربوط به مسیرهای \(8 \times 8\) و \(16 \times 16\)) شکل گرفت، خروجی مکانیزم توجه (\(O_{\text{Attention}}\)) با ضرب \(A\) در \(V^{(8)}\) حاصل می‌شود:
\[
O_{\text{Attention}} = A V^{(8)} = \text{softmax} \left( \frac{S^{(8)} + S^{(16)}}{2 \sqrt{d_k}} \right) V^{(8)},
\]
که در آن:
\[
A = \text{softmax} \left( \frac{S^{(8)} + S^{(16)}}{2 \sqrt{d_k}} \right),
\]
و:
\[
S^{(8)} = Q^{(8)} K^{(8)^T}, \quad S^{(16)} = Q^{(16)} K^{(16)^T}.
\]

با جای‌گذاری کامل، فرمول زیر به‌دست می‌آید:
\[
O_{\text{Attention}} = \text{softmax} \left( \frac{1}{2 \sqrt{d_k}} \left( Q^{(8)} K^{(8)^T} + Q^{(16)} K^{(16)^T} \right) \right) V^{(8)}.
\]

\section*{مزیت این رویکرد}

به این ترتیب:
\begin{itemize}
	\item وزن‌های توجه (\(A\)) از ترکیب میانگین‌شدهٔ شباهت‌های دو مسیر (اطلاعات محلی از پچ‌های کوچک و اطلاعات کلی از پچ‌های بزرگ) به دست می‌آید.
	\item بردار ارزش (\(V\)) تنها از مسیر پچ‌های \(8 \times 8\) استخراج می‌شود.
\end{itemize}

این روش باعث می‌شود ویژگی‌های محلی (که در \(V^{(8)}\) متمرکز هستند) مستقیماً در خروجی نهایی منعکس شوند، اما وزن‌دهی به این ویژگی‌ها تحت تأثیر هر دو نما (محلی و کلی) انجام گیرد. به بیان دیگر، با وجود آن‌که اطلاعات ارزش از مسیر ریزدانه انتخاب می‌شود، مکانیزم توجه نقشهٔ شباهت خود را از ادغام دو مقیاس به دست می‌آورد. این رویکرد می‌تواند توازنی مطلوب میان جزئی‌نگری و شناخت ساختار وسیع‌تر تصویر برقرار سازد.

\subsection{ادغام وزنی}



به‌جای آنکه شباهت‌های مربوط به پچ‌های \(8 \times 8\) و \(16 \times 16\) را به شکل مساوی (ضریب \(\frac{1}{2}\)) با هم جمع کنیم، می‌توان از پارامتری یادگیری‌پذیر (\textit{trainable parameter}) به‌نام \(\alpha\) استفاده کرد که در بازهٔ \([0, 1]\) قرار دارد. فرمول ترکیب ماتریس شباهت‌ها (\(S^{(8)}\) و \(S^{(16)}\)) به‌شکل زیر تغییر می‌کند:
\[
S_{\text{merged}} = \alpha S^{(8)} + (1 - \alpha) S^{(16)}.
\]

\subsection*{تأثیر مقدار \(\alpha\) در مدل}
\begin{itemize}
	\item اگر \(\alpha\) به‌سمت ۱ متمایل شود، نقش پچ‌های \(8 \times 8\) در توجه مدل پررنگ‌تر خواهد شد.
	\item اگر \(\alpha\) کوچک باشد، توجه بیشتری به پچ‌های \(16 \times 16\) اختصاص داده می‌شود.
\end{itemize}

با استفاده از پارامتر \(\alpha\) انعطاف پذیری مدل به صورت قابل توجه ای افزایش پیدا میکند.

\subsection*{آموزش پارامتر \(\alpha\)}
خود مدل در فرایند آموزش با پس‌انتشار خطا (\textit{Back-Propagation}) می‌تواند مقدار بهینهٔ \(\alpha\) را بیاموزد. این انعطاف‌پذیری به مدل اجازه می‌دهد تا به‌طور خودکار تعادلی میان اطلاعات جزئی (از پچ‌های کوچک‌تر) و اطلاعات کلی (از پچ‌های بزرگ‌تر) برقرار کند.


\section{روش دوم down sampling: }


در  down sampling  پس از چند مرحله پردازش (بلوک‌های ترنسفورمر)، حجم توکن‌های پچ را به شکل مؤثری کاهش دهیم تا هم هزینه‌ی محاسباتی (به‌ویژه در عملیات توجه چندسری) کمتر شود و هم مدل به‌تدریج بتواند از حالت توجه به جزییات ریز (ویژگی‌های محلی) به نمایی کلی‌تر از تصویر (ویژگی‌های کلی) برسد.

در این مدل ما cls token  را تا انتها دست نخورده باقی میگذاریم.

پس از عبور تصویر از مرحله‌ی پَچ‌گذاری (Patch Embedding) و الحاق توکن \texttt{[CLS]}، تعداد توکن‌ها در ابتدا به‌صورت:
\begin{equation}
	N = 1 + 784 = 785
\end{equation}
در نظر گرفته می‌شود. در این رابطه، عدد $784$ بیانگر تقسیم‌بندی یک تصویر $224 \times 224$ به پچ‌های $8 \times 8$ (یعنی $28 \times 28 = 784$ پچ) و عدد $1$ توکن ویژه‌ی \texttt{[CLS]} است. همچنین بعدِ ویژگی هر توکن (شامل پچ‌ها و \texttt{[CLS]}) برابر با $768$ خواهد بود. ازاین‌رو، شکل ورودی در شروع کار:
\begin{equation}
	(B, 785, 768)
\end{equation}
است که $B$ اندازه‌ی بچ (Batch Size) محسوب می‌شود.

در حالت نرمال در ترانسفورمرها اندازه ورودی اتنشن ها در بلوک های انکودر تغییری نمیکنند.


روش دانسمپلینگ جفتی بر این ایده استوار است که توکن \texttt{[CLS]} (ابتدای توالی) دست‌نخورده باقی بمانَد و سایر توکن‌ها (نماینده‌ی پچ‌ها) را دوتادوتا با هم میانگین بگیریم. اگر ورودی ما 
\begin{equation}
	(B, N, 768)
\end{equation}
باشد و در آن $N - 1$ توکن پچ وجود داشته باشد، آنگاه با جفت‌کردن و میانگین‌گیری پچ‌ها، تعداد نهایی از رابطه‌ی
\begin{equation}
	N_{new} = 1 + \frac{N - 1}{2} = \frac{N + 1}{2}
\end{equation}
به‌دست می‌آید. توکن \texttt{[CLS]} همچنان در موقعیت اول باقی می‌مانَد و ابعاد ویژگی (یعنی $768$) تغییری نمی‌کند.

بنابراین، پس از بلوک سوم که همچنان ورودی $(B, 785, 768)$ دارد و خروجی همان $(B, 785, 768)$ را تولید می‌کند، با اجرای دانسمپلینگ جفتی تعداد توکن‌ها از $785$ به $1 + \frac{784}{2} = 393$ کاهش می‌یابد؛ درنتیجه ورودی بلوک بعدی $(B, 393, 768)$ خواهد بود.

 
و همین کار پس از بلوک انکودر ششم و نهم اعمال می شود.  و همینطور ورودی انکودر بعدی کم و کم تر میشود.

به‌طور خلاصه، ابعاد ورودی هر بلوک پس از آنکه دانسمپلینگ در انتهای بلوک‌های ۳، ۶ و ۹ اعمال شود، بدین ترتیب تغییر می‌کند:

\[
(B, 785, 768) 
\xrightarrow{\text{بلوک 3 + دانسمپلینگ}} 
(B, 393, 768),
\]

\[
(B, 393, 768) 
\xrightarrow{\text{بلوک 6 + دانسمپلینگ}} 
(B, 197, 768),
\]

\[
(B, 197, 768) 
\xrightarrow{\text{بلوک 9 + دانسمپلینگ}} 
(B, 99, 768).
\]



\subsection{حرکت تدریجی از جزئیات به کلیت}

در لایه‌های اولیه، وقتی هنوز دانسمپلینگ انجام نشده، مدل همۀ پچ‌های ریز و «اطلاعات محلی» را در اختیار دارد و مسسسی‌تواند ویژگی‌های ظریف را پردازش کند. اما وقتی چند لایه گذشت و وارد بلوک‌های بالاتر شدیم، با اعمال میانگین‌گیری جفتی، اطلاعات هر دو پچ مجاور در یک بردار ادغام می‌شود. این وضعیت را می‌توان شکل‌گیری نوعی «نمای کلی‌تر» از تصویر دانست؛ زیرا به‌جای ۲ پچ مجزا، حالا یک پچ ترکیبی داریم که اطلاعاتشان را در خود گنجانده است. به این ترتیب، مدل در سطوح بالاتر روی ویژگی‌های انتزاعی‌تر یا خلاصه‌تر متمرکز می‌شود و نیازی نیست همچنان هزینه‌ی نگه‌داشتن تمام جزئیات محلی را بپردازد.

\subsection{کم نشدن پارامتر ها در این مدل}


در معماری‌های ترنسفورمر، پارامترهای قابل یادگیری تنها به شکل وزن‌ها و بایاس‌هایی تعریف می‌شوند که یا در مرحله‌ی تعبیه‌سازی (Embedding)، یا در بخش توجه چندسری، یا در شبکه‌های MLP هر بلوک ترنسفورمر به‌کار می‌روند. نکته‌ی کلیدی این است که ابعاد اغلب این وزن‌ها و بایاس‌ها تنها به بُعد پنهان (Hidden Dimension) یا نرخ گسترش (Expansion Ratio) وابسته است و تغییری در آن‌ها به تناسب کم یا زیاد شدن تعداد توکن‌های ورودی به وجود نمی‌آید.

\paragraph{لایه‌های تعبیه‌ساز و موقعیتی (Positional Embedding)}
ابتدا در این بخش، پارامترها به‌صورت ماتریس‌ها یا بردارهایی تعریف می‌شوند که بُعد آن‌ها با بُعد پنهان (مثلاً ۷۶۸) تنظیم می‌شود و همچنین به طول حداکثری توالی وابستگی دارد (مثلاً اگر حداکثر تعداد توکن ۷۸۵ در نظر گرفته شود، در همان ابتدای تعریف پارامتر شکل می‌گیرد). در هر صورت، این پارامترها فارغ از آن که در عمل چند توکن مؤثر باقی بماند، ثابت خواهند ماند.

\paragraph{توجه چندسری (Multi-Head Attention)}
در این بخش، ماتریس‌های $W_Q$, $W_K$, $W_V$ و $W_O$ موجودند که ابعادشان همگی تابعی از بُعد ویژگی $D$ هستند؛ برای مثال اگر $D = 768$ باشد، هر کدام از این ماتریس‌ها در ابعاد ثابتی (مانند $768 \times 768$) تعریف شده و یاد گرفته می‌شود. این پارامترها به «تعداد توکن» بستگی ندارند؛ بلکه تعیین می‌کنند چگونه هر توکن در فضای ویژگی (Feature Space) نگاشت یا پردازش شود.

\paragraph{شبکه‌های MLP داخل هر بلوک ترنسفورمر}
در این بخش، وزن‌ها و بایاس‌ها به‌صورت $W_1, b_1$ و $W_2, b_2$ تعریف می‌شوند. ابعاد این لایه‌ها عموماً از قانون 
\[
D \to 4D \to D
\]
پیروی می‌کند (اگر نسبت گسترش ۴ باشد). در نتیجه، شکل $W_1$ و $W_2$ هم تنها وابسته به $D$ است. تعداد کم یا زیاد شدن توکن‌ها (مثلاً نصف‌شدن توکن‌ها پس از دانسمپلینگ) صرفاً بر روی تعداد محاسبات اثر می‌گذارد، اما ساختار و تعداد این پارامترها را دگرگون نمی‌کند.


در یک لایۀ ترنسفورمر، مهم‌ترین قسمت از نظر هزینه، محاسبۀ Self-Attention است. اگر تعداد توکن‌ها را $N_{\text{tokens}}$ بنامیم و فرض کنیم بعد بردار ویژگی‌ها $D$ باشد، آنگاه مهم‌ترین بخش محاسباتی مربوط به ضرب ماتریسی $QK^\top$ است که پیچیدگی زمانی \( O(N_{\text{tokens}}^2 \cdot D) \) دارد.


وقتی دانسمپلینگ جفتی انجام می‌دهیم و تعداد توکن‌ها را از $N_{\text{tokens}}$ به حدود $\frac{N_{\text{tokens}}}{2}$ کاهش می‌دهیم، پیچیدگی زمانی در لایه‌های بعدی به شکل زیر تغییر می‌کند:

\[
O(N_{\text{tokens}}^2 \cdot D) \quad \longrightarrow \quad O\left(\left(\frac{N_{\text{tokens}}}{2}\right)^2 \cdot D\right) \longrightarrow  O\left(\frac{N_{\text{tokens}}^2}{4} \cdot D\right).
\]

و به این ترتیب حجم محاسبات به شکل چشم گیری کم میشود.






