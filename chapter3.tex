\chapter{پیشینه پژوهش}



\subsection*{استفاده از روش‌های تانسوری در شبکه‌های عصبی چندلایه}

در سال‌های اخیر، استفاده از روش‌های تانسوری به‌عنوان روشی نوین در بهینه‌سازی معماری‌های شبکه‌های عصبی، به‌ویژه در مدل‌هایی که تعداد پارامترهای آن‌ها بسیار زیاد است، مانند شبکه‌های عصبی چندلایه \footnote{\lr{Mlp}}، توجه بسیاری را به خود جلب کرده است. تانسورها تعمیمی از ماتریس‌ها به ابعاد بالاتر هستند و به‌طور طبیعی برای نمایش داده‌های چندبعدی همچون تصاویر، ویدیوها یا سری‌های زمانی چندکاناله مناسب‌اند. بهره‌گیری از ساختار تانسوری در معماری شبکه، این امکان را فراهم می‌سازد که بدون نیاز به فشرده‌سازی اولیه (مانند صاف کردن \footnote{\lr{flatten}} کردن ورودی)، اطلاعات ساختاری میان ابعاد مختلف حفظ شده و مدل بتواند از روابط درون‌تعاملی موجود میان این ابعاد بهره‌برداری نماید.

در معماری سنتی شبکه عصبی چند لایه، نگاشت از ورودی \lr{$\mathbf{x} \in \mathbb{R}^n$} به خروجی \lr{$\mathbf{y} \in \mathbb{R}^m$} به‌وسیله ضرب ماتریسی انجام می‌گیرد:

\[
\mathbf{y} = W\mathbf{x} + \mathbf{b}
\]

که در آن \lr{$W \in \mathbb{R}^{m \times n}$} ماتریس وزن و \lr{$\mathbf{b}$} بردار بایاس است. 




\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.8\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/mlp.png}
		\caption{\lr{Mlp}}
		\label{fig:Mlp}
	\end{minipage}
	\hfill
\end{figure}





در مقابل، در روش‌های تانسوری، وزن‌ها به‌صورت یک تانسور مرتبه بالاتر مدل‌سازی می‌شوند و نگاشت ورودی به خروجی با استفاده از ضرب‌های چندحالته (ضرب تانسوری در ابعاد مختلف) صورت می‌پذیرد:

\[
\mathbf{y} = \mathcal{W} \times_1 \mathbf{x}_1 \times_2 \mathbf{x}_2 \times_3 \cdots + \mathbf{b}
\]

در این رابطه، \lr{$\mathcal{W}$} یک تانسور وزن است و عملگر \lr{$\times_n$} نشان‌دهنده ضرب تانسوری در بعد $n$‌ام می‌باشد.

روش‌هایی نظیر \lr{TCL} \footnote{tensor contraction layer} و \lr{TRL} \footnote{\lr{Tensor regression layer}} به‌عنوان نمونه‌هایی از این رویکرد، با بهره‌گیری از تکنیک‌های تجزیه تانسوری همچون \lr{Tucker} یا تجزیه \lr{CP}، نه تنها باعث کاهش چشمگیر در تعداد پارامترها می‌شوند، بلکه ساختار چندبعدی داده‌ها را نیز حفظ می‌نمایند. این ویژگی به‌خصوص در مسائل دارای ورودی‌های دارای ساختار فضایی یا زمانی قابل‌توجه، بسیار حائز اهمیت است.


\subsection*{مزایای استفاده از روش‌های تانسوری}

استفاده از روش‌های تانسوری در شبکه‌های عصبی چندلایه مزایای متعددی به همراه دارد که برخی از مهم‌ترین آن‌ها عبارت‌اند از:

\begin{itemize}
	\item \textbf{کاهش چشمگیر تعداد پارامترها:} با بهره‌گیری از فشرده‌سازی تانسوری، می‌توان ابعاد تانسور وزن‌ها را به‌گونه‌ای کاهش داد که بدون افت محسوس در عملکرد مدل، مصرف حافظه و پیچیدگی محاسباتی به‌طور قابل توجهی کاهش یابد.
	
	\item \textbf{حفظ ساختار داده‌های ورودی:} برخلاف روش‌های سنتی که در آن‌ها داده‌ها پیش از ورود به لایه‌های چگال \footnote{\lr{Dense}} باید مسطح‌سازی \footnote{\lr{Flatten}} شوند، استفاده از ساختار تانسوری این امکان را فراهم می‌کند که ساختار فضایی، زمانی یا کانالی داده‌ها حفظ شده و ارتباط میان ابعاد مختلف ورودی بهتر درک و پردازش شود.
	
	
	
\end{itemize}


\subsection*{محدودیت‌ها و چالش‌ها}

با وجود مزایای متعدد، بهره‌گیری از روش‌های تانسوری در معماری‌های شبکه‌های عصبی با چالش‌ها و محدودیت‌هایی نیز همراه است که در ادامه به برخی از مهم‌ترین آن‌ها اشاره می‌شود:

\begin{itemize}
	\item \textbf{پیچیدگی بالاتر در پیاده‌سازی:} پیاده‌سازی لایه‌های مبتنی بر عملیات تانسوری معمولاً به ابزارها و کتابخانه‌های خاصی همچون \lr{Tensorly} یا \lr{Tensor Toolbox} نیاز دارد. این موضوع فرآیند طراحی و توسعه مدل را پیچیده‌تر از استفاده از لایه‌های استاندارد مانند شبکه عصبی چند لایه  یا  می‌سازد.
	
	\item \textbf{بهینه‌سازی دشوارتر:} فرآیند آموزش مدل‌های تانسوری می‌تواند نسبت به مدل‌های معمولی کندتر باشد. الگوریتم‌های مبتنی بر گرادیان ممکن است در فضای پارامتری تانسورها با سطوح خطای غیرهموار یا چندوجهی مواجه شوند که روند همگرایی را دشوار می‌کند.
	
	\item \textbf{احتمال کاهش دقت در فشرده‌سازی شدید:} در صورتی‌که میزان فشرده‌سازی تانسورها بیش از حد بالا باشد، مدل ممکن است توانایی لازم برای نمایش روابط غیرخطی و الگوهای پیچیده را از دست داده و در نتیجه، دقت نهایی پیش‌بینی کاهش یابد.
\end{itemize}



\subsection{لایه فشرده‌سازی تانسوری}


\lr{Tensor Contraction Layer}

در بسیاری از مدل‌های یادگیری عمیق، به‌ویژه در شبکه‌های عصبی پیچیشی \footnote{\lr{convolution}}، فعال‌سازی‌های لایه‌های میانی به‌صورت تانسورهایی با مرتبه بالا ظاهر می‌شوند. به‌طور سنتی، برای اعمال لایه‌های کاملا متصل \footnote{\lr{Fully Connected}}، ابتدا این تانسورها با عملیات مسطح سازی \footnote{\lr{ّflatting}} به بردار تبدیل شده و سپس به فضای خروجی نگاشت داده می‌شوند. این فرآیند هرچند رایج است، اما موجب از بین رفتن ساختار چندخطی \footnote{\lr{multilinear structure}} داده می‌شود و همچنین منجر به افزایش شدید تعداد پارامترهای شبکه می‌گردد.

برای مقابله با این چالش، لایه‌ای با عنوان \textbf{لایه فشرده‌سازی تانسوری} یا به اختصار \lr{TCL} معرفی شده است. این لایه بدون نیاز به مسطح سازی کردن تانسور ورودی، ابعاد آن را در هر \footnote{\lr{mode}} کاهش داده و در عین حال ساختار تانسوری داده را حفظ می‌کند.


\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/tcl.png}
		\caption{\lr{tensor contraction layer}}
		\label{fig:tensor_contraction_layer}
	\end{minipage}
	\hfill
\end{figure}




\subsubsection*{فرمول‌بندی ریاضی}

فرض شود تانسور فعال‌سازی ورودی به TCL به‌صورت زیر تعریف شده باشد:

\[
\mathcal{X} \in \mathbb{R}^{S \times I_0 \times I_1 \times \cdots \times I_N}
\]

که در آن $S$ اندازه‌ی دسته‌ی آموزشی و $I_0, I_1, \dots, I_N$ ابعاد تانسور هستند (برای مثال عرض، ارتفاع و کانال‌های رنگی یک تصویر).

هدف لایه TCL، کاهش هر یک از ابعاد $I_k$ به $R_k$ با استفاده از ماتریس‌های فشرده‌سازی قابل آموزش به فرم زیر است:

\[
V^{(k)} \in \mathbb{R}^{R_k \times I_k}, \quad \text{برای } k = 0, 1, \dots, N
\]

عملیات فشرده‌سازی نهایی با ضرب‌های چندحالته \footnote{\lr{n-mode product}} به‌صورت زیر انجام می‌شود:

\[
\mathcal{X}' = \mathcal{X} \times_1 V^{(0)} \times_2 V^{(1)} \cdots \times_{N+1} V^{(N)}
\]

که در آن $\times_n$ نشان‌دهنده‌ی ضرب تانسور در مد $n$ام است. خروجی لایه TCL، یعنی $\mathcal{X}'$، یک تانسور فشرده‌شده با ابعاد زیر خواهد بود:

\[
\mathcal{X}' \in \mathbb{R}^{S \times R_0 \times R_1 \times \cdots \times R_N}
\]

بدین ترتیب، به‌جای مسطج سازی و از بین رفتن ساختار چندبعدی داده، عملیات فشرده‌سازی در هر مد به‌صورت مستقل و ساختارمند انجام می‌شود.







\subsubsection*{تحلیل تعداد پارامترها}

استفاده از TCL منجر به کاهش قابل توجه در تعداد پارامترهای مدل می‌شود. تعداد پارامترهای موردنیاز برای این لایه برابر است با:

\[
\text{تعداد پارامترهای TCL} = \sum_{k=0}^{N} I_k \cdot R_k
\]

در حالی که در یک لایه کاملا متصل، پس از مسطج سازی کردن ورودی، تعداد پارامترها برابر خواهد بود با:

\[
\text{تعدا پارامتر های FC} = \left( \prod_{k=0}^{N} I_k \right) \cdot O
\]

که در آن $O$ تعداد نرون‌های خروجی لایه است. همان‌گونه که پیداست، ساختار TCL باعث جایگزینی ضرب با جمع در فرمول محاسبه‌ی پارامترها می‌شود، که این امر منجر به کاهش چشمگیر حافظه و هزینه محاسباتی مدل می‌گردد.







\subsection{لایه رگرسیون تانسوری}

این لایه به‌صورت مستقیم و بدون نیاز به مسطح سازی، تانسورهای با مرتبه بالا را به بردار خروجی مدل نگاشت می‌دهد؛ به‌گونه‌ای که ساختار چندبعدی داده حفظ شده و نگاشت خروجی نیز به‌صورت مرتبه پایین \footnote{\lr{low-rank}} مدل‌سازی می‌شود.

\subsubsection*{فرمول‌بندی ریاضی}

فرض شود تانسور ورودی به \lr{TRL} به‌صورت زیر باشد:

\[
\mathcal{X} \in \mathbb{R}^{S \times I_0 \times I_1 \times \cdots \times I_N}
\]

که در آن $S$ اندازه‌ی دسته‌ی آموزشی و $I_k$ ابعاد تانسور هستند.

هدف لایه TRL نگاشت این تانسور به یک بردار خروجی $Y \in \mathbb{R}^{S \times O}$ است، که در آن $O$ تعداد گره های  خروجی است. نگاشت خطی میان تانسور ورودی و خروجی با استفاده از ضرب درونی تعمیم‌یافته صورت می‌گیرد:

\[
\mathbf{Y} = \langle \mathcal{X}, \mathcal{W} \rangle_N + \mathbf{b}
\]

که در آن:

\begin{itemize}
	\item $\mathcal{W} \in \mathbb{R}^{I_0 \times I_1 \times \cdots \times I_N \times O}$ تانسور وزن‌های رگرسیون است.
	\item نماد $\langle \cdot, \cdot \rangle_N$ نشان‌دهنده‌ی ضرب داخلی بر روی $N$ بعد اول از $\mathcal{W}$ و $N$ بعد آخر از $\mathcal{X}$ است.
	\item $\mathbf{b} \in \mathbb{R}^{O}$ بردار بایاس است.
\end{itemize}



\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/trl.png}
		\caption{\lr{tensor regression network}}
		\label{fig:tensor_regression_network}
	\end{minipage}
	\hfill
\end{figure}





برای کنترل تعداد پارامترها و بهره‌گیری از ساختار تانسوری، تانسور $\mathcal{W}$ با استفاده از تجزیه \lr{Tucker} مدل‌سازی می‌شود:

\[
\mathcal{W} = \mathcal{G} \times_0 U^{(0)} \times_1 U^{(1)} \cdots \times_N U^{(N)} \times_{N+1} U^{(N+1)}
\]

که در آن:

\begin{itemize}
	\item $\mathcal{G} \in \mathbb{R}^{R_0 \times R_1 \cdots \times R_N \times R_{N+1}}$ هسته‌ی کم‌مرتبه \footnote{\lr{core tensor}} است.
	\item $U^{(k)} \in \mathbb{R}^{I_k \times R_k}$ ماتریس‌های فشرده‌سازی برای ورودی هستند.
	\item $U^{(N+1)} \in \mathbb{R}^{O \times R_{N+1}}$ ماتریس فشرده‌سازی برای خروجی است.
\end{itemize}

فرمول نهایی خروجی مدل با جایگذاری $\mathcal{W}$ به‌صورت زیر خواهد بود:

\[
\mathbf{Y} = \left\langle \mathcal{X}, \mathcal{G} \times_0 U^{(0)} \cdots \times_N U^{(N)} \times_{N+1} U^{(N+1)} \right\rangle_N + \mathbf{b}
\]

یا به‌صورتی معادل و محاسباتی بهینه‌تر:

\[
\mathbf{Y} = \left\langle \mathcal{X} \times_0 (U^{(0)})^\top \cdots \times_N (U^{(N)})^\top, \mathcal{G} \times_{N+1} U^{(N+1)} \right\rangle_N + \mathbf{b}
\]

\subsubsection*{تحلیل تعداد پارامترها}

در لایه کاملا متصل، پس از مسطج سازی ورودی، تعداد پارامترها برابر است با:

\[
\text{تعداد پارامترهای FC} = \left( \prod_{k=0}^{N} I_k \right) \cdot O
\]

اما در \lr{TRL}، با در نظر گرفتن تجزیه \lr{Tucker}، تعداد پارامترها به‌صورت زیر محاسبه می‌شود:

\[
\text{تعداد پارامترهای TRL} = \left( \prod_{k=0}^{N+1} R_k \right) + \left( \sum_{k=0}^{N} I_k \cdot R_k \right) + R_{N+1} \cdot O
\]

که معمولاً به‌طور چشم‌گیری کمتر از مدل سنتی است، به‌ویژه زمانی که مقادیر $R_k$ کوچک‌تر از $I_k$ انتخاب شوند.

\subsection{چرا در مبدل های بینایی از تانسور استفاده میکنیم؟}




\subsubsection*{1. \textbf{کاهش تعداد پارامترها و حافظه مصرفی}}

یکی از چالش‌های اصلی در معماری‌های مبتنی بر مبدل های بینایی، رشد نمایی تعداد پارامترها در لایه‌های کاملا متصل  به‌ویژه در بخش‌های انتهایی شبکه است.

با جایگزینی این لایه‌ها با ساختارهای تانسوری مانند TCl و TRl می‌توان از ساختار چندبعدی داده بهره برده و از طریق تجزیه‌های کم‌مرتبه، تعداد پارامترها را به‌صورت چشمگیری کاهش داد. این جایگزینی نه‌تنها سبب صرفه‌جویی در حافظه می‌شود، بلکه پیچیدگی محاسباتی مدل را نیز کاهش داده و امکان به‌کارگیری آن را در محیط‌های کم‌منبع (مانند دستگاه‌های لبه‌ای و موبایل) فراهم می‌سازد.


\subsubsection*{\textbf{افزایش تفسیرپذیری با حفظ ساختار چندخطی داده}}

در حالی‌که عملیات مسطج سازی روی تانسورهای ورودی منجر به از بین رفتن روابط ساختاری میان ابعاد مختلف داده می‌شود، استفاده از لایه‌های تانسوری مانند TCl و TRL، این امکان را فراهم می‌سازد که ساختار چندبعدی داده حفظ شود. با حفظ این ساختار چندخطی، مدل قادر خواهد بود تا وابستگی‌ها و تعاملات میان ابعاد مختلف تصویر (مانند فضا، زمان و کانال‌های رنگی) را بهتر تحلیل کند.

این ویژگی به‌طور خاص در کاربردهایی که نیازمند تفسیرپذیری بالا هستند—مانند سیستم‌های تشخیص پزشکی، بینایی ماشین صنعتی، یا کاربردهای قانونی—می‌تواند نقش مهمی ایفا کند. زیرا در چنین حوزه‌هایی، درک تصمیمات مدل توسط انسان اهمیت بالایی دارد و تحلیل ساختار داخلی مدل بر پایه روابط تانسوری، درک بهتری از رفتار مدل فراهم می‌سازد.


\subsubsection*{3. \textbf{کاهش نیاز به داده‌های آموزشی بزرگ}}

مدل‌های بینایی به‌دلیل فقدان سوگیری مکانی ذاتی (مانند آنچه در شبکه‌های کانولوشنی وجود دارد)، نیازمند حجم عظیمی از داده برای آموزش مؤثر هستند. این مسئله در شرایطی که داده‌های برچسب‌خورده محدود هستند، به یک چالش جدی تبدیل می‌شود.

با استفاده از لایه‌های تانسوری مانند \lr{TCL} و \lr{TRL}، که نگاشت‌ها را به‌صورت چندخطی و فشرده مدل‌سازی کرده و ساختار درونی داده را حفظ می‌کنند، می‌توان از ظرفیت مدل به‌صورت بهینه‌تر بهره برد. این ساختار نه‌تنها به مدل اجازه می‌دهد که وابستگی‌های میان‌بعدی را بهتر درک کند، بلکه از بیش‌برازش در شرایط داده‌ی کم جلوگیری می‌نماید.

در نتیجه، معماری‌های مبتنی بر تانسور قادرند در دیتاست‌های کوچک نیز عملکردی قابل قبول داشته باشند و نیاز به حجم عظیم داده‌های آموزشی را کاهش دهند.



\subsection{روش تانسوری مبدل پنجره متحرک:}




\subsection{پیاده‌سازی مرحله‌ی  تعبیه پچ
	با استفاده از فشرده‌سازی تانسوری}

در معماری‌های کلاسیک مبتنی بر مبدل های بیانی \footnote{\lr{Vision transformer}}  و همچنین در ساختار مبدل های پنجره ای \footnote{\lr{swin transformer}}، مرحله‌ی اولیه‌ی پردازش تصویر شامل تقسیم تصویر به قطعات کوچک (پچ‌ها) و سپس نگاشت هر پچ به یک بردار نهفته با ابعاد ثابت است. این نگاشت معمولاً با استفاده از یک لایه‌ی خطی \footnote{\lr{linear}} یا پیچشی \footnote{\lr{convolution}} با کرنل و گام برابر با اندازه‌ی پچ انجام می‌شود.

در روش تانسوری به‌جای استفاده از نگاشت برداری ساده، از لایه‌ی فشرده‌سازی تانسوری \footnote{\lr{tensor contruction layer}} برای تبدیل هر پچ به یک تانسور چندبعدی در فضای ویژگی استفاده شده است. این نگاشت تانسوری نه‌تنها باعث حفظ ساختار چندخطی پچ‌ها می‌شود، بلکه با فشرده‌سازی موثر، منجر به کاهش پارامترها می شود.




در ابتدا فرض میکنیم  ورودی مدل تصویری با اندازه‌ی \lr{$(B, C, H, W)$} باشد که در آن:

\begin{itemize}
	\item $B$ اندازه‌ی دسته‌ی آموزشی \footnote{\lr{Batch Size}} است.
	\item $C$ تعداد کانال‌های تصویر (برای مثال ۳ در تصاویر رنگی).
	\item $H \times W$ ابعاد تصویر است.
\end{itemize}

این تصویر با استفاده از پچ‌هایی با اندازه‌ی \lr{$(P \times P)$} به $\frac{H}{P} \times \frac{W}{P}$ قطعه تقسیم می‌شود. سپس با استفاده از عملیات بازآرایی \footnote{\lr{rearrangement}}، ساختار ورودی به شکل زیر تبدیل می‌شود:

\[
\mathcal{X} \in \mathbb{R}^{B \times P_1 \times P_2 \times C_1 \times C_2 \times C_3}
\]

که در آن:

\begin{itemize}
	\item $P_1 = \frac{H}{P}$ و $P_2 = \frac{W}{P}$ به‌ترتیب تعداد پچ‌ها در راستای ارتفاع و عرض تصویر هستند.
	\item $C_1 = P$ و $C_2 = P$ ابعاد مکانی هر پچ‌اند.
	\item $C_3 = C$ تعداد کانال‌های تصویر ورودی است.
\end{itemize}

در این بازنمایی، هر پچ در موقعیت $(i,j)$ به‌صورت یک تانسور سه‌بعدی با ابعاد $C_1 \times C_2 \times C_3$ نمایش داده می‌شود. این ساختار چندبعدی امکان استفاده‌ی مستقیم از عملیات تانسوری بدون نیاز به تخت‌سازی را فراهم می‌سازد.



برای نگاشت هر پچ به فضای نهفته، به جای استفاده از mlp  از یک لایه‌ی فشرده ساز تانسوری  استفاده می‌شود. در واقع سه تا بعد اخر با استفاده از لایه فشرده ساز تنسوری تعبیه میشود.

\[
\mathcal{Z} \in \mathbb{R}^{B \times P_1 \times P_2 \times D_1 \times D_2 \times D_3}
\]

در این ساختار، هر پچ به‌جای تبدیل به یک بردار تخت، به یک تانسور سه‌بعدی در فضای نهفته تبدیل می‌شود. این تانسور ساختار درونی پچ را حفظ کرد می کند. 

\subsubsection*{فرمول‌بندی ریاضی عملیات فشرده‌سازی}

فرض میکنیم  $\mathcal{X}_{patch} \in \mathbb{R}^{C_1 \times C_2 \times C_3}$ نمایانگر یک پچ ورودی باشد. برای فشرده‌سازی این تانسور به فضای نهفته $(D_1, D_2, D_3)$، از سه ماتریس فشرده‌سازی قابل آموزش استفاده می‌شود:

\[
V^{(0)} \in \mathbb{R}^{D_1 \times C_1}, \quad
V^{(1)} \in \mathbb{R}^{D_2 \times C_2}, \quad
V^{(2)} \in \mathbb{R}^{D_3 \times C_3}
\]

عملیات فشرده‌سازی با استفاده از ضرب‌های چندحالته \footnote{\lr{n-mode product}} صورت می‌گیرد:

\[
\mathcal{X}_{emb} = \mathcal{X}_{patch} \times_0 V^{(0)} \times_1 V^{(1)} \times_2 V^{(2)}
\]

که در آن $\mathcal{X}_{emb} \in \mathbb{R}^{D_1 \times D_2 \times D_3}$ نگاشت نهفته‌ی تانسوری برای آن پچ خواهد بود.

در این مدل ساختار چند بعدی به پج ها داده میشود و همچنین با استفاده از تانسور تعداد پارامتر ها کاهش پیدا میکنند و همچنین ظرفیت مدل نیز افزایش پیدا میکند.




در نتیجه، خروجی نهایی مرحله‌ی تعبیه پچ به‌صورت زیر تعریف می‌شود:

\[
\mathcal{Z} \in \mathbb{R}^{B \times P_1 \times P_2 \times D_1 \times D_2 \times D_3}
\]

که در آن هر پچ به‌صورت یک تانسور کم‌بعد در فضای نهفته نمایش داده شده است.


\subsection{ماژول توجه سلف چندسری مبتنی بر پنجره به‌صورت تانسوری}

همانطور که در فصل قبل بیان شد در ساختار مبدل پنجره ای، جهت کاهش پیچیدگی محاسباتی، از روشی با عنوان  \footnote{\lr{Window-based Multi-Head Self-Attention}}بهره گرفته می‌شود. در این روش، ویژگی‌های مکانی تصویر به پنجره‌های کوچک با اندازه‌ی ثابت (بدون هم‌پوشانی) تقسیم شده و سپس عملیات  خود‌توجهی به‌صورت محلی و درون هر پنجره مستقل انجام می‌گیرد.

در روش تانسوری، به‌جای تولید بردارهای تخت‌شده‌ی $Q$، $K$ و $V$، این بردارها به‌صورت \textbf{تانسورهای چندبعدی} با استفاده از لایه فشرده ساز تانسوری تولید می‌شوند.

\subsubsection*{ ساختار ورودی و تقسیم به پنجره‌ها}

خروجی مرحله تعبیه  به‌صورت تانسوری به صورت زیر است 

\[
\mathcal{X} \in \mathbb{R}^{B \times H \times W \times D_1 \times D_2 \times D_3}
\]



در گام اول، همانند مبدل های پنجره ای به پنجره‌های $w \times w$ تقسیم می‌شوند. و خروجی ما به صورت  زیر خواهد بود.

\[
\mathcal{X}_{win} \in \mathbb{R}^{B \times N_H \times N_W \times w \times w \times D_1 \times D_2 \times D_3}
\]

که در آن $N_H = \frac{H}{w}$ و $N_W = \frac{W}{w}$ به‌ترتیب تعداد پنجره‌ها در راستای ارتفاع و عرض تصویر هستند.

\subsubsection*{2. محاسبه‌ی Q، K و V با استفاده از لایه فشرده ساز تانسوری}

برای هر پنجره‌ی مکانی، سه لایه‌ی فشرده‌سازی تانسوری مستقل (\lr{TCL}) برای استخراج تانسورهای Q، K و V طراحی شده‌اند. فرض می‌شود تانسور ورودی هر پنجره دارای ابعاد:

\[
\mathcal{X}_{patch} \in \mathbb{R}^{w \times w \times D_1 \times D_2 \times D_3}
\]

باشد. با اعمال سه عملیات ضرب n-مد به‌ترتیب در ابعاد تانسوری، نگاشت‌های Q، K و V به‌صورت زیر حاصل می‌شوند:

\[
\mathcal{Q} = \mathcal{X}_{patch} \times_2 V_Q^{(1)} \times_3 V_Q^{(2)} \times_4 V_Q^{(3)}
\]
\[
\mathcal{K} = \mathcal{X}_{patch} \times_2 V_K^{(1)} \times_3 V_K^{(2)} \times_4 V_K^{(3)}
\]
\[
\mathcal{V} = \mathcal{X}_{patch} \times_2 V_V^{(1)} \times_3 V_V^{(2)} \times_4 V_V^{(3)}
\]

که در آن هر $V^{(i)}$ ماتریس فشرده‌سازی با ابعاد $\mathbb{R}^{D_i \times D_i}$ است. بنابراین، خروجی هر لایه به فضای تانسوری مشابه با ورودی بازمی‌گردد:

\[
\mathcal{Q}, \mathcal{K}, \mathcal{V} \in \mathbb{R}^{w \times w \times D_1 \times D_2 \times D_3}
\]

\subsubsection*{3. تقسیم سرهای توجه چندگانه}

برای پیاده‌سازی مکانیزم چندسر\footnote{\lr{multi-head}}، ابعاد تانسوری خروجی به بخش‌های کوچک‌تری تقسیم می‌شود. فرض می‌شود:

\[
D_1 = h_1 \cdot d_1, \quad D_2 = h_2 \cdot d_2, \quad D_3 = h_3 \cdot d_3
\]

که در آن:
\begin{itemize}
	\item $h_1, h_2, h_3$ تعداد سرها در هر بُعد تانسوری،
	\item $d_1, d_2, d_3$ ابعاد نهفته‌ی هر سر در هر بُعد هستند.
\end{itemize}

با استفاده از عملیات بازآرایی، تانسورهای Q، K و V به شکل زیر سازمان‌دهی می‌شوند:

\[
\mathcal{Q}_{head} \in \mathbb{R}^{B \times N_H \times N_W \times w \times w \times h_1 \times h_2 \times h_3 \times d_1 \times d_2 \times d_3}
\]

و ساختار مشابهی برای K و V در نظر گرفته می‌شود.

\subsubsection*{4. محاسبه ماتریس توجه تانسوری}

عملیات ضرب داخلی تعمیم‌یافته بین تانسورهای Q و K برای محاسبه‌ی ماتریس توجه انجام می‌شود. برای هر سر $(a,b,c)$ و برای موقعیت‌های $(i,j)$ و $(k,l)$ در پنجره، مقدار توجه به‌صورت زیر محاسبه می‌شود:

\[
\text{Attention}_{ijkl}^{abc} = \sum_{x=1}^{d_1} \sum_{y=1}^{d_2} \sum_{z=1}^{d_3} Q_{ij}^{abcxyz} \cdot K_{kl}^{abcxyz}
\]

که نتیجه نهایی یک ماتریس توجه با ابعاد زیر است:

\[
\text{Attention} \in \mathbb{R}^{B \times N_H \times N_W \times h_1 \times h_2 \times h_3 \times w^2 \times w^2}
\]

برای پایدارسازی محاسبات، نرمال‌سازی زیر اعمال می‌شود:

\[
\text{scale} = \frac{1}{\sqrt{d_1 d_2 d_3}}
\]

و از نسخه علامت‌دار تابع softmax برای اعمال توجه استفاده می‌گردد:

\[
\text{Attention}_{\text{soft}} = \text{sign}(A) \cdot \text{softmax}(|A|)
\]

\subsubsection*{5. اعمال توجه و بازسازی ویژگی}

پس از محاسبه ماتریس توجه، تانسور خروجی هر پنجره با استفاده از ترکیب توجه و تانسور V محاسبه می‌شود:

\[
\mathcal{Y}_{ij}^{abcxyz} = \sum_{k,l} \text{Attention}_{ijkl}^{abc} \cdot V_{kl}^{abcxyz}
\]

و پس از انجام توجه برای هر پنجره دوباره باز آرایی میشوند و به حالت اولیه باز میگردند و ابعاد آن به صورت زیر میشود.

\[
\mathcal{Y} \in \mathbb{R}^{B \times H \times W \times D_1 \times D_2 \times D_3}
\]

در این مدل چون سر ها در سه بعد تفسیم میشوند در نتیجه این مدل آزادی خیلی بیشتری دارد.






\subsection{توجه علامت‌دار}

در مکانیزم‌های استاندارد خود توجهی که در مبدل ها مورد استفاده قرار می‌گیرند، ماتریس‌های $Q$, $K$ و $V$ با یکدیگر تعامل دارند تا وزن‌های توجه استخراج شوند. در این فرآیند، بردارهای $Q$  \footnote{\lr{Query}}
 و $K$ 
 
 \footnote{\lr{Key}}
  با استفاده از ضرب داخلی محاسبه می‌شوند و نتیجه برای هر توکن، بیان‌گر میزان اهمیت نسبی سایر توکن‌ها در ارتباط با آن توکن است.

فرمول رایج محاسبه توجه به‌صورت زیر تعریف می‌شود:

\[
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
\]

در این ساختار، خروجی $QK^\top$ می‌تواند شامل مقادیر مثبت، منفی یا صفر باشد؛ اما پس از اعمال تابع \lr{Softmax}، تمامی مقادیر به‌صورت نرمال‌سازی‌شده و مثبت خواهند بود. در نتیجه،اطلاعات قطبیت (علامت) که در نتیجه‌ی ضرب داخلی بین بردارهای $Q$ و $K$ وجود داشت، از بین می‌رود. این ویژگی باعث محدود شدن ظرفیت مدل در شناسایی «رابطه منفی یا متضاد» میان توکن‌ها می‌شود.

برای حفظ این اطلاعات قطبیت، د از رویکردی با عنوان  توجه علامت دار استفاده شده است. ایده‌ی اصلی این روش آن است که پس از محاسبه‌ی امتیازهای توجه $A = QK^\top$، عملیات نرمال‌سازی \lr{Softmax} بر قدرمطلق این امتیازها انجام شده و سپس علامت اولیه آن‌ها به نتیجه بازگردانده می‌شود.

فرمول این روش به‌صورت زیر تعریف می‌شود:

\[
\text{Attention}_{\text{sign}} = \text{sign}(A) \cdot \text{Softmax}(|A|)
\]

که در آن:
\begin{itemize}
	\item $A = QK^\top$ ماتریس امتیازهای اولیه توجه است.
	\item $\text{sign}(A)$ عملیاتی است که علامت هر مقدار را حفظ می‌کند (مقادیر $+1$, $-1$ یا $0$).
	\item $|A|$ قدرمطلق مقادیر است که شدت شباهت را بدون در نظر گرفتن جهت نشان می‌دهد.
\end{itemize}


برای درک بهتر تفاوت میان مکانیزم‌های توجه معمولی و توجه علامت دار یک مثال عددی ساده اما گویای زیر ارائه می‌شود.

فرض کنیم  بردار پرس‌وجو (Q) و دو بردار کلید (K) به‌صورت زیر تعریف شده‌اند:

\[
Q = [1,\ 2,\ -1], \quad
K_1 = [1,\ 0,\ -1], \quad
K_2 = [-1,\ 1,\ 1]
\]

\paragraph{محاسبه امتیاز توجه}

ابتدا شباهت میان Q و هر کلید با استفاده از ضرب داخلی محاسبه می‌شود:

\[
Q \cdot K_1 = (1)(1) + (2)(0) + (-1)(-1) = 1 + 0 + 1 = 2
\]
\[
Q \cdot K_2 = (1)(-1) + (2)(1) + (-1)(1) = -1 + 2 -1 = 0
\]

بنابراین بردار امتیاز توجه به‌صورت زیر حاصل می‌شود:

\[
A = [2,\ 0]
\]

\paragraph{الف) توجه معمولی:}

در توجه معمولی، از تابع softmax مستقیماً روی مقادیر $A$ استفاده می‌شود:

\[
\text{Softmax}(A) =
\left[
\frac{e^2}{e^2 + e^0},\ 
\frac{e^0}{e^2 + e^0}
\right]
\approx [0.88,\ 0.12]
\]

در اینجا، هر دو کلید مقدار وزن مثبت می‌گیرند، حتی اگر امتیاز شباهت دومی برابر با صفر بوده باشد. مدل نمی‌تواند تفاوت میان بی‌اثر بودن و تأثیر منفی یا متضاد را به‌خوبی درک کند.

\paragraph{ب) توجه علامت‌دار:}

در این رویکرد، ابتدا بردار $A$ به دو بخش علامت و قدرمطلق تفکیک می‌شود:

\[
\text{sign}(A) = [1,\ 0], \quad
|A| = [2,\ 0]
\]

سپس تابع softmax فقط روی قدرمطلق‌ها اعمال شده و خروجی زیر حاصل می‌شود:

\[
\text{Softmax}(|A|) = 
\left[
\frac{e^2}{e^2 + e^0},\ 
\frac{e^0}{e^2 + e^0}
\right]
\approx [0.88,\ 0.12]
\]

در نهایت، با ضرب عنصر‌به‌عنصر با علامت‌ها:

\[
\text{Attention}_{\text{sign}} = 
\text{sign}(A) \cdot \text{Softmax}(|A|) = [0.88,\ 0.00]
\]

در این حالت، کلید دوم به‌طور کامل کنار گذاشته شده است، چرا که امتیاز آن صفر بوده و نشانی از تأثیر مثبت یا منفی ندارد.


در توجه معمولی، تمامی مقادیر صفر یا منفی به وزن‌های مثبت نرمال‌سازی می‌شوند. در مقابل، توجه علامت‌دار توانایی تمایز بین ارتباط مثبت، منفی و خنثی را دارد. این قابلیت می‌تواند در مدل‌سازی دقیق‌تر تضادهای معنایی یا شباهت‌های منفی نقش مهمی ایفا کند.


\subsection{توجه مبتنی بر پنجره‌های جابه‌جا‌شده به صورت تانسوری}



توجه پنجره ای دارای یک محدودیت مهم است: پچ‌های واقع در پنجره‌های متفاوت هیچ‌گونه تبادل اطلاعاتی ندارند.
در حالی که ممکن هست این پج ها به هم نزدیک باشند به‌منظور رفع این محدودیت، مبدل های پنجره ای مکانیزم توجه مبتنی بر پنجره جا به جا شده معرفی شده است که در آن پنجره‌ها در برخی لایه‌ها به‌صورت جابه‌جا‌شده تعریف می‌شوند و از ماسک توجه برای جلوگیری از تداخل غیرمجاز استفاده می‌شود.

\subsubsection*{مراحل دقیق مکانیزم }

\begin{enumerate}
	\item \textbf{شیفت چرخشی تانسور ورودی:} خروجی مرحله قبلی  با یک  شیفت چرخه ای 
	\footnote{\lr{cyclic shift}} به اندازه $\lfloor \frac{w}{2} \rfloor$ پیکسل در راستای افقی و عمودی جابه‌جا می‌شود. این جابه‌جایی باعث می‌شود که پچ‌هایی که پیش‌تر در پنجره‌های جداگانه بودند، اکنون در یک پنجره جدید قرار گیرند.
	
	به این شیفت روی بعد اول و دوم تانسور که مکان پج را مشخص میکرد صورت میگیرد.
	
	\item \textbf{اعمال پنجره‌بندی جدید:} تانسور جابه‌جا‌شده به پنجره‌های بدون هم‌پوشانی جدید با اندازه $w \times w$ تقسیم می‌شود. پنجره‌های جدید از موقعیت‌های مختلف تصویر تشکیل شده‌اند.
	
	\item \textbf{اعمال خودتوجهی به‌صورت محلی با ماسک:} از آنجا که پنجره‌ها پس از شیفت ممکن است شامل توکن‌هایی از مکان‌های نامرتبط باشند، نیاز است که \textbf{توجه به‌صورت مفید  انجام شود. یک ماسک دودویی روی ماتریس توجه اعمال می‌شود که فقط اجازه توجه به توکن‌هایی را می‌دهد که واقعاً در یک پنجره معتبر قرار دارند.}
	
\end{enumerate}



خودتوجهی پنجره جابه جا شده باعث ارتباط میان پج های نزدیکی میشود که در خودتوجهی پنجره ای در یک پنجره قرار نداشته اند. 



\subsection{ادغام پچ‌ها به‌صورت تانسوری}

در مدل‌های سلسله‌مراتبی بینایی مانند پس از هر مرحله از استخراج ویژگی، لازم است ابعاد مکانی تصویر کاهش یابد تا ویژگی‌ها در سطوح بالاتر به‌صورت فشرده‌تر و معنایی‌تر نمایش داده شوند. این فرایند با عنوان ادغام پچ‌ها \footnote{\lr{Patch Merging}} شناخته می‌شود. در نسخه‌ی اصلی این مدل، ادغام پچ‌ها با استفاده از ترکیب ۴ پچ مجاور (به‌صورت یک پنجره‌ی $2 \times 2$) و سپس اعمال یک لایه‌ی خطی پس از مسطح‌سازی انجام می‌شود. این کار باعث نصف شدن ابعاد مکانی و افزایش ابعاد ویژگی می‌شود.

در این پژوهش، رویکرد متفاوتی برای ادغام پچ‌ها ارائه شده است که مبتنی بر نگاشت تانسوری است. به‌جای انجام \footnote{\lr{flatten}} بر روی پچ‌ها، از ساختار چندبعدی پچ‌ها استفاده شده و با کمک یک لایه‌ی فشرده‌سازی تانسوری، ادغام پچ‌ها بدون از بین رفتن ساختار چندخطی آن‌ها انجام می‌شود.

\subsubsection*{ساختار ورودی}

مانند  خروجی مرحله قبلی شبکه دارای ساختار تانسوری زیر باشد:

\[
\mathcal{X} \in \mathbb{R}^{B \times H \times W \times R_1 \times R_2 \times C}
\]

که در آن:

\begin{itemize}
	\item $B$ اندازه دسته آموزشی،  
	\item $H \times W$ ابعاد مکانی پچ‌ها،  
	\item $(R_1, R_2, C)$ ابعاد فضای نهفته‌ی هر پچ به‌صورت تانسوری.
\end{itemize}

هدف این مرحله، کاهش ابعاد مکانی به $\left( \frac{H}{2}, \frac{W}{2} \right)$ و در عین حال، افزایش غنای بازنمایی ویژگی‌ها است.

\subsubsection*{ادغام پچ‌های مجاور}

برای کاهش ابعاد مکانی، ابتدا هر چهار پچ مجاور در پنجره‌ای $2 \times 2$ انتخاب می‌شوند. این چهار پچ به‌صورت زیر دسته‌بندی می‌شوند:

\begin{itemize}
	\item پچ بالا-چپ،
	\item پچ بالا-راست،
	\item پچ پایین-چپ،
	\item پچ پایین-راست.
\end{itemize}

در مرحله بعد، این چهار پچ با یکدیگر ترکیب می‌شوند، به این صورت که هرکدام به‌صورت مستقل نگه داشته شده و در امتداد بعد کانالی $(C)$ به یکدیگر متصل می‌شوند. در نتیجه، در هر موقعیت مکانی جدید، تانسوری با ابعاد $(R_1, R_2, 4C)$ خواهیم داشت. در سطح کلان، خروجی میان‌مرحله‌ای به‌صورت زیر خواهد بود:

\[
\mathcal{X}_{\text{merged}} \in \mathbb{R}^{B \times \frac{H}{2} \times \frac{W}{2} \times R_1 \times R_2 \times 4C}
\]

\subsubsection*{فشرده‌سازی ویژگی‌های ادغام‌شده}

پس از ادغام کانالی، لازم است ابعاد ویژگی‌ها به صورت کنترل‌شده کاهش یابد تا بتوان آن را به شکل ورودی ماژول‌های بعدی تطبیق داد. برای این منظور، از یک لایه‌ی فشرده‌سازی تانسوری استفاده می‌شود که به‌صورت مستقیم بر روی ابعاد نهفته $(R_1, R_2, 4C)$ عمل می‌کند. برخلاف لایه‌های خطی کلاسیک، در اینجا عملیات فشرده‌سازی در چندین بعد به‌صورت همزمان و با حفظ ساختار چندخطی انجام می‌شود.

خروجی حاصل، تانسوری با ساختار:

\[
\mathcal{Y} \in \mathbb{R}^{B \times \frac{H}{2} \times \frac{W}{2} \times R_1' \times R_2' \times C'}
\]

که در آن $(R_1', R_2', C')$ ابعاد جدید ویژگی‌ها هستند و با توجه به محدودیت طراحی، باید رابطه زیر میان ابعاد قدیم و جدید برقرار باشد:

\[
2 \cdot R_1' \cdot R_2' \cdot C' = 4 \cdot R_1 \cdot R_2 \cdot C
\]

این رابطه تضمین می‌کند که ادغام از نظر اطلاعاتی قابل‌پشتیبانی و از نظر مدل‌سازی متوازن باقی می‌ماند.


