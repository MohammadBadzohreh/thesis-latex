\chapter{پیشینه پژوهش}

\section{مقدمه}
ظهور مدل‌های Transformer و انقلاب در یادگیری عمیق، یکی از تحولات اساسی در حوزهٔ پردازش زبان طبیعی (NLP) و یادگیری ماشین به شمار می‌رود \cite{vaswani2017attention,bahdanau2014neural}. این مدل‌ها باعث تغییرات عمده‌ای در نحوهٔ ساخت و آموزش مدل‌های زبانی و همچنین در بسیاری از کاربردهای دیگر یادگیری ماشین شده‌اند و توانستند بسیاری از مشکلات مدل‌های قبلی را حل کنند\cite{devlin2018bert,radford2018improving}.
\section{مشکلات ترجمه ماشینی و ترانسفورمرها}
در ابتدا، ترجمه ماشینی (MT) یک چالش اساسی در زمینهٔ پردازش زبان طبیعی بود. مدل‌های اولیه‌ای مانند مدل‌های مبتنی بر قواعد (\lr{Rule-based Models}) برای ترجمه استفاده می‌شدند که در آن‌ها، ترجمه‌ها به‌صورت دستی با استفاده از قواعد زبانی مشخص تنظیم می‌شدند \cite{nagao1984framework,hutchins1986machine}. این روش‌ها هرچند دقیق بودند، اما محدودیت‌های زیادی داشتند و نمی‌توانستند ویژگی‌های پیچیده‌تر زبان را مدل‌سازی کنند.

سپس مدل‌های آماری (\lr{Statistical Models}) معرفی شدند \cite{brown1993mathematics,koehn2010statistical}. این مدل‌ها از داده‌های ترجمه‌شده برای آموزش مدل‌های آماری استفاده می‌کردند که احتمال ترجمهٔ صحیح را براساس شواهد آماری محاسبه می‌کردند. مدل‌هایی مانند مدل‌های ترجمهٔ آماری مبتنی بر جمله (\lr{Phrase-based Statistical Models}) \cite{koehn2003statistical} از این نوع بودند که قادر به ترجمهٔ جملات بهتر از مدل‌های مبتنی بر قواعد بودند، اما هنوز هم در ترجمه‌های پیچیده با مشکلاتی روبه‌رو بودند.

بعد از این مدل‌ها، مدل‌های بازگشتی (\lr{Recurrent Models}) به وجود آمدند که مشکلات آن‌ها در فصل گذشته بیان شد \cite{elman1990finding,sutskever2014sequence}. در نهایت، این مشکلات باعث به وجود آمدن ترانسفورمرها شد \cite{bahdanau2014neural}.


\section{ظهور ترانسفورمرها}
در سال ۲۰۱۷، مقاله‌ای توسط گوگل منتشر شد که مفهوم جدیدی به نام ترانسفورمرها را معرفی کرد \cite{vaswani2017attention}. این مقاله به موضوع ترجمهٔ ماشینی پرداخت و نشان داد که با استفاده از مکانیزم توجه (\lr{Attention Mechanism}) می‌توان بسیاری از مشکلات مدل‌های قبلی را حل کرد \cite{luong2015effective}.

مدل‌های ترانسفورمر برخلاف مدل‌های قبلی که از پردازش سریالی استفاده می‌کردند، از پردازش موازی بهره می‌برند. این ویژگی به ترانسفورمرها اجازه می‌دهد که به‌طور همزمان به تمام بخش‌های ورودی توجه کنند. این قابلیت باعث شد که ترانسفورمرها در پردازش تصویر و متن بسیار سریع‌تر و دقیق‌تر از مدل‌های قبلی عمل کنند \cite{vaswani2017attention}.

\section{معماری ترانسفورمرها}
در \textbf{تصویر \ref{fig:transformer_architecture}}، معماری ترانسفورمر نمایش داده شده است و بخش‌ها و اجزای مختلف آن مشخص شده است. معماری ترانسفورمر از دو بخش اصلی تشکیل شده است:
\begin{itemize}
	\item \textbf{انکودر (\lr{Encoder}):}
	وظیفهٔ انکودر این است که دادهٔ ورودی را دریافت کند و ویژگی‌های آن را استخراج کند.
	\item \textbf{دیکودر (\lr{Decoder}):}
	وظیفهٔ دیکودر این است که ویژگی‌های استخراج‌شده را به زبان مقصد تبدیل کند.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{Transformer-model-architecture.png}
	\caption{معماری ترانسفورمرها}
	\label{fig:transformer_architecture}
\end{figure}

\subsection{Embedding}
ددر زبان طبیعی، کلمات به شکل رشته‌های متنی هستند مانند \lr{کتاب}، \lr{ماشین} و ... . کامپیوترها نمی‌توانند به‌طور مستقیم این کلمات را به شکل رشته‌های متنی پردازش کنند. به همین دلیل، در یادگیری ماشین این کلمات را به شکل یک بردار نمایش می‌دهیم. این بردار بیانگر آن کلمه در مدل است تا ماشین بتواند آن کلمه را پردازش کند \cite{bengio2003neural}.

این بردارها ویژگی‌های کلمه را در فضای عددی نمایش می‌دهند. روش‌های مختلفی برای تبدیل متن به بردار وجود دارند. از جمله این روش‌ها می‌توان به روش‌های \lr{Word2Vec} \cite{mikolov2013distributed} و \lr{GloVe} \cite{pennington2014glove} اشاره کرد.

همان‌طور که در \autoref{fig:word_embedding} نشان داده شده است، هر کلمه که به صورت توکن است، ابتدا در دیکشنری تعریف‌شده پیدا می‌شود و پس از پیدا شدن در دیکشنری، با استفاده از روش‌های \lr{Embedding}، هر کلمه به برداری از اعداد تبدیل می‌شود. این \lr{Embedding}‌ها شباهت‌های معنایی بین کلمات را مدل‌سازی می‌کنند و کلماتی که از نظر معنایی شبیه به هم هستند، بردار آن‌ها نیز به یکدیگر نزدیک‌تر است. به این ترتیب، کلمات برای مدل‌ها و شبکه‌های عصبی قابل‌فهم می‌شوند \cite{mikolov2013distributed,pennington2014glove}.





 \begin{figure}[h]
 	\centering
 	\begin{minipage}[b]{0.7\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{transformer_images/word_embedding.png}
 		\caption{\lr{word embedding}}
 		\label{fig:word_embedding}
 	\end{minipage}
 	\hfill
 \end{figure}



\subsection{\lr{positional embedding}}

ما تا الان هر کلمه را به برداری از اعداد که برای مدل قابل فهم باشد، تبدیل کرده‌ایم. اما مدل‌های ترانسفورمر نمی‌توانند جایگاه هر کلمه را تشخیص دهند. در مدل‌های ترانسفورمر، برخلاف مدل‌های بازگشتی، به دلیل اینکه کلمات به‌صورت موازی وارد می‌شوند، نیاز داریم تا جایگاه هر کلمه را بدانیم. به‌طور مثال، در جملهٔ «من تو را دوست دارم» باید به‌طور دقیق بدانیم که «من» کلمهٔ اول جمله است، «تو» کلمهٔ دوم جمله است و ... .

حال ما باید به مدل توالی این کلمات را بفهمانیم. بنابراین، نیاز داریم به مدل یک سری اطلاعات اضافی بدهیم به‌طوری‌که مدل توالی کلمات را یاد بگیرد. روش‌های مختلفی برای اضافه‌کردن \lr{Positional Embedding} به مدل وجود دارد. در ترانسفورمرها از روش \lr{Sinusoidal Positional Embedding} استفاده می‌شود \cite{vaswani2017attention}.

این روش قابل یادگیری نیست و صرفاً از یک سری فرمول‌های ساده برای \lr{Positional Embedding} استفاده می‌کند.  
برای موقعیت \( pos \) در توالی و بُعد \( i \) در فضای برداری، تعبیهٔ موقعیتی به‌صورت زیر تعریف می‌شود:  


\begin{equation}
	PE(pos, 2i) = \sin\left( \frac{pos}{10000^{\frac{2i}{d}}} \right)
	\label{eq:pe_even}
\end{equation}

و برای مقادیر فرد:  

\begin{equation}
	PE(pos, 2i+1) = \cos\left( \frac{pos}{10000^{\frac{2i}{d}}} \right)
	\label{eq:pe_odd}
\end{equation}
	



\begin{itemize}
	\item \( pos \): موقعیت کلمه در توالی است (مثلاً از \( 0 \) تا \( N-1 \) برای یک توالی \( N \)-تایی).
	\item \( i \): شاخص بعد در بردار موقعیتی (از \( 0 \) تا \( d-1 \) برای بعد فضای برداری \( d \)).
	\item \( d \): ابعاد فضای برداری مدل که نشان می‌دهد هر کلمه در چند بعد نمایش داده می‌شود.
	\item \( 10000 \): یک مقدار ثابت برای تنظیم مقیاس توابع تناوبی و ایجاد فرکانس‌های مختلف در ابعاد گوناگون.
\end{itemize}  

همان‌طور که در شکل \autoref{fig:word_embedding_positional_embedding} مشاهده می‌کنید، بعد از \lr{embedding} کلمات، به آن \lr{positional embedding} اضافه می‌شود. در این روش از توابع سینوس و کسینوس استفاده می‌شود. این توابع موقعیت‌ها را در فضای برداری به‌گونه‌ای نگاشت می‌کنند که مدل بتواند از ترتیب کلمات در توالی آگاه باشد \cite{vaswani2017attention}. این ویژگی به مدل کمک می‌کند تا توالی زمانی را درک کرده و الگوهای زمانی را شبیه‌سازی کند. از مزایای این روش می‌توان به عدم نیاز به آموزش و توزیع متوازن جایگاه کلمات اشاره کرد.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/positional_embedding.png}
		\caption{\lr{word embedding + positional embedding}}
		\label{fig:word_embedding_positional_embedding}
	\end{minipage}
	\hfill
\end{figure}






\subsection{\lr{attention}}



در روش‌های قدیمی (مانند RNN یا LSTM)، توالی ورودی (مثلاً یک جمله) معمولاً به‌صورت گام‌به‌گام پردازش می‌شد \cite{elman1990finding,hochreiter1997long}. اما در ترانسفورمر می‌خواهیم مدلی داشته باشیم که به هر موقعیت (مثلاً یک کلمه) در توالی نگاه کند و به همهٔ موقعیت‌های دیگر نیز به‌صورت موازی دسترسی داشته باشد. به این مفهوم \textbf{توجه} می‌گوییم.

به زبان ساده، وقتی توکن (کلمه) \( i \) به توکن‌های دیگر نگاه می‌کند، می‌خواهد بداند کدام توکن‌ها برای تفسیر معنای خودش مهم‌ترند.

به طور مثال در جمله‌ی «یک گربه روی زمین نشسته است» می‌خواهد بداند کلمه‌ی «گربه» به واژه‌ی «نشستن» بیشتر توجه کند یا به «زمین». در این‌جا فعل «نشستن» ارتباط نزدیک‌تری به «گربه» دارد و از نظر معنایی مرتبط‌تر است.



\[
Q = \text{Query (پرسش)}, \quad K = \text{Key (کلید)}, \quad V = \text{Value (مقدار / ارزش)}
\]





در \lr{Scaled Dot-Product Attention} \cite{vaswani2017attention}، ابتدا شباهت یا ارتباط بین \lr{Query} و \lr{Key} را با محاسبهٔ ضرب داخلی (\lr{Dot Product}) به‌دست می‌آوریم، سپس آن را نرمال می‌کنیم (با تقسیم بر \( d_k \)) و از تابع \lr{softmax} استفاده می‌کنیم تا ضرایب توجه (\lr{Attention Weights}) را به‌دست آوریم. در نهایت با همین ضرایب، ترکیبی خطی از بردارهای \lr{Value} را می‌گیریم.

فرمول به‌شکل زیر است:

\begin{equation}
	\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
	\label{eq:attention}
\end{equation}

که در آن:

\[
Q \in \mathbb{R}^{n \times d_k} \quad \text{ماتریس پرسش برای } n \text{ توکن}
\]
\[
K \in \mathbb{R}^{n \times d_k} \quad \text{ماتریس کلید برای } n \text{ توکن}
\]
\[
V \in \mathbb{R}^{n \times d_v} \quad \text{ماتریس مقدار برای } n \text{ توکن}
\]

تقسیم بر \( d_k \) باعث می‌شود مقدار ضرب داخلی در ابعاد بالا خیلی بزرگ نشود و شیب‌ها (Gradients) پایدار بمانند.

\begin{equation}
	\alpha = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)
	\label{eq:alpha}
\end{equation}
\(\alpha\) یک ماتریس با ابعاد \( n \times n \) است که سطر \( i \)-ام آن ضرایب توجه برای توکن \( i \) را نشان می‌دهد.

تفسیر ضرایب توجه: هر سطر از \( \alpha \) نشان می‌دهد که توکن فعلی به چه توکن‌هایی در جمله، با چه شدتی توجه می‌کند.



\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/attention.png}
		\caption{Attention}
		\label{fig:attention}
	\end{minipage}
	\hfill
	
\end{figure}





ایدهٔ چندسری:  
به‌جای آنکه فقط یک‌بار \( Q, K, V \) بسازیم و عملیات توجه را انجام دهیم، چندین مجموعهٔ متفاوت \( Q_i, K_i, V_i \) می‌سازیم (هر کدام یک «Head» یا سر نام دارد) و به‌صورت موازی محاسبات Attention را انجام می‌دهیم. سپس خروجی همهٔ این Headها را کنار هم قرار داده (Concat) و در نهایت با یک ماتریس وزن دیگر ضرب می‌کنیم تا به بعد اصلی بازگردیم.

فرمول مربوط به این ایده به‌شکل زیر است:


\begin{equation}
	\text{head}_i = \text{Attention}(Q_i, K_i, V_i)
	\label{eq:head_i}
\end{equation}


\begin{equation}
	\text{MultiHead}(Q, K, V) = [\text{head}_1 \oplus \cdots \oplus \text{head}_h] W_O
	\label{eq:multihead}
\end{equation}

که در آن \( \oplus \) نشان‌دهندهٔ عمل الحاق (Concatenation) است.

ماتریس وزن \( W_O \) به‌شکل زیر است:

\[
W_O \in \mathbb{R}^{(h \cdot d_v) \times d_{\text{model}}}
\]

که \( W_O \) ماتریسی است که خروجی الحاق‌شده را به بعد \( d_{\text{model}} \) برمی‌گرداند.





\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/multi_head_attention.png}
		\caption{\lr{multi head attention}}
		\label{fig:attention}
	\end{minipage}
	\hfill
	
\end{figure}



\subsection*{چرا چندین سر؟}

\textbf{مشاهدهٔ چند منظر متفاوت:} هر Head می‌تواند الگوهای گوناگونی از وابستگی‌ها را بیاموزد (مثلاً یک Head می‌تواند یاد بگیرد کلمهٔ فعلی با کلمات همسایهٔ نزدیک خود بیشتر مرتبط شود، یک Head دیگر روی ارتباط با کلماتی در فاصلهٔ دورتری متمرکز باشد، Head دیگر روی مطابقت جنس و تعداد در دستور زبان و ...).

\textbf{افزایش ظرفیت مدل:} با داشتن چند Head، مدل می‌تواند قدرت بیان بیشتری داشته باشد.

\textbf{ابعاد کمتر در هر Head:} در عمل، اگر \( d_{\text{model}} \) مثلاً 512 باشد، و تعداد Headها \( h = 8 \)، آنگاه هر Head ابعادی در حدود \( d_k = 64 \) خواهد داشت؛ و این محاسبات ضرب داخلی را نیز مقیاس‌پذیر و قابل موازی‌سازی می‌کند.


\subsection{\lr{Residual Connection (Add)}}
در معماری‌های عمیق، هنگامی که تعداد لایه‌ها زیاد می‌شود، اغلب دچار ناپایداری گرادیان (\lr{Vanishing/Exploding Gradients}) می‌شوند و این مشکل باعث دشواری در آموزش مدل می‌گردد \cite{hochreiter1997long,bengio1994learning}. 

در مدل ترانسفورمر \cite{vaswani2017attention}، به جای این که خروجی \lr{attention} را به‌صورت مستقیم به لایهٔ بعدی بدهیم، ورودی آن را نیز حفظ کرده و به خروجی اضافه می‌کنیم. ایدهٔ اصلی این روش از اتصالات باقی‌مانده (Residual Connections) در شبکه‌های عمیق الهام گرفته شده است \cite{he2016deep}.

اگر \( x \) ورودی به زیرماژول و \( \text{SubLayer}(x) \) خروجی آن زیرماژول باشد، در انتهای کار عبارت زیر را محاسبه می‌کنیم:

\begin{equation}
	x + \text{SubLayer}(x)
	\label{eq:sublayer}
\end{equation}

این جمع به‌صورت عنصر به‌عنصر (\lr{Element-wise Addition}) انجام می‌شود.

\subsection{مزایای \lr{Residual Connection} در ترانسفورمر}

\subsection*{کمک به جریان یافتن گرادیان}
وقتی ورودی مستقیماً به خروجی اضافه می‌شود، مسیری مستقیم برای عبور شیب (گرادیان) به عقب ایجاد می‌گردد.
در صورت نبود این اتصال، اگر شبکه عمیق شود، گرادیان‌ها ممکن است در لایه‌های پایین محو شوند و عملاً \lr{gradient vanishing} رخ دهد \cite{hochreiter1997long,bengio1994learning}.

\subsection*{حفظ اطلاعات اصلی (هویت ورودی)}
حتی اگر زیرماژول تغییری در اطلاعات ورودی ایجاد کند، با وجود \lr{Residual Connection}، ورودی اصلی همواره در خروجی نهایی حضور دارد.
این ویژگی باعث می‌شود در صورت ناکافی‌بودن یادگیری زیرماژول یا در مراحل اولیهٔ آموزش، دست‌کم بخشی از سیگنال (اطلاعات) خام به لایه‌های بالاتر برسد \cite{he2016deep,vaswani2017attention}.

\subsection*{کاهش ریسک تخریب ویژگی‌ها}
در شبکه‌های عمیق، یکی از مشکلات این است که هر لایه ممکن است بخشی از اطلاعات مفید را تخریب کند. \lr{Residual Connection} تضمین می‌کند که اگر لایه‌ای به هر دلیل نتوانست الگوی بهینه را یاد بگیرد، اطلاعات قبلی حداقل به‌صورت دست‌نخورده تا حدی منتقل می‌شود.


\section{Layer Normalization (Norm):}

در یادگیری عمیق، نرمال‌سازی (Normalization) داده‌های یک لایه یا فعال‌سازی‌ها، اغلب به سرعت بخشیدن به همگرایی و پایدار کردن آموزش کمک شایانی می‌کند. شاید معروف‌ترین نوع نرمال‌سازی، \lr{Batch Normalization} باشد که پیش‌تر در کارهای بینایی (CNNها) بسیار مورداستفاده قرار گرفت \cite{ioffe2015batch}.

\lr{Layer Normalization} روشی جایگزین است که در ترانسفورمر استفاده می‌شود \cite{ba2016layer,vaswani2017attention}. علت اصلی این انتخاب، ماهیت توالی‌محور (Sequence) بودن داده‌ها در NLP و عدم تمایل به وابستگی به آمار مینی‌بچ است.

\subsection*{تفاوت \lr{Layer Norm} با \lr{Batch Norm}}

\subsection*{\lr{Batch Normalization}}
در \lr{Batch Norm}، برای نرمال‌سازی، میانگین و واریانس روی تمام نمونه‌های موجود در مینی‌بچ (و نیز در طول ابعاد ویژگی) محاسبه می‌شود \cite{ioffe2015batch}.
این موضوع در NLP کمی دردسرساز است؛ چون ترتیب (Order) توکن‌ها، طول جمله‌ها و حتی اندازهٔ مینی‌بچ ممکن است نامنظم باشد.
همچنین به خاطر تنوع طول توالی‌ها (\lr{Sequence Length})، پیاده‌سازی \lr{Batch Norm} می‌تواند پیچیده شود.

\subsection*{\lr{Layer Normalization}:}
در \lr{Layer Norm}، برای هر توکن به‌صورت جداگانه (در طول بُعد ویژگی)، میانگین و واریانس گرفته می‌شود \cite{ba2016layer}.
فرض کنید در یک لایه، بردار \( h_i \in \mathbb{R}^{d_{\text{model}}} \) مربوط به توکن \( i \) باشد؛ یعنی ابعاد ویژگی آن \( d_{\text{model}} \) است. ما میانگین \( \mu_i \) و واریانس \( \sigma_i^2 \) را از اجزای این بردار محاسبه می‌کنیم:

\begin{equation}
	\mu_i = \frac{1}{d_{\text{model}}} \sum_{k=1}^{d_{\text{model}}} h_{i,k}, \quad
	\sigma_i^2 = \frac{1}{d_{\text{model}}} \sum_{k=1}^{d_{\text{model}}} (h_{i,k} - \mu_i)^2
	\label{eq:mean_variance}
\end{equation}


سپس نرمال‌سازی برای هر مؤلفهٔ \( k \) در بردار توکن \( i \) به شکل زیر انجام می‌شود:

\begin{equation}
	\hat{h}_{i,k} = \frac{h_{i,k} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
	\label{eq:normalized_h}
\end{equation}

در نهایت، برای این‌که مدل بتواند مقیاس و بایاس جدیدی یاد بگیرد، شبیه \lr{Batch Norm}، دو پارامتر \( \gamma \) (Scale) و \( \beta \) (Bias) نیز در طول بعد ویژگی اعمال می‌شوند:

\begin{equation}
	\text{LayerNorm}(h_i) = \gamma \odot \hat{h}_i + \beta
	\label{eq:layernorm}
\end{equation}


که در آن \( \gamma, \beta \in \mathbb{R}^{d_{\text{model}}} \) هستند و \( \odot \) ضربِ عنصر به عنصر است \cite{ba2016layer}.

\subsection*{مزایای \lr{Layer Normalization} در ترانسفورمر}

\begin{itemize}
	\item \textbf{بی‌نیازی از وابستگی به ابعاد مینی‌بچ:}  
	با \lr{Layer Norm}، می‌توان حتی با اندازهٔ مینی‌بچ برابر ۱ نیز به‌خوبی آموزش دید، چراکه آمارها وابسته به ابعاد ویژگی‌اند و نه مینی‌بچ \cite{ba2016layer}.
	
	\item \textbf{پایدارسازی توزیع فعال‌سازی‌ها:}  
	زمانی که مدل در حال یادگیری است، توزیع‌های داخلی لایه‌های میانی ممکن است تغییر کند (پدیدهٔ \lr{Internal Covariate Shift}). \lr{Layer Norm} با نرمال‌سازی این توزیع، آموزش را پایدارتر و سریع‌تر می‌کند \cite{ioffe2015batch,ba2016layer}.
	
	\item \textbf{سازگاری با داده‌های توالی‌محور:}  
	هر توکن را جداگانه نرمال می‌کند و نگرانی‌ای بابت ترتیب طول جمله‌ها، یا قرار گرفتن چند جملهٔ کوتاه/بلند در یک مینی‌بچ نداریم \cite{vaswani2017attention}.
\end{itemize}

در معماری ترانسفورمر، پس از خروجیِ هر زیرماژول (مثل \lr{Attention} یا \lr{MLP})، مراحل به‌شکل زیر است:

\paragraph{\lr{Residual Connection}:}
ابتدا ورودی همان زیرماژول (مثلاً بردار \( x \)) را با خروجی زیرماژول (\( \text{SubLayer}(x) \)) جمع می‌کنیم. حاصل این جمع را می‌توان چنین نوشت:

\[
z = x + \text{SubLayer}(x)
\]

این \( z \) حالا ترکیبی از اطلاعات اصلی ورودی و اطلاعات یادگرفته‌شده توسط \lr{SubLayer} است.

\paragraph{\lr{Layer Normalization}:}
سپس این بردار \( z \) را وارد لایهٔ \lr{LayerNorm} می‌کنیم:

\[
y = \text{LayerNorm}(z)
\]

خروجی نهایی را می‌توان به لایهٔ بعدی پاس داد یا به مرحلهٔ بعدی در همین لایه.

به‌عبارتی اگر بخواهیم در یک فرمول واحد بیان کنیم:

\[
\text{Add \& Norm} = \text{LayerNorm}\bigl(x + \text{SubLayer}(x)\bigr)
\]


\section{\lr{decoder}}
دیکودر در معماری ترانسفورمرها وظیفهٔ تولید خروجی نهایی را بر عهده دارد. این خروجی معمولاً می‌تواند توالی هدف (\lr{Target Sequence}) باشد، مانند ترجمهٔ یک جمله یا پیش‌بینی توکن‌های بعدی در یک توالی \cite{vaswani2017attention}. 

در این بخش، دیکودر دو ورودی اصلی دارد:  
1. توالی هدف که معمولاً به‌صورت خودکار تولید می‌شود (مثلاً در ترجمهٔ ماشینی یا تولید متن)،  
2. نمایش (\lr{Representation}) کدشده که توسط انکودر (\lr{Encoder}) تولید شده است و شامل ویژگی‌های استخراج‌شده از توالی ورودی می‌باشد.  

دیکودر از این ورودی‌ها استفاده می‌کند تا به‌صورت گام‌به‌گام، خروجی نهایی خود را تولید کند \cite{bahdanau2014neural,sutskever2014sequence}.

همان‌طور که در شکل \ref{fig:Decoder} مشاهده می‌کنید، دیکودر دو ورودی دارد.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.25\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/decoder.png}
		\caption{Decoder}
		\label{fig:Decoder}
	\end{minipage}
	\hfill
\end{figure}

تمامی بخش‌های دیکودر مانند انکودر هستند اما در دیکودر \lr{masked multi-head attention} وجود دارد \cite{vaswani2017attention}.

\section{masked multi head attention}
در ترانسفورمر، مکانیزم \lr{Multi-Head Attention} در بخش دیکودر به‌صورت \lr{Masked} پیاده‌سازی می‌شود تا مدل نتواند توکن‌های آینده را ببیند و به‌صورت خودبازگشتی (\lr{Autoregressive}) توکن بعدی را پیش‌بینی کند \cite{vaswani2017attention}.  
در واقع ایدهٔ اصلی استفاده از \lr{mask} جلوگیری از مشاهدهٔ آینده است.

در معماری‌های خودبازگشتی (\lr{Autoregressive})، مدل در گام \( i \) از دیکودر تنها باید به توکن‌های قبلی \( \{ y_1, \dots, y_{i-1} \} \) دسترسی داشته باشد؛ اما نه به توکن‌های \( \{ y_{i+1}, y_{i+2}, \dots \} \). اگر مدل بتواند توکن‌های آینده را «نگاه» کند، پیش‌بینی توکن بعدی آسان و غیرواقعی می‌شود (مشکل نشت اطلاعات) \cite{bahdanau2014neural,sutskever2014sequence}.

به همین دلیل در \lr{Masked Multi-Head Self-Attention} دیکودر، از یک ماتریس ماسک \( M \) استفاده می‌کنیم که اجازه نمی‌دهد هر توکن به توکن‌های آینده‌اش توجه کند.

\section{مثال عددی \lr{mask attention}}
فرض کنید دنبالهٔ 4 توکنی داریم:
\[
[y_1, y_2, y_3, y_4]
\]
خروجی \lr{Scaled Dot-Product} (قبل از \texttt{softmax}) یک ماتریس \(4 \times 4\) خواهد بود:
\[
S =
\begin{bmatrix}
	s_{1,1} & s_{1,2} & s_{1,3} & s_{1,4} \\
	s_{2,1} & s_{2,2} & s_{2,3} & s_{2,4} \\
	s_{3,1} & s_{3,2} & s_{3,3} & s_{3,4} \\
	s_{4,1} & s_{4,2} & s_{4,3} & s_{4,4}
\end{bmatrix}
\]

\begin{itemize}
	\item \textbf{سطر 1 (توکن اول)}: تنها می‌تواند خودش (\textbf{ستون 1}) را ببیند، اما \textbf{ستون‌های 2 تا 4} ماسک می‌شوند.
	\item \textbf{سطر 2 (توکن دوم)}: می‌تواند به \textbf{ستون‌های 1 و 2} نگاه کند، اما \textbf{ستون‌های 3 و 4} ماسک می‌شوند.
	\item \textbf{سطر 3}: می‌تواند \textbf{ستون‌های 1، 2 و 3} را ببیند، اما \textbf{ستون 4} ماسک می‌شود.
	\item \textbf{سطر 4}: می‌تواند به \textbf{ستون‌های 1، 2، 3 و 4} دسترسی داشته باشد (چهارمین توکن می‌تواند توکن‌های قبلی را ببیند. همچنین این توکن \textbf{خودش} نیز معمولاً در دسترس است — بسته به پیاده‌سازی، ممکن است توکن فعلی از خودش نیز استفاده کند یا نه. در معماری استاندارد، سطر \( i \) معمولاً به ستون \( i \) هم دسترسی دارد).
\end{itemize}

در عمل، ماتریس ماسک \( M \) به شکل زیر خواهد بود (با نشانه‌گذاری پایین‌مثلثی):


\[
M =
\begin{bmatrix}
	0 & -\infty & -\infty & -\infty \\
	0 & 0 & -\infty & -\infty \\
	0 & 0 & 0 & -\infty \\
	0 & 0 & 0 & 0
\end{bmatrix}
\]

به این ترتیب، پس از جمع شدن با \( S \) و اجرای \texttt{softmax} در هر سطر، ضرایب توجه ستون‌های ماسک‌شده به صفر میل می‌کنند \cite{vaswani2017attention}.

 
\section{\lr{vision transformer}}

ایدهٔ ترانسفورمرها در حوزهٔ بینایی (\lr{Vision}) از تعمیم ترانسفورمر متن به تصاویر به وجود آمده است \cite{dosovitskiy2020image}.

ما در این بخش از \lr{Vision Transformer} برای وظیفهٔ \lr{Classification} (کلاس‌بندی) استفاده می‌کنیم.

در روش‌های متداول برای پردازش تصویر، از \lr{convolution}‌های متوالی استفاده می‌کردند؛ اما در ترانسفورمرها تصاویر به پچ‌های مختلف شکسته می‌شوند \cite{dosovitskiy2020image}. هر پچِ شکسته‌شده از تصویر می‌تواند با سایر پچ‌ها به‌صورت موازی وارد مکانیزم توجه (\lr{Attention}) شود و شباهت یا ارتباطشان با یکدیگر سنجیده شود. در بخش‌های بعد، به‌طور مفصل روند انجام این کار را توضیح خواهیم داد.


\subsection{\lr{patch embedding in vision transformer}}
در ترانسفورمرهای مبتنی بر متن، هر کلمه به توکن تبدیل می‌شود و سپس هر کلمه به برداری تبدیل می‌گردد. این بردارها پس از افزودن \lr{positional embedding} وارد مکانیزم \lr{Attention} می‌شوند \cite{vaswani2017attention}. 

حال همین ایده در تصویر پیاده‌سازی شده است. همان‌طور که در شکل \ref{fig:image to patch in vision transformer} مشاهده می‌کنید، در \lr{Vision Transformer}، به‌جای استفاده از عملیات کانولوشن‌های متوالی که در شبکه‌های CNN مرسوم است \cite{lecun1998gradient,krizhevsky2012imagenet,he2016deep}، تصویر را به بلاک‌های غیرهم‌پوشان (\(P \times P\)) تقسیم می‌کنیم. این کار علاوه بر ساده‌سازی موازی‌سازی، به مدل اجازه می‌دهد از سازوکار \lr{Self-Attention} برای ارتباط بین این بلاک‌ها استفاده کند \cite{dosovitskiy2020image}.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/image_patch_embedding.png}
		\caption{image to patch}
		\label{fig:image to patch in vision transformer}
	\end{minipage}
	\hfill
\end{figure}

\subsection{شکل پچ‌ها:}
فرض کنید ابعاد تصویر ورودی (\(H \times W \times C\)) باشد. به‌عنوان مثال، اگر اندازهٔ تصویر \(224 \times 224 \times 3\) باشد، طول و عرض تصویر به‌ترتیب \(224\) و تصویر دارای سه کانال رنگی است:

\[
H = 224, \quad W = 224, \quad C = 3
\]

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/space_original_image.png}
		\caption{original Image}
		\label{fig:Original Image}
	\end{minipage}
	\hfill
\end{figure}

حال اگر اندازهٔ هر پچ (\(P \times P\)) باشد (برای نمونه \(16 \times 16\))، تصویر به‌صورت یک جدول مشبک از پچ‌های کوچک تقسیم می‌شود.  
به هر پچ می‌توان مانند یک «کاشی» از تصویر نگاه کرد:
- پچ اول: مختصات \((0 \text{ تا } 15 \text{ در ارتفاع}) \) و \((0 \text{ تا } 15 \text{ در عرض})\)،  
- پچ دوم: مختصات \((0 \text{ تا } 15 \text{ در ارتفاع}) \) و \((16 \text{ تا } 31 \text{ در عرض})\)،  
- و به همین ترتیب تا کل تصویر پوشش داده شود.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/space_patch_iamge.png}
		\caption{\lr{paches of image}}
		\label{fig:Patch Image}
	\end{minipage}
	\hfill
\end{figure}

\subsection{تعداد پچ‌ها:}
اگر پچ‌ها بدون هم‌پوشانی باشند، ابعاد پچ باید بر ابعاد تصویر بخش‌پذیر باشد.  

- تعداد پچ‌های افقی: \(\frac{W}{P}\)  
- تعداد پچ‌های عمودی: \(\frac{H}{P}\)  

در مجموع:  
\begin{equation}
	\left(\frac{H}{P}\right) \times \left(\frac{W}{P}\right) = \frac{H}{P} \times \frac{W}{P}.
	\label{eq:grid_size}
\end{equation}


برای مثال اگر:
\[
H = 224, \quad W = 224, \quad P = 16:
\]
\[
\frac{224}{16} = 14 \quad \Rightarrow \quad 14 \times 14 = 196 \quad (\text{تعداد پچ‌ها}).
\]

در اکثر نسخه‌های مبدل‌های بینایی، پچ‌ها بدون هم‌پوشانی (\lr{Non-overlapping}) هستند. اندازهٔ پچ‌های کوچک باعث می‌شود تعداد پچ‌ها زیاد شود و در نتیجه هزینهٔ \lr{Attention} بالا رود. از طرفی، پچ‌های بزرگ هزینهٔ \lr{Attention} را کاهش می‌دهند؛ اما ممکن است جزییات محلی (\lr{local details}) را از دست بدهیم \cite{dosovitskiy2020image}.

\subsection{بردارکردن هر پچ}
هر پچ دارای ابعاد \((P \times P \times C)\) است. برای مثال اگر \(P=16\) و \(C=3\)، آنگاه پچ ابعاد \(16 \times 16 \times 3\) خواهد داشت.  
برای این‌که بتوانیم پچ‌ها را مانند «توکن»‌های \lr{NLP} به مدل ترانسفورمر بدهیم، باید آن‌ها را به یک بردار یک‌بعدی تبدیل کنیم. در صورت قرار دادن پیکسل‌های پچ به‌صورت ردیفی (Row-major)، طول این بردار خواهد بود:

\begin{equation}
	P \times P \times C = P^2 \times C.
	\label{eq:patch_volume}
\end{equation}

در مثال \((16 \times 16 \times 3)\)، طول بردار می‌شود \(768\).


\section{اعمال لایهٔ خطی (\lr{Projection})}
بعد از \lr{Flatten} کردن، معمولاً یک لایهٔ خطی (\lr{Fully-Connected Layer}) روی این بردار اعمال می‌شود تا آن را به بعد \(d_{\text{model}}\) (مثلاً 768 یا 1024) ببرد. در حقیقت، این لایه یک تبدیل ویژگی (\lr{Feature Transformation}) انجام می‌دهد تا همهٔ پچ‌ها یک نمایندگی (\lr{Embedding}) با ابعاد یکنواخت \(d_{\text{model}}\) پیدا کنند:
\[
(P^2 \times C) \quad \rightarrow \quad d_{\text{model}}.
\]

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/vision_transformer_embedding.png}
		\caption{\lr{Embedding in Vision Transformer}}
		\label{fig:Embedding Vision Transformer}
	\end{minipage}
	\hfill
\end{figure}

این مرحله شبیه ساخت توکن در \lr{NLP} است؛ با این تفاوت که در \lr{NLP}، توکن «کلمه» یا «زیرکلمه» است و از قبل دارای بردار تعبیه‌شده (\lr{Embedding}) بوده است \cite{vaswani2017attention}. در \lr{Vision Transformer} \cite{dosovitskiy2020image}، ما ابتدا باید تصاویر را پچ کنیم و سپس بردارهای \lr{Embedding}‌شده را از این پچ‌ها به دست آوریم.

ترانسفورمر نیاز دارد ورودی‌اش توالی توکن‌ها باشد. در \lr{NLP} توالی کلمات داریم، در \lr{ViT} توالی «پچ»‌ها:
\[
\{ x_{\text{patch}_1}, x_{\text{patch}_2}, \dots, x_{\text{patch}_N} \}.
\]
هر پچ اکنون یک بردار \(d_{\text{model}}\)-بعدی است. پس یک مجموعه با طول \(N\) (تعداد پچ‌ها) و عرض \(d_{\text{model}}\) خواهیم داشت.  
اگر عدد پچ‌ها \(N\) باشد (مثلاً 196)، ترانسفورمر می‌تواند با مکانیزم \lr{Self-Attention}، وابستگی (\lr{Relations}) میان پچ‌ها را یاد بگیرد: کدام بخش از تصویر برای کدام بخش دیگر مهم‌تر است، چگونه ترکیب جهانی (\lr{Global Context}) شکل گیرد و غیره \cite{vaswani2017attention,dosovitskiy2020image}.

معمولاً پچ‌ها را به‌صورت ردیفی (\lr{Row by Row}) شماره‌گذاری می‌کنند (ابتدا پچ‌های ردیف بالایی از چپ به راست، سپس ردیف بعدی و …)، تا مدل در صورت نیاز بتواند از موقعیت‌ها، اطلاعات مکانی تقریبی داشته باشد.  
در عمل، چون قصد داریم (در مراحل بعد) به هر پچ یک \lr{Positional Embedding} هم اضافه کنیم، مکان دقیق هر پچ در بُعد دوم (ویژگی) کد می‌شود.

در ویژن ترانسفورمر \cite{dosovitskiy2020image} دیگر به کانولوشن وابسته نیستیم. در عوض، از \lr{Embedding} استفاده می‌شود.  
\lr{Split} کردن تصویر به بلاک‌های \((P \times P)\)، \lr{Flatten} و \lr{Linear Projection} همگی عملیات ریاضی ساده‌ای هستند که به‌راحتی روی \lr{GPU}/\lr{TPU} قابل موازی‌سازی‌اند.

\subsection{\lr{CLS Token}}
\lr{CLS Token} یک بردار ویژه است که به ابتدای دنبالهٔ ورودی اضافه می‌شود و نقش آن، خلاصه‌کردن اطلاعات کل ورودی (چه متن، چه تصویر) است \cite{devlin2018bert,dosovitskiy2020image}.

در ویژن ترانسفورمر، این توکن در ابتدای پچ‌های تصویری قرار می‌گیرد.  
این توکن یک بردار با ابعاد \(d_{\text{model}}\) است (همان ابعاد سایر توکن‌ها) و پارامتری یادگرفتنی محسوب می‌شود؛ یعنی مدل طی آموزش، مقادیر آن را برای ذخیره و تجمیع اطلاعات بهینه می‌کند.

در وظایف دسته‌بندی (\lr{Classification})، هدف این است که یک پیش‌بینی کلی برای کل ورودی (مثلاً یک جمله یا یک تصویر) ارائه دهیم؛ \lr{CLS Token} دقیقاً همین وظیفه را بر عهده دارد \cite{devlin2018bert}. این توکن از طریق مکانیزم \lr{Self-Attention} در ترانسفورمر با تمامی توکن‌های دیگر (پچ‌های تصویر) ارتباط می‌گیرد و اطلاعات مهم آن‌ها را در لایه‌های مختلف ترانسفورمر به‌صورت تجمعی یاد می‌گیرد. به عبارتی، \lr{CLS Token} نقش نمایندهٔ کل تصویر یا متن را بر عهده دارد.

\lr{CLS Token} از طریق ضرب داخلی در مکانیزم \lr{Attention}، می‌تواند به تمام پچ‌ها نگاه کند و با ضرایب توجه (\(\alpha\)) مشخص کند که از هر پچ چه مقدار اطلاعات بگیرد. بدین‌ترتیب، به‌طور ضمنی یاد می‌گیرد روی ویژگی‌هایی که برای دسته‌بندی مهم هستند (نظیر الگوها، اشکال و بخش‌های کلیدی تصویر) متمرکز شود.

در طول لایه‌های ترانسفورمر، \lr{CLS Token} نقش محوری در خلاصه‌سازی بازنمایی کل تصویر ایفا می‌کند. این توکن به‌صورت پارامتر قابل یادگیری تعریف شده و در طول فرآیند آموزش به‌روزرسانی می‌شود \cite{devlin2018bert,dosovitskiy2020image}.

\subsection{\lr{Encoder in vision transformer}}

انکودر در ترانسفورمرها همانند ترانسفورمر اصلی است \cite{vaswani2017attention}، با این تفاوت که در \lr{Vision Transformer} \cite{dosovitskiy2020image} دیگر به دیکودر نمی‌رویم. پس از عبور از بلاک‌های ترانسفورمر، در ساده‌ترین حالت یک لایهٔ خطی (\lr{Fully Connected}) یا یک لایهٔ \lr{MLP (Multi-Layer Perceptron)} بر روی بردار نهایی اعمال می‌شود و این لایه‌ها به تعداد کلاس‌ها خروجی می‌دهند.  
سپس خروجی هر لایه با گذر از تابع \lr{softmax} به احتمال هر کلاس تبدیل می‌شود و در نهایت مدل کلاس با بیشترین احتمال را به‌عنوان خروجی پیش‌بینی می‌کند.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/vision_transformer_after_embedding.png}
		\caption{\lr{Cls Token in Vision Transformer}}
		\label{fig:Cls Token In Vision Transformer}
	\end{minipage}
	\hfill
\end{figure}

در ترانسفورمرها، هر لایهٔ انکودر و دیکودر با پردازش عمیق‌تر روی توالی ورودی، می‌تواند نمایش بهتری از ویژگی‌ها به‌دست بیاورد \cite{vaswani2017attention}. تکرار چندین‌بارهٔ \lr{Encoder} یا \lr{Decoder} موجب می‌شود مدل بتواند ساختارهای پیچیده‌ای را یاد بگیرد و کیفیت و دقت آن در شناسایی توالی‌های طولانی و معانی پنهان افزایش یابد \cite{vaswani2017attention,dosovitskiy2020image}. در نتیجه، مدل با تعداد لایه‌های بیشتر اغلب عملکرد بهتری از خود نشان می‌دهد.

   
   
\section{Swin Transformer:}
ایدهٔ \lr{Swin Transformer} از ترکیب چند مفهوم کلیدی در مدل‌های ترانسفورمر و شبکه‌های کانولوشنی شکل گرفت \cite{vaswani2017attention,he2016deep,liu2021swintransformer}.  
یکی از بزرگترین مشکلات در ترانسفورمرهای اولیه، نیاز به محاسبات بسیار زیاد در زمانی بود که تصویر ورودی ابعاد بسیار بزرگی داشت \cite{dosovitskiy2020image}. در ترانسفورمر معمولی هر پچ به تمامی پچ‌های دیگر توجه (\lr{attention}) می‌کرد و در مواقعی که تعداد پچ‌ها زیاد می‌شد، هزینهٔ محاسباتی و حافظه به‌شدت افزایش پیدا می‌کرد.

در شبکه‌های کانولوشنی، معماری معمولاً به‌صورت سلسله‌مراتبی پیش می‌رود \cite{he2016deep}؛ یعنی ابتدا ویژگی‌های محلی استخراج می‌شود، سپس با عمیق‌تر شدن لایه‌ها، این ویژگی‌ها در سطوح بالاتر با یکدیگر ترکیب می‌شوند. در \lr{Swin Transformer} \cite{liu2021swintransformer}، با دانش بر این موضوع توانسته‌اند هم هزینه‌های محاسباتی را کاهش دهند و هم دقت مدل را افزایش دهند.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{1\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/swin_transformer.png}
		\caption{\lr{Swin Transformer}}
		\label{fig: swin transformer}
	\end{minipage}
	\hfill
\end{figure}

در \lr{Swin Transformer}، به‌جای آن‌که مدل به تمام پچ‌ها در یک سطح ویژگی نگاه کند، تصویر را به «پنجره‌های محلی» (\lr{Local Windows}) تقسیم می‌کند و توجه را محدود به همان ناحیه می‌سازد \cite{liu2021swintransformer}. سپس با تکنیک جابه‌جایی (\lr{Shift}) این پنجره‌ها در لایه‌های بعدی، توان مدل برای ترکیب اطلاعات از نواحی مختلف تصویر (و در نهایت دیدن کل تصویر) افزایش پیدا می‌کند. این رویکرد، ایدهٔ کلیدی‌ای بود که باعث شد مدل هم محاسبات سبک‌تری داشته باشد و هم بتواند ارتباط‌های جهانی (\lr{Global}) را در طول لایه‌ها به‌دست آورد.

یکی دیگر از ایده‌های مهم در \lr{Swin}، کوچک‌کردن تدریجی نقشهٔ ویژگی (\lr{Feature Map}) در طول معماری است؛ مشابه کاری که در \lr{ResNet} یا سایر \lr{CNN}ها انجام می‌شود \cite{he2016deep}. این امر ضمن کاهش هزینهٔ محاسباتی، باعث می‌شود مدل بتواند با سطوح مختلفی از ویژگی‌ها کار کند و در نهایت خروجی نهایی باکیفیت‌تری ارائه دهد.

\subsection{قطعه‌بندی پچ (\lr{Patch Partition})}
فرض کنیم تصویر ورودی \(\displaystyle I\) دارای ابعاد \(\displaystyle (H \times W \times 3)\) باشد. گام نخست، تقسیم تصویر به پچ‌های کوچک \(\displaystyle (P \times P)\) است \cite{dosovitskiy2020image}. اگر \(P\) اندازهٔ پچ (\lr{Patch size}) باشد، آنگاه تعداد پچ‌ها در بعد افقی و عمودی، به‌ترتیب \(\displaystyle \frac{H}{P}\) و \(\displaystyle \frac{W}{P}\) خواهد بود. هر پچ را می‌توان به‌صورت یک بردار درآورد:

\[
X_{\text{patch}} \in \mathbb{R}^{(P^2 \cdot 3)}.
\]

سپس کل تصویر به \(\displaystyle \frac{H}{P} \times \frac{W}{P}\) پچ تبدیل خواهد شد و در نتیجه، ماتریس \(\displaystyle X\) از کنار هم قرار گرفتن این پچ‌ها به صورت زیر به‌دست می‌آید:

\[
X \in \mathbb{R}^{\Bigl(\frac{H}{P} \cdot \frac{W}{P}\Bigr) \times \Bigl(P^2 \cdot 3\Bigr)}.
\]

\subsection{\lr{Linear Embedding}}
در ادامه، برای این‌که بتوانیم هر پچ را در یک فضای برداری با بعد \(\displaystyle C\) (ابعاد مدل) نمایش دهیم، یک لایهٔ خطی (\lr{Fully Connected Layer}) روی هر پچ اعمال می‌شود \cite{dosovitskiy2020image,liu2021swintransformer}:

\[
Z = X \cdot W_{\text{embed}} + b_{\text{embed}}, 
\quad
Z \in \mathbb{R}^{\Bigl(\tfrac{H}{P} \cdot \tfrac{W}{P}\Bigr) \times C}.
\]

در عمل، این عملیات معادل یک تبدیل خطی ساده است:

\[
W_{\text{embed}} \in \mathbb{R}^{\bigl(P^2 \cdot 3\bigr) \times C},
\quad
b_{\text{embed}} \in \mathbb{R}^{C}.
\]

پس از این مرحله، ما در هر موقعیت \((h, w)\) (از شبکهٔ پچ‌ها) یک بردار 
\(\displaystyle z_{h,w} \in \mathbb{R}^{C}\) داریم. این ماتریس \(\displaystyle Z\) 
ورودیِ اولین مرحله (\lr{Stage}) از \lr{Swin Transformer} خواهد بود \cite{liu2021swintransformer}.

هر بلوک \lr{Swin Transformer} از چند بخش اصلی تشکیل شده است \cite{liu2021swintransformer}:
\begin{itemize}
	\item پنجره‌بندی تصویر (\textbf{\lr{Window Partition}}) 
	یا پنجره‌بندی جابه‌جاشده (\textbf{\lr{Shifted Window Partition}})
	\item اعمال \(\mathrm{WMSA}\) (\textbf{\lr{Window Multi-Head Self Attention}})
	\item لایهٔ \textbf{\lr{Skip Connection}} و \textbf{\lr{Layer Norm}}
	\item مسیر \textbf{\lr{MLP}}:
	\begin{itemize}
		\item یک لایهٔ \lr{MLP} شامل دو لایهٔ \textbf{\lr{Fully-Connected}} 
		و تابع فعال‌ساز \textbf{\lr{GeLU}} (یا تابع مشابه)
		\item لایهٔ \textbf{\lr{Skip Connection}} و \textbf{\lr{Layer Norm}}
	\end{itemize}
\end{itemize}



\subsection{\lr{Window Multi-Head Self-Attention}}

\subsubsection{تعریف پنجره‌های محلی}

در \lr{Swin Transformer}، به‌جای آن‌که تمام پیکسل‌های یک نقشهٔ ویژگی بزرگ را یک‌جا 
در محاسبهٔ \lr{Attention} درگیر کنیم، نقشهٔ ویژگی را به قطعه‌های کوچکی به‌اندازهٔ 
\(\displaystyle (M \times M)\) تقسیم می‌کنیم. این قطعه‌های کوچک را 
«پنجره‌های محلی» می‌نامیم.

اگر اندازهٔ نقشهٔ ویژگی در یک لایه 
\(\displaystyle (H' \times W')\) باشد، 
با تقسیم آن به پنجره‌های 
\(\displaystyle (M \times M)\)، 
در راستای طول تقریباً 
\(\displaystyle \tfrac{H'}{M}\) پنجره خواهیم داشت 
و در راستای عرض هم 
\(\displaystyle \tfrac{W'}{M}\) پنجره.
(برای راحتی، فرض می‌کنیم 
\(\displaystyle H'\) و \(\displaystyle W'\) 
دقیقاً مضربی از \(\displaystyle M\) باشند 
تا تقسیم بدون باقی‌مانده انجام شود.)

هر کدام از این پنجره‌های 
\(\displaystyle (M \times M)\) 
دارای 
\(\displaystyle M^2\) پیکسل (یا موقعیت مکانی) است، 
و در هر پیکسل هم یک بردار ویژگی با بعد \(\displaystyle C\) قرار دارد.

به بیان ساده‌تر:
\begin{itemize}
	\item نقشهٔ ویژگی مثل یک صفحهٔ بزرگ است.
	\item آن را مانند شطرنج به مربع‌های کوچکی \(\displaystyle (M \times M)\) بخش می‌کنیم.
	\item در هر مربع (پنجره)، فقط به همان مربع نگاه می‌کنیم و محاسبات \lr{Attention} را انجام می‌دهیم.
	\item این کار باعث می‌شود تعداد پیکسل‌هایی که درگیر محاسبهٔ \lr{Attention} هستند، 
	به‌مراتب کمتر شود و هزینهٔ محاسباتی کاهش یابد.
\end{itemize}


\subsection{\lr{Attention}}

برای هر بلوک، ابتدا بردارهای \textbf{\lr{Query}}، \textbf{\lr{Key}} و \textbf{\lr{Value}} ساخته می‌شوند. 
اگر \(\displaystyle z_i \in \mathbb{R}^C\) بردار ورودی مربوط به موقعیت \(i\) باشد، آنگاه:

\[
q_i = z_i W_Q, 
\quad
k_i = z_i W_K,
\quad
v_i = z_i W_V,
\]
که 
\[
W_Q, W_K, W_V \;\;\in \;\;\mathbb{R}^{C \times d}.
\]

پارامتر \(\displaystyle d\) معمولاً به‌صورت \(\displaystyle \tfrac{C}{h}\) در نظر گرفته می‌شود 
که در آن \(\displaystyle h\) تعداد سربندی (\lr{Head})ها است. 
در \textbf{\lr{Multi-Head Attention}}، خروجی نهایی با ترکیب \(\displaystyle h\) سر توجه محاسبه می‌شود.

در یک سر توجه، توجه به‌صورت زیر تعریف می‌شود:

\[
\mathrm{Attention}(Q, K, V)
=
\mathrm{Softmax}\Bigl(\frac{QK^\top}{\sqrt{d}}\Bigr)\,V,
\]

که در آن:

\begin{itemize}
	\item \(\displaystyle Q, K, V\) به‌ترتیب ماتریس‌هایی هستند که از کنار هم قرار دادن 
	\(\displaystyle q_i, k_i, v_i\) (برای تمام پیکسل‌های آن پنجره) ساخته می‌شوند.
	\item \(\displaystyle \sqrt{d}\): عامل مقیاس‌کننده برای جلوگیری از بزرگ شدن بیش‌ازحد ضرب داخلی است.
\end{itemize}

در \textbf{\lr{Swin Transformer}}، این محاسبات به‌صورت پنجره‌ای انجام می‌شوند؛ یعنی 
برای هر پنجره، تنها پیکسل‌های داخل همان پنجره در ماتریس‌های 
\(\displaystyle Q\)، \(\displaystyle K\) و \(\displaystyle V\) لحاظ می‌شوند. 
به این ترتیب، زمان محاسبه و مصرف حافظه به‌شدت کاهش می‌یابد 
(در مقایسه با \textbf{\lr{ViT}} که همه‌چیز را با هم مقایسه می‌کند).

تعداد سربندی \(\displaystyle h\) معمولاً طوری انتخاب می‌شود که 
\(\displaystyle C = h \times d.\) 
خروجی هر سر پس از محاسبهٔ \lr{Attention} به‌صورت زیر با هم ادغام می‌شوند:

\[
\mathrm{MultiHead}(Q,K,V) 
= 
\bigl[\text{head}_1,\ \text{head}_2,\ \dots,\ \text{head}_h\bigr]\,
W_O,
\]
که 
\[
\text{head}_j = \mathrm{Attention}\bigl(Q_j,\ K_j,\ V_j\bigr),
\quad 
W_O \in \mathbb{R}^{C \times C}
\]
ماتریس ترکیب نهایی است.

\subsection{\lr{shifted Windows}}

در \textbf{\lr{Swin Transformer}}، ایدهٔ «پنجره‌های جابه‌جاشده» (\textit{\lr{Shifted Windows}})
به این منظور ارائه شده است تا مدل، ارتباط پیکسل‌های واقع در پنجره‌های مجاور را هم یاد بگیرد \cite{liu2021swintransformer}.
اگر فقط از پنجره‌های ثابت (بدون جابه‌جایی) استفاده کنیم، هر بلوک از تصویر تنها با پیکسل‌های همان 
پنجره در ارتباط خواهد بود و ممکن است اطلاعات نواحی مرزی با نواحی مجاور به‌خوبی تبادل نشود.

روش \textbf{\lr{Swin}} برای رفع این محدودیت از یک تکنیک ساده اما مؤثر استفاده می‌کند \cite{liu2021swintransformer}:
\begin{itemize}
	\item در یک لایه، محاسبات \lr{\lr{Attention}} در پنجره‌های محلی ثابت انجام می‌شود.
	\item در لایهٔ بعدی، پنجره‌ها به اندازه‌ای مشخص جابه‌جا می‌شوند (به‌صورت شیفت افقی و عمودی) 
	تا نواحی مرزی نیز در محاسبات گنجانده شوند.
	\item این فرآیند باعث می‌شود که پیکسل‌ها در پنجره‌های مختلف (و در مرزهای مختلف) 
	در محاسبات دخیل شوند و تبادل اطلاعات بهتری میان نواحی تصویر رخ دهد.
\end{itemize}

\subsubsection{بلوک اول (\lr{\lr{W-MSA}}):}
در این بلوک، نقشهٔ ویژگی به پنجره‌های \(\displaystyle (M \times M)\) تقسیم می‌شود \cite{liu2021swintransformer}.
هیچ جابه‌جایی در این تقسیم‌بندی وجود ندارد؛ یعنی اگر نقشهٔ ویژگی را یک مستطیل بزرگ در نظر بگیریم،
آن را شبیه کاشی‌کاری یا شطرنج‌بندی به بلوک‌های مربعی \(\displaystyle (M \times M)\) برش می‌زنیم.
در این حالت، پیکسل‌های هر پنجره فقط با همدیگر (درون همان پنجره) ارتباط برقرار می‌کنند.

\subsubsection{بلوک دوم (\lr{\lr{SW-MSA}}):}
مطابق شکل \ref{fig:window vs Shifted Window in Swin Transformer}، بعد از اینکه بلوک اول کارش تمام شد، در بلوک دوم، قبل از تقسیم‌بندی به پنجره‌های 
\(\displaystyle (M \times M)\)، نقشهٔ ویژگی را جابه‌جا (\textit{\lr{Shift}}) می‌کنیم \cite{liu2021swintransformer}.
در مقالهٔ اصلی، این مقدار جابه‌جایی معمولاً نیمِ اندازهٔ پنجره 
\(\displaystyle \frac{M}{2}\)
در راستای افقی و عمودی است. به این ترتیب:

\begin{itemize}
	\item پیکسل‌هایی که پیش از این در دو پنجرهٔ جداگانه قرار داشتند، ممکن است حالا به دلیل جابه‌جایی وارد یک پنجرهٔ مشترک شوند.
	\item مدل حالا می‌تواند بین این پیکسل‌های «مرزی» نیز \lr{\lr{Attention}} برقرار کند و اطلاعات را بهتر مبادله کند.
\end{itemize}

با این جابه‌جایی، بخشی از پیکسل‌ها در نقشهٔ ویژگی از یک طرف «خارج» می‌شوند. برای اینکه این پیکسل‌ها را از دست ندهیم، از ترفندی به نام \lr{\lr{Cyclic Shift}} استفاده می‌شود. در \lr{\lr{Cyclic Shift}}، پیکسل‌هایی که از سمت راست بیرون می‌روند دوباره از سمت چپ وارد می‌شوند و بالعکس؛ درست شبیه وقتی که یک تصویر را به‌صورت حلقه‌ای اسکرول می‌کنیم (\lr{\lr{Wrap around}}). مثالی از \lr{\lr{Cycle Shift}} در شکل \ref{fig:Cycle Shift in Swin Tranformer} آمده است.

در بلوک اول (بدون جابه‌جایی)، پنجره‌ها ثابت‌اند و پیکسل‌های مرزی در هر پنجره ممکن است فرصت کافی برای تبادل اطلاعات با پیکسل‌های مرزیِ پنجرهٔ کناری را نداشته باشند.

در بلوک دوم (جابه‌جاشده)، مرزهای پنجره‌ها تغییر می‌کند و برخی پیکسل‌هایی که قبلاً در پنجره‌های جدا بودند، اکنون در یک پنجرهٔ مشترک‌اند؛ در نتیجه مدل می‌تواند رابطه و همبستگی بین آن‌ها را هم یاد بگیرد.

این جابه‌جایی و قرارگیری مجدد پیکسل‌ها کنار هم در نهایت کمک می‌کند تا مدل بتواند اطلاعات کل تصویر را با هزینهٔ محاسباتی کمتر (نسبت به توجهِ سراسریِ کامل) در اختیار داشته باشد \cite{liu2021swintransformer}.

اگر بخواهیم با مثال توضیح دهیم، فرض کنید در یک تابلوی شطرنجی، خانه‌های کناری همدیگر را «نمی‌بینند» چون در دو بلوک مختلف هستند.
اما اگر کمی تابلوی شطرنجی را به سمت بالا-چپ یا پایین-راست جابه‌جا کنیم،
حالا بخشی از آن خانه‌ها وارد یک بلوک واحد می‌شوند و اطلاعاتشان با هم ترکیب می‌شود.
سپس به‌طور دوره‌ای (\textit{\lr{Cyclic}})، گوشه‌های اضافی را به آن سمت دیگر تابلوی شطرنجی می‌آوریم
تا هیچ چیز از دست نرود.

به این شکل، سِری اول و دوم بلوک‌های \textbf{\lr{Swin Transformer}} (\textbf{\lr{W-MSA}} و \textbf{\lr{SW-MSA}})
تکمیل‌کنندهٔ یکدیگر می‌شوند \cite{liu2021swintransformer}:
\begin{itemize}
	\item \textbf{بلوک اول:} محاسبهٔ \lr{\lr{Attention}} در چهارچوب پنجره‌های ثابت.
	\item \textbf{بلوک دوم:} محاسبهٔ \lr{\lr{Attention}} در پنجره‌های جابه‌جاشده که منجر به تعامل بیشتر بین مرزهای مختلف می‌شود.
\end{itemize}


\subsection{Mlp}
پس از انجام \textit{\lr{Shifted Window Multi-Head Self-Attention}}، 
خروجی به یک مسیر \textbf{\lr{MLP}} می‌رود \cite{liu2021swintransformer}. ساختار این \lr{MLP} به‌صورت زیر است:

\begin{equation}
	X' = \mathrm{GELU}(X W_1 + b_1) \; W_2 + b_2,
	\label{eq:gelu_transform}
\end{equation}

که در آن
\[
W_1 \in \mathbb{R}^{C \times (rC)}, 
\quad
W_2 \in \mathbb{R}^{(rC) \times C}
\]
هستند و \(\displaystyle r\) معمولاً ضریب افزایش بعد را نشان می‌دهد (مثلاً ۴). 

تابع فعال‌ساز \lr{GELU} (یا \lr{ReLU} و سایر توابع) نیز در این‌جا قابل استفاده است \cite{hendrycks2016gelu}.

\subsection{\lr{patch merging}}
در مدل \lr{Swin Transformer}، ساختار سلسله‌مراتبی به این معناست که ما در چند مرحله (\lr{Stage}) مختلف، نقشهٔ ویژگی (\lr{Feature Map}) را کوچک‌تر می‌کنیم و در عین حال، عمق (تعداد کانال‌های ویژگی) را افزایش می‌دهیم. هدف اصلی از این کار عبارت است از:

\begin{itemize}
	\item \textbf{استخراج ویژگی‌های سطح بالاتر:}
	وقتی نقشهٔ ویژگی کوچک‌تر می‌شود، هر واحد از نقشهٔ ویژگی بیانگر بخش گسترده‌تری از تصویر اصلی است؛ 
	پس مدل به‌تدریج جزئیات محلی را با درک کلی‌تری از تصویر جایگزین می‌کند \cite{he2016deep}.
	
	\item \textbf{کاهش هزینهٔ محاسبات:}
	در مراحل بعدی، چون ابعاد فضایی کمتر می‌شود، مدل راحت‌تر می‌تواند با ویژگی‌های جدید کار کند 
	(چون مثلاً به‌جای \((H \times W)\) پیکسل، تعداد کمتری پیکسل داریم) \cite{liu2021swintransformer}.
\end{itemize}

این فرایند کوچک‌سازی در \lr{Swin Transformer} با نام 
\lr{Patch Merging} شناخته می‌شود 
که شبیه به \textit{Downsampling} در شبکه‌های کانولوشنی (مثل \textit{Pooling} یا \textit{Stride-Convolution}) عمل می‌کند \cite{liu2021swintransformer}.

پس از چندین بلوک پردازشی، نقشهٔ ویژگی، ابعادی به شکل \((\tfrac{H}{P}, \tfrac{W}{P})\) با تعداد کانال \(\displaystyle C\) دارد. 
این یعنی پس از برش‌دادن تصویر به پچ‌ها و گذر از چند لایه، اکنون یک نقشهٔ ویژگی داریم که کوچک‌تر از تصویر اصلی است، 
اما هنوز ممکن است خیلی بزرگ باشد.

در مرحلهٔ بعد (\lr{Stage} بعدی)، می‌خواهیم این نقشه را نصف کنیم 
(یعنی طول و عرض را دو برابر کوچک کنیم) و در عوض عمق کانال را دو برابر کنیم 
(تا ظرفیت مدل در استخراج ویژگی‌های پیچیده‌تر بیشتر شود). برای انجام این کار از فرایندی به نام 
\lr{Patch Merging} استفاده می‌کنیم \cite{liu2021swintransformer}:

\subsubsection{1. انتخاب بلوک‌های \((2 \times 2)\)}
ابتدا نقشهٔ ویژگی را در بُعد مکانی به بلوک‌های \((2 \times 2)\) تقسیم می‌کنیم.  
اگر \(\displaystyle Z_{i,j}\) ویژگیِ مکان \((i, j)\) باشد، 
یک بلوک \((2 \times 2)\) شامل چهار پیکسل است:
\[
Z_{2i, 2j}, \quad Z_{2i, 2j+1}, \quad Z_{2i+1, 2j}, \quad Z_{2i+1, 2j+1}.
\]

\subsubsection{2. ادغام (Concat) ویژگی‌های چهار پیکسل}
برای هر بلوک \((2 \times 2)\)، این چهار پیکسل را در بُعد کانال به هم می‌چسبانیم (\lr{Concat}).  
اگر هر پیکسل یک بردار از بعد \(\displaystyle C\) باشد، اکنون بعدِ حاصل از کنار هم گذاشتن این چهار پیکسل می‌شود \(\displaystyle 4C\).  
نام این بردار ادغام‌شده را \(\displaystyle Z'\) می‌گذاریم.

\subsubsection{3. لایهٔ خطی برای تغییر بعد}
وقتی چهار بردار \(\displaystyle C\)-بعدی را کنار هم می‌گذاریم، یک بردار \(\displaystyle 4C\)-بعدی شکل می‌گیرد.  
حال با یک لایهٔ خطی (\textit{Fully Connected})، بعدِ \(\displaystyle 4C\) را به بعد جدیدی تبدیل می‌کنیم.  
معمولاً این بعد جدید برابر \(\displaystyle 2C\) در نظر گرفته می‌شود؛ 
یعنی دو برابر بزرگ‌تر از قبل اما نه چهار برابر:
\begin{equation}
	Z' \mapsto Z'' = Z' \, W_{\text{merge}} + b_{\text{merge}},
	\label{eq:merge_transform}
\end{equation}

که بعد ویژگی را از \(\displaystyle 4C\) به \(\displaystyle 2C\) کاهش می‌دهد.

\subsubsection{4. کاهش ابعاد مکانی}
در عین حال، وقتی هر چهار پیکسل \((2 \times 2)\) را ادغام می‌کنیم، 
نقشهٔ ویژگی ما ابعاد فضایی \(\bigl(\tfrac{H}{2P} \times \tfrac{W}{2P}\bigr)\) خواهد داشت 
(چون هر بلوک \((2 \times 2)\) تبدیل به یک بردار می‌شود).

به عبارت دیگر، تعداد نقاط مکانی نصف می‌شود (هم در طول و هم در عرض)، 
اما کانال از \(\displaystyle C\) به \(\displaystyle 2C\) افزایش می‌یابد.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{1\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/patch_merging.png}
		\caption{Patch Merging}
		\label{fig:patch merging in Swin Transformer}
	\end{minipage}
	\hfill
\end{figure}

در شبکه‌های کانولوشنی، مرتبا از لایه‌های \textit{Pooling} یا \textit{Stride-Convolution} 
برای کوچک‌کردن (\lr{Downsampling}) ابعاد استفاده می‌شود تا اطلاعات سطح بالاتر (مثل ساختار کلی اشیا) راحت‌تر استخراج شود \cite{he2016deep}.  
\lr{Swin Transformer} هم همین ایدهٔ سلسله‌مراتب را به دنیای ترانسفورمرها آورده است \cite{liu2021swintransformer}.  
همچنین اگر ابعاد فضایی را کم نکنیم، هزینهٔ \lr{Attention} به‌شدت زیاد می‌شود 
(چون باید در هر لایه برای همهٔ پیکسل‌ها \lr{Attention} محاسبه گردد).

در معماری کلی \lr{Swin Transformer}، پس از \lr{Stage 1} و عبور از بلوک‌های \textbf{W-MSA} و \textbf{SW-MSA}، عملیات \lr{Patch Merging} انجام می‌شود. سپس در \lr{Stage 2}، ویژگی‌های کوچک‌تری داریم، اما تعداد کانال‌ها افزایش یافته است \cite{liu2021swintransformer}.  
مشابه معماری‌های کانولوشنی، با افزایش \lr{Depth}، ابعاد فضایی کاهش و تعداد کانال‌ها افزایش پیدا می‌کند.

در انتهای \lr{Stage} آخر، خروجی به یک لایهٔ \lr{FC} داده می‌شود تا تعداد کلاس‌ها (\( \mathrm{num\_classes} \)) را پیش‌بینی کند.  
پس از گذر از \lr{Softmax}، احتمال هر کلاس به‌دست می‌آید و مدل در نهایت کلاس نهایی را برمی‌گزیند.






