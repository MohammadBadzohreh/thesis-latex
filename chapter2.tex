\texttt{}\chapter{پیشینه پژوهش}

\section{مقدمه}
ظهور مدل‌های Transformer و انقلاب در یادگیری عمیق، یکی از تحولات اساسی در حوزه پردازش زبان طبیعی (NLP) و یادگیری ماشین به شمار می‌رود. این مدل‌ها باعث تغییرات عمده‌ای در نحوه ساخت و آموزش مدل‌های زبانی و همچنین در بسیاری از کاربردهای دیگر یادگیری ماشین شده‌اند. و توانستند بسیاری از مشکلات مدل های قبلی را حل کنند.

\section{مشکلات ترجمه ماشینی و ترانسفورمرها:}
در ابتدا، ترجمه ماشینی (MT) یک چالش اساسی در زمینه پردازش زبان طبیعی بود. مدل‌های اولیه‌ای مانند مدل‌های مبتنی بر قواعد (Rule-based models) برای ترجمه استفاده می‌شدند که در آن‌ها، ترجمه‌ها به صورت دستی با استفاده از قواعد زبانی مشخص تنظیم می‌شدند.
این روش‌ها هرچند دقیق بودند، اما محدودیت‌های زیادی داشتند و نمی‌توانستند ویژگی‌های پیچیده‌تر زبان را مدل‌سازی کنند.
سپس مدل‌های آماری (Statistical Models) معرفی شدند. این مدل‌ها از داده‌های ترجمه‌شده برای آموزش مدل‌های آماری استفاده می‌کردند که احتمال ترجمه‌ای صحیح را براساس شواهد آماری محاسبه می‌کردند. مدل‌هایی مانند مدل‌های ترجمه آماری مبتنی بر جمله (Phrase-based Statistical Models) از این نوع بودند، که قادر به ترجمه جملات بهتر از مدل‌های مبتنی بر قواعد بودند، اما هنوز هم در ترجمه‌های پیچیده با مشکلاتی روبه‌رو بودند.  بعد از این مدل ها مدل ها بازگشتی به وحود آمدند که مشکلات آن را در فصل گذشته بیان کردیم و د نهایت این مشکلات باعث به وجود آمدن ترانسفورمر ها شد.

\section{ظهور ترانسفورمر ها:}
در سال 2017 مقاله ... توسط گوگل منتشر شد که مفهوم جدیدی به نام ترانسفورمر ها را بیان کرد.
این مقاله به موضوع ترجمه ماشینی پرداخت و نشان داده با استفاده از مفهوم مکانیزم توجه (Attention-Mechanism) می توان بسیاری از مشکلات مدل های قبلی را حل کرد.
این مدل ها از پردازش موازی استفاده میکردند بر خلاف مدل های قبلی که از پردازش های سریالی استفاده میکردند. و این مدل ها قادر هستند به طور همزمان به تمام بخش های ورودی توجه کند.
و این کار باعث شد که ترانسفورمر ها در پردازش تصویر و متن بسیار سریع تر و دقیق تر از مدل های قبلی عمل کنند.


 \section{معماری ترانسفورمر ها:}
 
 در تصویر ... معماری ترانسفورمر نمایش داده شده است  و بخش هاو اجزای مختلف آن مشخص شده است که از دو بخش اصلی انکودر و دیکدر تشکیل شده است هدف انکودر این است که داده ورودی را میگیرد  و ویژگی های آن را استخراج میکند و هدف دیکدر این است که ویژگی های استخراج شده را به زبان مقصد تبدیل میکند. که به طور مختصر به معماری  و بخش های مختلف آن میپردازیم.
 
 \begin{figure}[h]
 	\centering
 	\begin{minipage}[b]{0.45\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{Transformer-model-architecture.png}
 		\caption{معماری ترانسفورمر ها}
 		\label{fig:transformer_architecture}
 	\end{minipage}
 	\hfill

 \end{figure}

\subsection{Embedding:}
در زبان طبیعی کلمات به شکل رشته های متنی هستند مانند کتاب ، ماشین و ... 
کامپیوتر ها نمیتوانند به طور مستقیم به شکل رشته های متنی این کلمات را پردازش کنند. 
به همین دلیل در یادگیری ماشین ما این کلمات را به شکل یک بردار نمایش می دهیم. و آن بردار بیانگر آن کلمه در مدل است تا ماشین بتواند آن کلمه را پردازش کند.

این بردار ها ویژگی های کلمه را فضای عددی نمایش میدهد. 
روش های مختلفی برای تبدیل متن به بردار وجود دارند.
از جمله این روشها میتوان به روش Word2Vec و GloVe  اشاره کرد.
همانطور که در تصویر ... نشان داده شده است، هر کلمه که به صورت توکن است ابتدا در دیکشنری تعریف شده پیدا می شود و پس از پیدا شدن در دیکشنری با استفاده از روش های embedding، هر کلمه به برداری از اعداد تبدیل می شوند.
این embedding  ها شباهت های معنایی بین کلمات را مدل سازی میکنند به طوری که کلماتی که از نظر معنایی شبیه به هم هستند، بردار آن ها نیز به یک دیگر نزدیک تر است. و به این ترتیب کلمات برای مدل ها و شبکه های عصبی قابل فهم می شود.



 \begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/word_embedding.png}
		\caption{word embedding}
		\label{fig:word_embedding}
	\end{minipage}
	\hfill
	
\end{figure}



\subsection{positional embedding:}

ما تا الان هر کلمه را به برداری از اعداد که برای مدل قابل فهم باشد، تبدیل کرده ایم. 
اما مدل های ترانسفورمر نمیتوانند جایگاه هر کلمه را تشخیص دهند. در مدل های ترانسفومر بر خلاف مدل های بازگشتی  به دلیل این که کلمه های به صورت موازی وارد میشوند نیاز داریم تا جایگاه هر کلمه را بدانیم به طور مثال جمله من تو رو دوست داریم باید به طور دقیق بدانیم کلمه من، کلمه اول حمله است، کلمه تو کلمه دوم جمله است و ...
حال ما باید به مدل توالی این کلمات را بفهمیم، پس ما نیاز داریم به مدل یک سری اطلاعات اضافی بدهیم به طوری که مدل توالی کلمات را یاد بگیرد. روش های مختلفی برای اضافه کردن positional embedding  به مدل وجود دارد که در ترانسفومرر ها از روش Sinusoidal Positional Embedding استفاده میشود.
این روش قابل یادگیری نیست و صرفا از یک سری فرمول های ساده برای positional embedding  استفاده میکند.

برای موقعیت \( pos \) در توالی و بعد \( i \) در فضای برداری، تعبیه موقعیتی به صورت زیر تعریف می‌شود:

\[
PE(pos, 2i) = \sin\left( \frac{pos}{10000^{\frac{2i}{d}}} \right)
\]

و برای مقادیر فرد:

\[
PE(pos, 2i+1) = \cos\left( \frac{pos}{10000^{\frac{2i}{d}}} \right)
\]







\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/positional_embedding_formula .png}
		\caption{word embedding}
		\label{fig:positional embedding}
	\end{minipage}
	\hfill
	
\end{figure}





- \( pos \): موقعیت کلمه در توالی است. برای مثال، اگر توالی ورودی شامل \( N \) کلمه باشد، موقعیت‌ها از \( 0 \) تا \( N-1 \) تغییر می‌کنند. به عبارت دیگر، \( pos \) می‌تواند هر عددی از مجموعه \( \{0, 1, 2, \dots, N-1\} \) باشد که نشان‌دهنده موقعیت یک کلمه خاص در توالی است.

- \( i \): شاخص بعد در بردار موقعیتی است. این متغیر به اندیس بعدی که موقعیت کلمه در آن نمایش داده می‌شود اشاره دارد. برای مثال، اگر فضای برداری مدل دارای ابعاد \( d \) باشد، \( i \) از \( 0 \) تا \( d-1 \) تغییر می‌کند.

- \( d \): ابعاد فضای برداری مدل است. این مقدار مشخص می‌کند که هر کلمه در توالی به چه تعداد ابعاد در فضای برداری نگاشت می‌شود. به عبارت دیگر، \( d \) نشان‌دهنده تعداد ویژگی‌ها (یا ابعاد) در بردار موقعیتی است.

- \( 10000 \): یک مقدار ثابت است که برای تنظیم مقیاس توابع تناوبی استفاده می‌شود. این مقدار به‌گونه‌ای تنظیم شده است که از نوسانات زیاد جلوگیری کند و همچنین فرکانس‌های مختلفی را برای ابعاد مختلف به وجود آورد.

همانطور که در شکل زیر مشاهده میکنید بعد از embedding  کلمات  positional embedding  به آن اضافه می شود.
که در این روش از توابع سینوس و کسینوس استفاده میشود.
این توابع موقعیت ها را در فضای برداری به گونه ای نگاشت میکنند که مدل بتواند از ترتیب کلمات در توالی آگاه باشد.این ویژگی به مدل کمک میکند تا مدل توالی زمانی را بتواند درک کند و الگو های زمانی را شبیه سازی کند.
از مزایای این مدل میتوان به عدم نیاز به آموزش و توزیع متوازن جایگاه هر کدام از کلمات اشاره کرد.


\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/positional_embedding.png}
		\caption{word embedding}
		\label{fig:word embedding + positional embedding}
	\end{minipage}
	\hfill
	
\end{figure}






\subsection{attention:}



در روش‌های قدیمی (مانند RNN یا lstm)، توالی ورودی (مثلاً یک جمله) معمولاً به‌صورت گام‌به‌گام پردازش می‌شد. اما در ترانسفورمر می‌خواهیم مدلی داشته باشیم که به هر موقعیت (مثلاً یک کلمه) در توالی نگاه کند و به همهٔ موقعیت‌های دیگر نیز به‌صورت موازی دسترسی داشته باشد. به این مفهوم \textbf{توجه} می‌گوییم.
م
به زبان ساده، وقتی توکن (کلمه) $i$ به توکن‌های دیگر نگاه می‌کند، می‌خواهد بداند کدام توکن‌ها برای تفسیر معنای خودش مهم‌ترند.

به طور مثال در جمله  یک گربه روی زمین نشسته است میخواده بداند کلمه گربه به واژه نشستن بیشتر توجه کند یا به زمین، مثلا در این جا فعل نشستن ارتباط نزدیک تری به گربه دارد، و از نظر معنایی مرتبط تر است.




\[
Q = \text{Query (پرسش)}, \quad K = \text{Key (کلید)}, \quad V = \text{Value (مقدار / ارزش)}
\]

در \textbf{Scaled Dot-Product Attention}، ابتدا شباهت یا ارتباط بین \textit{Query} و \textit{Key} را با محاسبهٔ ضرب داخلی (Dot Product) به‌دست می‌آوریم، سپس آن را نرمال می‌کنیم (با تقسیم بر \( d_k \)) و از تابع \textit{softmax} استفاده می‌کنیم تا ضرایب توجه (Attention Weights) را به‌دست آوریم. در نهایت با همین ضرایب، ترکیبی خطی از بردارهای \textit{Value} را می‌گیریم.

فرمول به‌شکل زیر است:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
\]

که در آن:

\[
Q \in \mathbb{R}^{n \times d_k} \quad \text{ماتریس پرسش برای} \, n \, \text{توکن}
\]
\[
K \in \mathbb{R}^{n \times d_k} \quad \text{ماتریس کلید برای} \, n \, \text{توکن}
\]
\[
V \in \mathbb{R}^{n \times d_v} \quad \text{ماتریس مقدار برای} \, n \, \text{توکن}
\]

\[
d_k \quad \text{ابعاد بردارهای پرسش و کلید است (معمولاً} \, d_k = d_{\text{model}} \, \text{در حالت چندسری)}
\]

تقسیم بر \( d_k \) باعث می‌شود مقدار ضرب داخلی در ابعاد بالا خیلی بزرگ نشود و شیب‌ها (Gradients) پایدار بمانند.

\[
\alpha = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)
\]
\(\alpha\) یک ماتریس با ابعاد \( n \times n \) است که سطر \( i \)-ام آن ضرایب توجه برای توکن \( i \) را نشان می‌دهد.

تفسیر ضرایب توجه: هر سطر از \( \alpha \) نشان می‌دهد که توکن فعلی به چه توکن‌هایی در جمله، با چه شدتی توجه می‌کند.



\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/attention.png}
		\caption{Attention}
		\label{fig:attention}
	\end{minipage}
	\hfill
	
\end{figure}





ایدهٔ چندسری:  
به‌جای آنکه فقط یک‌بار \( Q, K, V \) بسازیم و عملیات توجه را انجام دهیم، چندین مجموعهٔ متفاوت \( Q_i, K_i, V_i \) می‌سازیم (هر کدام یک «Head» یا سر نام دارد) و به‌صورت موازی محاسبات Attention را انجام می‌دهیم. سپس خروجی همهٔ این Headها را کنار هم قرار داده (Concat) و در نهایت با یک ماتریس وزن دیگر ضرب می‌کنیم تا به بعد اصلی بازگردیم.

فرمول مربوط به این ایده به‌شکل زیر است:

\[
\text{head}_i = \text{Attention}(Q_i, K_i, V_i)
\]
\[
\text{MultiHead}(Q, K, V) = [\text{head}_1 \oplus \cdots \oplus \text{head}_h] W_O
\]

که در آن \( \oplus \) نشان‌دهندهٔ عمل الحاق (Concatenation) است.

ماتریس وزن \( W_O \) به‌شکل زیر است:

\[
W_O \in \mathbb{R}^{(h \cdot d_v) \times d_{\text{model}}}
\]

که \( W_O \) ماتریسی است که خروجی الحاق‌شده را به بعد \( d_{\text{model}} \) برمی‌گرداند.





\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{transformer_images/multi_head_attention.png}
		\caption{multi head attention}
		\label{fig:attention}
	\end{minipage}
	\hfill
	
\end{figure}



\subsection*{چرا چندین سر؟}

\textbf{مشاهدهٔ چند منظر متفاوت:} هر Head می‌تواند الگوهای گوناگونی از وابستگی‌ها را بیاموزد (مثلاً یک Head می‌تواند یاد بگیرد کلمهٔ فعلی با کلمات همسایهٔ نزدیک خود بیشتر مرتبط شود، یک Head دیگر روی ارتباط با کلماتی در فاصلهٔ دورتری متمرکز باشد، Head دیگر روی مطابقت جنس و تعداد در دستور زبان و ...).

\textbf{افزایش ظرفیت مدل:} با داشتن چند Head، مدل می‌تواند قدرت بیان بیشتری داشته باشد.

\textbf{ابعاد کمتر در هر Head:} در عمل، اگر \( d_{\text{model}} \) مثلاً 512 باشد، و تعداد Headها \( h = 8 \)، آنگاه هر Head ابعادی در حدود \( d_k = 64 \) خواهد داشت؛ و این محاسبات ضرب داخلی را نیز مقیاس‌پذیر و قابل موازی‌سازی می‌کند.


\subsection{Residual Connection (Add):}
در معماری های های مختلف هنگامی که تعداد لایه ها زیاد  میشود، اغلب دچار ناپایداری گرادیان (Vanishing/Exploding Gradients) میشوند. و باعث مشکل در آموزش مدل میشود.
در مدل ترانسفورمر به جای این که خروجی attention  را به صورت مستقیم به لایه بعدی بدهیم، ورودی آن را نیز حفظ کرده و به خروجی اضافه میکنیم.
اگر \( x \) ورودی به زیرماژول و \( \text{SubLayer}(x) \) خروجی آن زیرماژول باشد، در انتهای کار، ما عبارت زیر را محاسبه می‌کنیم:


\[
x + \text{SubLayer}(x)
\]

این جمع به صورت عنصر به عنصر (Element-wise Addition)  انجام میشود.


\subsection{مزایای Residual Connection در ترانسفورمر}

\subsection*{کمک به جریان یافتن گرادیان}

وقتی ورودی مستقیماً به خروجی اضافه می‌شود، مسیری مستقیم برای عبور شیب (گرادیان) به عقب ایجاد می‌گردد.
در صورت نبود این اتصال، اگر شبکه عمیق شود، گرادیان‌ها ممکن است در لایه‌های پایین محو شوندو عملا gradient vanishing رخ میدهد.

\subsection*{حفظ اطلاعات اصلی (هویت ورودی)}

حتی اگر زیرماژول تغییری در اطلاعات ورودی ایجاد کند، با وجود Residual Connection، ورودی اصلی همواره در خروجی نهایی حضور دارد.
این ویژگی باعث می‌شود در صورت ناکافی بودن یادگیری زیرماژول یا در مراحل اولیهٔ آموزش، دست‌کم بخشی از سیگنال/اطلاعات خام به لایه‌های بالاتر برسد.

\subsection*{کاهش ریسک تخریب ویژگی‌ها}

در شبکه‌های عمیق، یکی از مشکلات این است که هر لایه ممکن است بخشی از اطلاعات مفید را تخریب کند. Residual Connection تضمین می‌کند که اگر لایه‌ای به هر دلیل نتوانست الگوی بهینه را یاد بگیرد، اطلاعات قبلی حداقل به صورت دست‌نخورده تا حدی منتقل می‌شود.

\section{Layer Normalization (Norm):}

در یادگیری عمیق، نرمال‌سازی (Normalization) داده‌های یک لایه یا فعال‌سازی‌ها، اغلب به سرعت بخشیدن به همگرایی و پایدار کردن آموزش کمک شایانی می‌کند. شاید معروف‌ترین نوع نرمال‌سازی، \textbf{Batch Normalization} باشد که پیش‌تر در کارهای بینایی (CNNها) بسیار مورداستفاده قرار گرفت.

\textbf{Layer Normalization} روشی جایگزین است که در ترانسفورمر استفاده می‌شود. علت اصلی این انتخاب، ماهیت توالی‌محور (Sequence) بودن داده‌ها در NLP و عدم تمایل به وابستگی به آمار مینی‌بچ است.



\subsection*{تفاوت Layer Norm با Batch Norm}

\subsection*{Batch Normalization:}

در Batch Norm، برای نرمال‌سازی، میانگین و واریانس روی تمام نمونه‌های موجود در مینی‌بچ (و نیز در طول ابعاد ویژگی) محاسبه می‌شود.
این موضوع در NLP کمی دردسرساز است؛ چون ترتیب (Order) توکن‌ها، طول جمله‌ها و حتی اندازهٔ مینی‌بچ ممکن است نامنظم باشد.
همچنین به خاطر تنوع طول توالی‌ها (Sequence Length)، پیاده‌سازی Batch Norm می‌تواند پیچیده شود.

\subsection*{Layer Normalization:}

در Layer Norm، برای هر توکن به‌صورت جداگانه (در طول بُعد ویژگی)، میانگین و واریانس گرفته می‌شود.
فرض کنید در یک لایه، بردار \( h_i \in \mathbb{R}^{d_{\text{model}}} \) مربوط به توکن \( i \) باشد؛ یعنی ابعاد ویژگی آن \( d_{\text{model}} \) است. ما میانگین \( \mu_i \) و واریانس \( \sigma_i^2 \) را از اجزای این بردار محاسبه می‌کنیم:

\[
\mu_i = \frac{1}{d_{\text{model}}} \sum_{k=1}^{d_{\text{model}}} h_{i,k}, \quad
\sigma_i^2 = \frac{1}{d_{\text{model}}} \sum_{k=1}^{d_{\text{model}}} (h_{i,k} - \mu_i)^2
\]

سپس نرمال‌سازی برای هر مؤلفهٔ \( k \) در بردار توکن \( i \) به شکل زیر انجام می‌شود:

\[
\hat{h}_{i,k} = \frac{h_{i,k} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
\]

در نهایت، برای این‌که مدل بتواند مقیاس و بایاس جدیدی یاد بگیرد، شبیه Batch Norm، دو پارامتر \( \gamma \) (Scale) و \( \beta \) (Bias) نیز در طول بعد ویژگی اعمال می‌شوند:

\[
\text{LayerNorm}(h_i) = \gamma \odot \hat{h}_i + \beta
\]

که در آن \( \gamma, \beta \in \mathbb{R}^{d_{\text{model}}} \) هستند و \( \odot \) ضرب عنصر به عنصر است.

\subsection*{مزایای Layer Normalization در ترانسفورمر}

\begin{itemize}
	\item \textbf{بی‌نیازی از وابستگی به ابعاد مینی‌بچ:}  
	با Layer Norm، می‌توان حتی با اندازهٔ مینی‌بچ برابر ۱ نیز به‌خوبی آموزش دید، چراکه آمارها وابسته به ابعاد ویژگی‌اند و نه مینی‌بچ.
	
	\item \textbf{پایدارسازی توزیع فعال‌سازی‌ها:}  
	زمانی که مدل در حال یادگیری است، توزیع‌های داخلی لایه‌های میانی ممکن است تغییر کند (پدیدهٔ Internal Covariate Shift). Layer Norm با نرمال‌سازی این توزیع، آموزش را پایدارتر و سریع‌تر می‌کند.
	
	\item \textbf{سازگاری با داده‌های توالی‌محور:}  
	هر توکن را جداگانه نرمال می‌کند و نگرانی‌ای بابت ترتیب طول جمله‌ها، یا قرار گرفتن چند جملهٔ کوتاه/بلند در یک مینی‌بچ نداریم.
\end{itemize}





در معماری ترانسفورمر، پس از خروجیِ هر زیرماژول (مثل Attention یا MLP)، مراحل به‌شکل زیر است:

\textbf{Residual Connection:} ابتدا ورودی همان زیرماژول (مثلاً بردار \( x \)) را با خروجی زیرماژول (\( \text{SubLayer}(x) \)) جمع می‌کنیم. حاصل این جمع را می‌توان چنین نوشت:

\[
z = x + \text{SubLayer}(x)
\]

این \( z \) حالا ترکیبی از اطلاعات اصلی ورودی و اطلاعات یادگرفته‌شده توسط SubLayer است.

\textbf{Layer Normalization:} سپس این بردار \( z \) را وارد لایهٔ \text{LayerNorm} می‌کنیم:

\[
y = \text{LayerNorm}(z)
\]

خروجی نهایی را می‌توان به لایهٔ بعدی پاس داد یا به مرحلهٔ بعدی در همین لایه.

به‌عبارتی اگر بخواهیم در یک فرمول واحد بیان کنیم:

\[
\text{Add \& Norm} = \text{LayerNorm}\left(x + \text{SubLayer}(x)\right)
\]














