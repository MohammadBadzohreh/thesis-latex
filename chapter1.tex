\chapter{مفاهیم اولیه}
در این فصل به معرفی مقدمات و مفاهیم مورد نیاز در این پایان‌نامه می‌پردازیم. 
\section{مقدمه}
در این بخش به تاریخچه هوش مصنوعی، دستاورد های اولیه، چالش ها، دلایل رکود هوش مصنوعی و پایان عصر تاریک هوش مصنوعی صحبت میکنیم 

% update the abstract


\subsection{آغاز هوش مصنوعی و هدف اصلی}

هوش مصنوعی به عنوان شاخه ای از علوم کامپیوتر، در دهه 1950 با هدف ساخت  سیستم ها و ماشین هایی که توانایی تقلد از هوش انسانی را دارند، آغاز شد.
نخستین بار مکارتی در سال 1956 این اصطلاخ را به کار گرفت. و هوش مصنوعی به عنوان علمی که در آن به مطالعه الگوریتم هایی برای تقلید رفتار انسانی می پردازد، شناخته شد.
اهداف اولیه هوش مصنوعی شامل توانایی درک زبان، یادگیری، حل مسئله و تولید موجودات هوشمند بود.
در این دوران پروژه های تحقیقاتی زیادی به امید دستیابی به هوش مصنوعی عمومی 
 AGI (Artificial General Intelligence)
شروع به کار کردند

\subsection{دورهٔ طلایی و پیشرفت‌های اولیه}
در دهه 50 و60 میلادی، هوش مصنوعی به عنوان یکی از پرچمداران پژوهش های نوین شناخته می شد.
الگوریتم های اولیه به کمک روش های منطقی و ریاضیاتی برای حل مسئله و بازی های ساده توسعه یافتند مانند انواع الگوریتم جستوجوی درختی که در این دوره به وجود آمدند و زمینه ساز اولین دستاوردهای هوش مصنوعی در بازی های تخته ای همچون شطرنج شدند.
در این دوران پیشرفت های بیشتری در پردازش زبان طبیعی (NLP)
و سیستم های خبره (Expert Systems) نیز صورت گرفت که این امید را در دانشمندان و محققان تقویت کرد که دستیابی به هوش مصنوعی عمومی به زودی ممکن خواهد بود.

\subsection{انتظارات بیش از حد و ظهور عصر تاریک}
با وجود پیشرفت های های هوش مصنوعی، محدودیت های تکنولوژی مثل gpu ها  و محاسباتی در آن زمان و همچنین کمبود داده های کافی برای آموزش مدل های پیچیده تر، باعث شد که بسیاری از پروژه های تحقیقاتی نتوانند به نتایج پیش بینی شده دست یابند. و در نتیجه، هوش مصنوعی در دهه 70 به مرحله ای از رکود وارد شد که به آن عصر تاریک هوش مصنوعی یا (AI Winter) می گویند.
در این دوران بسیاری از پروژه ها تعطیل و سرمایه گذاری ها قطع شدند و دولت ها و سازمان های سرمایه گذار به دلیل عدم دستیابی به نتایج مطلوب از ادامه سرمایه گذاری منصرف شدند.

\subsection{عوامل اصلی عصر تاریک هوش مصنوعی}
\begin{itemize}
	\item \textbf{محدودیت‌های سخت‌افزاری:}
در آن زمان، سیستم‌های اولیه هوش مصنوعی  به محاسبات سنگینی نیاز داشتند که با توان پردازشی محدود آن زمان همخوانی نداشت.
	
	\item \textbf{کمبود داده‌ها:}
 در آن زمان، دسترسی به داده‌های کافی برای آموزش مدل‌های پیچیده ممکن نبود و الگوریتم‌های موجود به داده‌های بیشتری نیاز داشتند تا بتوانند به درستی آموزش ببینند و عملکرد مطلوبی داشته باشند.
	
	\item \textbf{روش‌های محدود یادگیری:} 
الگوریتم‌های اولیه به شدت به برنامه‌ریزی انسانی وابسته بودند و در بسیاری از موارد، مدل‌ها قادر به تعمیم به مسائل جدید نبودند و نمی توانستند تعمیم پذیری خیلی بالایی داشته باشند. \cite{russell2016artificial}.
	\end{itemize}

\subsection{پایان عصر تاریک و بازگشت هوش مصنوعی}
پس از چندین سال رکود و عدم سرمایه گذاری در حوزه هوش مصنوعی، سرانجام در دهه 1980 و 1990  عصر تاریک هوش مصنوعی با تحولات تکنولوژی و از همه مهم تر ظهور سیستم های خبره
(Expert-Systems)
به پایان رسید.
سیستم های خبره به عنوان یکی از اولین تلاش های موفق برای کاربردهای صنعتی در هوش مصنوعی به وجود آمدند.بر خلاف الگوریتم های اولیه، این سیستم هااز پایگاه بزرگ قواعد و قوانین 
(Rule-Based-Systems) 
استفاده میکردند. 
در سیستم های خبره به جای تلاش برای شبیه سازی کلی هوش مصنوعی، بر حل مسائل تخصصی برای صنایع و سازمان ها تمرکز میکردند. برای مثال، سیستم های خبره در پزشکی برای تشخیص بیماری ها و پیشنهاد درمان، در صنعت برای مدیریت و پیش بینی خرابی ماشین آلات، و در امور مالی برای تحلیل و ارزیابی ریسک کاربرد داشتند.
هر چند این سیستم ها نمی توانستند درک عمیق و هوشمندی عمومی را ایجاد کنند. اما برای رفع نیاز های پیچیده مناسب بودند.
همزمان با موفقیت این سیستم ها، بهبودهای زیادی در سخت افزارها و کاهش هزینه های پردازش به وجود آمد. در دهه های 1980  و 1990 کامپیوتر ها به تدریج قوی تر و مقرون به رفه تر شدند و امکان پردازش داده های بیشتر و اجرای الگوریتم های پیچیده تر فراهم شد.
این افزایش توان محاسباتی، نیاز به پردازش داده های بزرگ و پیچیده را فراهم کرد و در نتیجه دسترسی به داده ها و انجام محاسبات سنگین برای توسعه الگوریتم های جدید فراهم شد.
از طرف دیگر، پیشرفت های انجام شده در ذخیره سازی داده و رشد اینترنت باعث دسترسی گسترده تر به داده ها و منابع اطلاعاتی شد.
به این ترتیب، مجموعه ای از عوامل شامل ظهور سیستم های خبره، افزایش قدرت پردازش و دسترسی به داده های بیشتر، منجر به بازگشت هوش مصنوعی شد و این دوره نه تنها پایان عصر هوش مصنوعی بود، بلکه راه را برای الگوریتم های یادگیری ماشین و توسعه شبکه های عصبی هموار کرد. \cite{(McCorduck, 2004; Russell & Norvig, 2016).}


\section{انواع مدل یادگیری ماشین و شبکه های عصبی}

\subsection{یادگیری ماشین: مروری کلی}

یادگیری ماشین (Machine-Learning) شاخه‌ای از هوش مصنوعی است که به مدل های محاسباتی این امکان را می دهد الگو ها  از داده ها را به طور خودکار یاد بگیرند و بتوانند تصمیم گیری کنند در واقع، هدف یادگیری ماشین این است که مدل ها بتوانند  از داده ها الگو ها و روابط پنهان را استخراج کنند و به نتایج و رفتار و تصمیم های قابل اعتماد دست یابند.
\subsection{تقسیم‌بندی‌های اصلی در یادگیری ماشین}

یادگیری ماشین به سه دستهٔ اصلی تقسیم می‌شود:
یادگیری با نظارت(Supervised-Learning)
یادگیری بدون نظارت (Unsupervised-Learning)
یادگیری تقویتی (Reinforcement-Learning)

\subsection{یادگیری نظارت شده (Supervised-Learning)}
یادگیری نظارت شده یکی از رایج ترین روش ها در یادگیری ماشین شناخته می شود. که در آن از مجموعه داده های برچسب گذاری شده برای آموزش مدل استفاده می کنیم.
هدف این الگوریتم تشخیص الگو ها در میان داده های ورودی است که این امکان را می دهد  پیش بینی یا طبقه بندی هایی روی داده جدید انجام دهد.
این نوع شامل دو الگوریتم Regression  و classification  می شود.

\subsubsection{طبقه‌بندی (Classification)}

طبقه بندی یکی از مهم ترین و اصلی ترین وظایف در یادگیری نظارت شده است که هدف آن تخصیص داده ها به یک لیبل مشخص است. در این روش مدل با داده های برچسب دار (label) آموزش میبیند و یاد میگیرد که داده های جدید را بر اساس الگو ها و ویژگی هایی که در داده های آموزشی دیده است، به دسته مناسب اختصاص دهد. از کاربرد های طبقه بندی می توان به تشخصیص اسپم (spam)، تشخیص بیماری که آیا یک فرد مبتلا به بیماری هست یا نه، تشخصیص چهره و ...  استفاده کرد.
\subsubsection{رگرسیون (regression)}

رگرسیون یکی از مهم ترین وظایف یادگیری ماشین است و هدف آن پیش بینی مقادیر پیوسته است.
بر حلاف طبقه بندی که خروجی آن دسته بندی مجزا است، در رگرسیون، خروجی یک مقدار پیوسته است و مدل یاد میگیرد روابط بین متغیر های مستقل و متغیر های هدف را شناسایی کند.
از کاربرد های رگرسیون میتوان به پیش بینی قیمت مسکن، پیش بینی اب و هوا و ... اشاره کرد.
 \subsubsection{یادگیری تقویتی (Reinforcement-Learning)}
 
یادگیری تقویتی نوعی یادگیری بر پایه پاداش و تنبیه است که در آن، مدل با محیط تعامل میکند و بر اساس پاداش یا تنبیه یاد میگیرد.
برخلاف یادگیری نظارت‌شده و بدون نظارت، یادگیری تقویتی به مدل این امکان را می دهد، که از طریق آزمون و خطا بهترین راهکارها را برای انجام یک عمل یاد بگیرد.
در این روش، مدل به جای برچسب، از یک تابع پاداش استفاده میکند که مشخص می کند چه اقداماتی باعث نتیجه بهینه میشود. از کاربرد های یادگیری تقویتی میتوان به بازی ها (games)، کنترل رباتیک (robotic -control) , سیستم های توصیه گر (Recommender-Systems) نام برد.


\subsection{معرفی چند مدل از الگوریتم یادگیری کلاسیک }

\subsubsection{نزدیک ترین همسایه(k-nearest-Neighbors)}

KNN  یکی از الگوریتم های ساده  و کار آمد در یادگیری نظارت شده است  که هم در دسته بندی و هم در رگرسیون کاربرد دارد این الگوریتم برای پیش بینی دسته بندی های یک نمونه جدید، به k نزدیک ترین داده ها در فضای ویژگی نگاه میکند و بر اساس اکثریت نزدیک ترین همسایه دسته بندی را انجام میدهد
\subsubsection{مزایا:}

\begin{itemize}
	
	    \item \textbf{سادگی و قابل فهم بودن}: این الگوریتم به سادگی با اندازه‌گیری فاصله بین نقاط داده کار می‌کند و بدون نیاز به آموزش مدل پیچیده قابل استفاده است.
	\item \textbf{عملکرد خوب در داده‌های با تعداد ویژگی کم}:
	در مسائلی که تعداد ویژگی ها کم است این الگوریتم به خوبی کار میکند.
	\end{itemize}

\subsubsection{معایب:}

\begin{itemize}
	\item \textbf{حساسیت به داده‌های پرت}: نقاط پرت می‌توانند به‌طور قابل توجهی بر نتایج تاثیر بگذارند.
	\item \textbf{کندی در داده‌های بزرگ}: این الگوریتم نیاز به محاسبه فاصله برای هر نقطه جدید دارد که در داده‌های بزرگ بار محاسباتی سنگین می‌شود.
	\item \textbf{عدم کارایی در داده‌های با ابعاد بالا}: در داده‌های با تعداد ویژگی‌های زیاد، کارایی الگوریتم کاهش می‌یابد.
\end{itemize}


\subsection{ماشین بردار پشتیبان:(Support-Vector-Machine)}
الگوریتمی است که با یافتن یک ابر صفحه بهینه، داده ها را به کلاس مختلف تقسیم میکند.
این  الگوریتم یک ابر صفحه به دست می آورد که هدف آن  حداکثر کردن فاصله میان داده های دو کلاس است و به این ترتیب میتواند طبقه بندی دقیفی داشته باشد.

\subsubsection{مزایا:}
\begin{itemize}
	
	    \item \textbf{توانایی مقابله با داده‌های پیچیده و ابعاد بالا}: SVM می‌تواند به خوبی با داده‌های چندبعدی و پیچیده کار کند.
	\item \textbf{مقاومت در برابر بیش‌براش (Overfitting)}: با استفاده از هسته‌ها (kernels) می‌توان داده‌های غیرخطی را نیز به فضای بالاتر برد و جداسازی بهتری انجام داد.
	
\end{itemize}

\subsubsection{معایب:}
\begin{itemize}
	\item \textbf{پیچیدگی محاسباتی}: آموزش SVM به دلیل نیاز به حل مسائل بهینه‌سازی، در حجم‌های بالای داده محاسباتی زمان‌بر است.
	\item \textbf{کارایی پایین در داده‌های پرت}: در صورتی که داده‌ها شامل نقاط پرت زیادی باشند، دقت مدل کاهش می‌یابد.
\end{itemize}

\subsection{ بیز ساده (Naive-Bayes):}
بیز ساده مبتنی بر قضیه بیز است و فرض میکند ویژگی ها به صورت شرطی مستقل از هم هستند. این مدل برای اولین بار در حوزه پردازش متن به کار رفت و هنوز هم در بسیاری از موارد مانند طبقه بندی ایمیل و تحلیل احساسات مورد استفاده قرار میگیرد.
نایو بیز بر اساس احتمالات محاسبه میکند که یک نمونه جدید به کدام دسته تعلق دارد. این الگوریتم بر اساس قضیه بیز، احتمال تعلق یک نمونه ب دسته را به ازای هر ویژگی محاسبه کرده و بیشترین احتمال را بر اساس جواب نهایی در نظر میگبرد.
\subsubsection{مزایا:}

\begin{itemize}
	\item \textbf{سرعت بالا}: به دلیل محاسبات ساده و فرض استقلال ویژگی‌ها، Naive Bayes بسیار سریع و کم‌حجم است.
	\item \textbf{کارایی در داده‌های کوچک}: حتی با داده‌های کم، این الگوریتم عملکرد نسبتاً خوبی دارد.
\end{itemize}

\subsection{معایب:}

\begin{itemize}
	\item \textbf{فرض استقلال ویژگی‌ها}: فرض استقلال ویژگی‌ها ممکن است در بسیاری از مسائل واقعی صادق نباشد و این می‌تواند دقت مدل را کاهش دهد.
	\item \textbf{حساسیت به داده‌های نادرست}: در صورت داده‌های نادرست یا پرت، مدل ممکن است دقت کمتری داشته باشد.
\end{itemize}

\subsection{شبکه‌های عصبی بازگشتی (RNN) و شبکه‌های حافظه بلند مدت کوتاه‌مدت (LSTM)}
شبکه عصبی بازگشتی با RNN  ها و مدل هایی با حافظه بلند مدت تر مانند Lstm  با هدف داده های ترتیبی و وابسته به زمان توسعه یافتند. 
این مدل ها به ویژه در تحلیل زبان طبیعی، صوت و پیش بینی سری های زمانی بسیار موفق عمل کرده اند.
زیرا قادر به حفظ اطلاعات گذشته بودند و از این اطلاعات گذشته برای پیش بینی در لحظه حال و آینده استفاده میکنند.

\subsection{RNN:}
مدل های اولیه شبکه های عصبی، مانند شبکه های چند لایه (Mlp)، قادر به پردازش داده های مستقل و ثابت بودند و نمتوانستند وابستگی های زمانی را یاد بگیرند.
در بسیاری از مباحث دنیای واقعی، مانند تحلیل متن و صدا، داده ها به ترتیب خاصی وابسته هستند. به همین دلیل شبکه های Rnn  معرفی شدند تا از  اطلاعات پیشین در پردازش  داده های بعدی استفاده کنند.

\subsubsection{ساختار و عملکرد RNN:}
شبکه های Rnn داری حلقه بازگشتی هستند که به مدل این امکان را می دهد اطلاعات را در توالی نگه دارند. و در هر گام زمانی ورودی فعلی \( x_t \) و وضعیت قبلی \( h_{t-1} \) به عنوان ورودی به نورون داده می‌شود.


\begin{equation}
	h_t = \sigma(W \cdot x_t + U \cdot h_{t-1} + b)
\end{equation}

در اینجا:

$h_t$ وضعیت مخفی یا حالت در گام زمانی $t$ است.

$W$ وزن‌هایی است که به ورودی $x_t$ اعمال می‌شود.

$U$ وزن‌های اعمال‌شده بر وضعیت قبلی $h_{t-1}$ است.

$b$ بایاس مدل است.

$\sigma$ تابع فعال‌سازی، معمولاً تانژانت هیپربولیک یا سیگموید.

با استفاده از این فرایند، مدل این توانایی را دارد که اطلاعات گذشته را در خود ذخیره کرده و در پردازش های بعدی آن ها را به کار بگیرد.





\subsection{مزایا و معایب Rnn:}
در این قسمت به مزایا و معایب RNN میپردازیم.



\subsubsection{مزایا:}


\begin{itemize}
	\item \textbf{حفظ وابستگی زمانی:} RNN قادر به پردازش توالی‌های طولانی است و می‌تواند اطلاعات را در طول توالی به خاطر بسپارد.
	
	\item \textbf{کاربردهای گسترده در داده‌های ترتیبی:} این مدل در تحلیل زبان طبیعی، پیش‌بینی سری‌های زمانی و پردازش صوتی بسیار موفق عمل می‌کند.
	
\end{itemize}


\subsubsection{معایب:}


\begin{itemize}
	\item \textbf{مشکل ناپدید شدن و انفجار گرادیان (Vanishing and Exploding Gradient):} در فرآیند آموزش با روش پس‌انتشار، اگر توالی داده طولانی باشد، گرادیان‌ها ممکن است به سرعت کوچک یا بزرگ شوند، که منجر به ناپایداری آموزش و کاهش دقت می‌شود.
	
	\item \textbf{محدودیت در پردازش توالی‌های بسیار بلند:} RNN در حفظ اطلاعات طولانی‌مدت دچار مشکل است و برای پردازش وابستگی‌های طولانی، عملکرد ضعیفی دارد.
	
\end{itemize}

\subsection{شبکه‌های حافظه بلندمدت-کوتاه‌مدت (LSTM):}

\subsubsection{علل پیدایش lstm:}
شبکه های lstm  به عنوان یک راه حل برای یکی از بزرگترین مشکلات شبکه های عصبی بازگشتی (RNN) ها معرفی شدند.
یکی از بزرگترین مشکلاتی که در شبکه های بازگشتی وجود داشت مشکل ناپدید شدن گرادیان (Vanishing Gradient) بود. این مشکل باعث میشد RNN  ها قادر به یادگیری وابستگی های بلند مدت نباشند. برای درک عمیق تر ابتدا به توضیح مشکل ناپدید شدن گرادیان و سپس حل آن توسط lstm میپردازیم.

\subsubsection{ Gradient Vanishing:}


شبکه‌های RNN برای پردازش داده‌های ترتیبی، از حالت‌های بازگشتی استفاده می‌کنند. در فرآیند آموزش RNN، از الگوریتم پس‌انتشار خطا از طریق زمان (Backpropagation Through Time - BPTT) استفاده می‌شود. این الگوریتم گرادیان‌ها را برای به‌روزرسانی وزن‌ها محاسبه می‌کند. با این حال، به دلایل زیر، RNNها در یادگیری وابستگی‌های بلندمدت ناکام می‌مانند:

\begin{itemize}
	\item \textbf{ضریب‌های بازگشتی کوچک‌تر از ۱:} در فرآیند محاسبه گرادیان‌ها، اگر مقدار مشتقات یا ضرایب در هر مرحله کوچک‌تر از ۱ باشد، ضرب مکرر این مقادیر در طول توالی منجر به کوچک شدن گرادیان‌ها به سمت صفر می‌شود. این پدیده، ناپدید شدن گرادیان نام دارد.
\end{itemize}

فرمول کلی گرادیان در زمان \( t \) به صورت زیر است:

\[
\frac{\partial L}{\partial W} = \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}} \cdot \frac{\partial h_t}{\partial L}
\]

در اینجا، \( \frac{\partial h_k}{\partial h_{k-1}} \) می‌تواند مقداری کوچک‌تر از ۱ باشد، و ضرب مکرر این مشتقات باعث کاهش شدید مقدار گرادیان می‌شود.

\begin{itemize}
	\item \textbf{تاثیر مستقیم بر وزن‌ها:} زمانی که گرادیان‌ها نزدیک به صفر شوند، وزن‌های مدل به‌طور موثری به‌روزرسانی نمی‌شوند. این امر مانع از یادگیری وابستگی‌های طولانی‌مدت در داده‌ها می‌شود.
\end{itemize}

\subsection{ظهور Lstm:}
در سال ۱۹۹۷، Sepp Hochreiter و Jürgen Schmidhuber شبکه‌های حافظه بلندمدت-کوتاه‌مدت (LSTM) را معرفی کردند. انگیزه اصلی توسعه LSTM مشکل ناپدید شدن گرادیان در شبکه های RNN بود. این مشکل در مسائل یادگیری داده‌های ترتیبی طولانی باعث می‌شد RNN نتواند وابستگی‌های بلندمدت را به درستی یاد بگیرد.


\subsubsection{راه‌حل LSTM برای پایداری جریان گرادیان‌ها:}

LSTM با معرفی یک معماری جدید در شبکه‌های بازگشتی، جریان گرادیان‌ها را در طول توالی پایدار نگه می‌دارد. این کار از طریق اضافه کردن وضعیت سلولی (Cell State) و دروازه‌ها (Gates) به ساختار RNN انجام می‌شود. این اجزا به LSTM امکان می‌دهند که:


\begin{enumerate}
	\item اطلاعات غیرضروری را فراموش کند.
	\item اطلاعات مهم جدید را اضافه کند.
	\item اطلاعات مهم قبلی را حفظ کند.
\end{enumerate}


\section{اختار LSTM: نوآوری در مقایسه با RNN}

LSTM شامل اجزای جدیدی است که به آن امکان مدیریت بهتر اطلاعات را می‌دهد:

\subsection{وضعیت سلولی (Cell State):}

مسیر اصلی ذخیره اطلاعات در LSTM است که می‌تواند اطلاعات مهم را در طول توالی حفظ کند. برخلاف RNN که وابسته به خروجی‌های بازگشتی \( h_t \) است، LSTM یک مسیر جداگانه برای عبور اطلاعات از وضعیت سلولی دارد که به حفظ گرادیان‌ها کمک می‌کند.

\subsection{دروازه‌ها (Gates):}
دروازه‌ها نقش فیلترهای اطلاعاتی را دارند که جریان اطلاعات را در طول فرآیند یادگیری کنترل می‌کنند:

\begin{itemize}
	\item \textbf{دروازه فراموشی (Forget Gate):} تعیین می‌کند چه اطلاعاتی از وضعیت سلولی باید حذف شود.
	\[
	f_t = \sigma \left( W_f \cdot \left[ h_{t-1}, x_t \right] + b_f \right)
	\]
	\[
	f_t: \text{مقدار فراموشی برای هر عنصر از وضعیت سلولی.}
	\]
	\[
	\sigma: \text{تابع سیگموید که خروجی آن بین ۰ و ۱ است.}
	\]
	وقتی \( f_t = 0 \)، اطلاعات به طور کامل حذف می‌شود؛ وقتی \( f_t = 1 \)، اطلاعات حفظ می‌شود.
	
	\item \textbf{دروازه ورودی (Input Gate):} تعیین می‌کند چه اطلاعات جدیدی باید به وضعیت سلولی اضافه شود.
	\[
	i_t = \sigma \left( W_i \cdot \left[ h_{t-1}, x_t \right] + b_i \right)
	\]
	\[
	\tilde{C}_t = \tanh \left( W_C \cdot \left[ h_{t-1}, x_t \right] + b_C \right)
	\]
	\[
	i_t: \text{میزان اطلاعات جدیدی که به وضعیت سلولی وارد می‌شود.}
	\]
	\[
	\tilde{C}_t: \text{مقدار جدید محاسبه شده برای اضافه شدن به وضعیت سلولی.}
	\]
	
	\item \textbf{دروازه خروجی (Output Gate):} تعیین می‌کند چه اطلاعاتی از وضعیت سلولی به خروجی منتقل شود.
	\[
	o_t = \sigma \left( W_o \cdot \left[ h_{t-1}, x_t \right] + b_o \right)
	\]
	\[
	h_t = o_t \cdot \tanh(C_t)
	\]
\end{itemize}

\subsection{به‌روزرسانی وضعیت سلولی:}

وضعیت سلولی \( C_t \) با استفاده از اطلاعات جدید و قدیمی به‌روزرسانی می‌شود:
\[
C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
\]
این ساختار باعث می‌شود که اطلاعات قدیمی مهم حفظ شده و اطلاعات غیرضروری حذف شوند.

\subsubsection{علت پایداری گرادیان در Lstm:}

\begin{itemize}
	\item \textbf{حذف ضرب‌های مکرر:} برخلاف RNN که به ضرب‌های مکرر وزن‌ها و گرادیان‌ها وابسته است، LSTM با مسیر جداگانه وضعیت سلولی، از کاهش نمایی گرادیان جلوگیری می‌کند.
	
	\item \textbf{استفاده از توابع سیگموید و تانژانت هیپربولیک:} توابع سیگموید در دروازه‌ها و تانژانت هیپربولیک در وضعیت سلولی باعث محدود کردن مقادیر و جلوگیری از انفجار گرادیان می‌شوند.
	
	\item \textbf{مدیریت اطلاعات توسط دروازه‌ها:} دروازه‌های فراموشی و ورودی به مدل اجازه می‌دهند تنها اطلاعات مهم حفظ شود و داده‌های غیرضروری حذف شوند، که این موضوع از پیچیدگی‌های محاسباتی غیرضروری جلوگیری می‌کند.
\end{itemize}

%\subsection{مقایسه RNN  و LStm:}

\iffalse
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{ویژگی} & \textbf{RNN} & \textbf{LSTM} \\ \hline
		\textbf{مشکل ناپدید شدن گرادیان} & وجود دارد & برطرف شده \\ \hline
		\textbf{توانایی حفظ وابستگی‌های طولانی‌مدت} & محدود به وابستگی کوتاه‌مدت & بسیار خوب \\ \hline
		\textbf{ساختار دروازه‌ها} & ندارد & دارای دروازه‌های فراموشی، ورودی و خروجی \\ \hline
		\textbf{پایداری گرادیان} & ضعیف & پایدار \\ \hline
	\end{tabular}
	\caption{مقایسه ویژگی‌های RNN و LSTM}
\end{table}
\fi


\subsection{مشکلات کلی rnn  و lstm  و ظهور ترانسفومرها}
شبکه های بازگشتی rnn  , lstm  ها توانستند بسیازی از مشکلات و محدودیت های مدل های اولیه را حل کنند اما همچنان با جالش ها و محدودیت هایی مواجهه بودند که در مسائل پیچیده تر، مانند ترجمه زبان یا تحلیل داده های بلند مدت و حجیم، مشکلاتی را ایجاد میکردند. این مشکلات در نهایت باعث پیدایش ترانسفورمر ها شد. که در اینجا مشکلات آن ها را بررسی میکنیم 

\subsubsection{ مشکل وابستگی ترتیبی در RNNها و LSTMها}

RNNها و LSTMها داده‌ها را به ترتیب پردازش می‌کنند، به این معنی که برای پردازش داده‌های گام زمانی \( t \)، باید تمامی داده‌های قبلی (\( t-1 \)) را پردازش کرده باشند. این ویژگی مشکلات زیر را ایجاد می‌کند:

\begin{itemize}
	\item \textbf{غیرقابل موازی‌سازی:} به دلیل وابستگی ترتیبی، پردازش داده‌ها به صورت موازی ممکن نیست، که باعث افزایش زمان محاسباتی می‌شود. این مشکل در داده‌های بلند، مانند متن‌های طولانی یا سری‌های زمانی بزرگ، قابل توجه است.
	\item \textbf{کندی آموزش و استنتاج:} ترتیب خطی باعث می‌شود که زمان آموزش و پیش‌بینی مدل‌ها به شدت افزایش یابد، به ویژه زمانی که با حجم زیادی از داده‌ها سر و کار داریم.
\end{itemize}


\subsubsection{ محدودیت در یادگیری وابستگی‌های بسیار طولانی}
با وجود پیشرفت LSTM در یادگیری وابستگی‌های بلندمدت نسبت به RNNها، این مدل‌ها همچنان در یادگیری وابستگی‌های بسیار بلند، مانند ارتباطات بین کلمات در دو جمله متفاوت یا درک ساختار کلی یک متن، محدودیت دارند:


\begin{itemize}
	\item \textbf{مشکل در داده‌های بسیار طولانی:} حتی در LSTM، ظرفیت حفظ اطلاعات محدود است و با افزایش طول توالی، دقت مدل کاهش می‌یابد.
	\item \textbf{تاثیر تدریجی داده‌های اولیه:} داده‌های ابتدایی توالی ممکن است با گذشت زمان اهمیت خود را از دست بدهند، زیرا گرادیان‌ها به تدریج ضعیف‌تر می‌شوند.
\end{itemize}


\subsubsection{پیچیدگی محاسباتی و حافظه}

LSTMها به دلیل ساختار پیچیده‌ای که شامل چندین ماتریس ضرب (برای دروازه‌های فراموشی، ورودی و خروجی) و به‌روزرسانی وضعیت سلول است، نیاز به حافظه و محاسبات زیادی دارند:


\begin{itemize}
	\item \textbf{نیاز به حافظه بیشتر:} برای ذخیره وضعیت سلولی و گرادیان‌ها، به حافظه بیشتری نسبت به مدل‌های ساده‌تر نیاز است.
	\item \textbf{هزینه محاسباتی بالا:} به خصوص در داده‌های بزرگ، محاسبات سنگین باعث کندی اجرای مدل‌ها می‌شود.
\end{itemize}


\subsubsection{مشکل پردازش وابستگی‌های غیرمتوالی}

RNNها و LSTMها به طور طبیعی برای یادگیری وابستگی‌های محلی و متوالی مناسب هستند. با این حال، در مسائل پیچیده مانند ترجمه زبان یا تحلیل متون، وابستگی‌های غیرمحلی و غیرمتوالی نیز وجود دارند. به عنوان مثال:

در یک جمله طولانی، ممکن است کلمه‌ای در ابتدای جمله با کلمه‌ای در انتهای جمله ارتباط معنایی داشته باشد. RNNها و LSTMها برای یادگیری این نوع وابستگی‌ها به شدت محدود هستند.


\subsubsection{گرادیان‌های ناپایدار و مشکلات بهینه‌سازی:}

با وجود بهبودهایی که LSTM نسبت به RNN در پایداری گرادیان‌ها ارائه داد، هنوز هم:
\begin{itemize}
	\item \textbf{مسائل گرادیان‌های ناپایدار:} در توالی‌های بسیار بلند، گرادیان‌ها ممکن است همچنان کاهش یابند یا حتی در برخی موارد به حد انفجار برسند.
	\item \textbf{مشکلات بهینه‌سازی:} در مسائل پیچیده‌تر، یافتن مینیمم مناسب تابع هزینه با استفاده از RNNها و LSTMها دشوار است.
\end{itemize}

\subsubsection{نیاز به مدلی با ظرفیت بیشتر و سرعت بالاتر:}

\begin{itemize}
	\item \textbf{مدل‌های بزرگ‌تر:} برای مسائل پیچیده‌تر، مدل‌هایی با تعداد پارامتر بیشتر نیاز است که RNNها و LSTMها به دلیل محدودیت در حافظه و پردازش، پاسخگوی این نیاز نیستند.
	\item \textbf{کارایی در داده‌های چندوجهی (Multimodal):} برای داده‌هایی که شامل اطلاعات متنی، صوتی و تصویری هستند، RNNها و LSTMها توانایی لازم برای هم‌زمان پردازش این اطلاعات را ندارند.
\end{itemize}
	
وابستگی ترتیبی در RNN  و LSTM  یک مانع اساسی در برای استفاده از این مدل ها در مسئل پیچیده و بزرگ بود. که باعصث به وحود آمدن ترانسفورمر ها شد. 
که با طراحی مبتنی بر  موازی سازی و ماکانیزم توجه،، این محدودیت را حذف کردند و راه حلی کارآمد تر برای پردازش داده های ترتیبی ارائه دادند.

	



