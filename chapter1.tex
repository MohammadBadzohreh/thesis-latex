\chapter{مفاهیم اولیه}
در این فصل به معرفی مقدمات و مفاهیم مورد نیاز در این پایان‌نامه می‌پردازیم. 
\section{مقدمه}
در این بخش به تاریخچه هوش مصنوعی، دستاورد های اولیه، چالش ها، دلایل رکود هوش مصنوعی و پایان عصر تاریک هوش مصنوعی صحبت میکنیم 

% update the abstract


\subsection{آغاز هوش مصنوعی و هدف اصلی}

هوش مصنوعی به عنوان شاخه ای از علوم کامپیوتر، در دهه 1950 با هدف ساخت  سیستم ها و ماشین هایی که توانایی تقلد از هوش انسانی را دارند، آغاز شد.
نخستین بار مکارتی در سال 1956 این اصطلاخ را به کار گرفت. و هوش مصنوعی به عنوان علمی که در آن به مطالعه الگوریتم هایی برای تقلید رفتار انسانی می پردازد، شناخته شد.
اهداف اولیه هوش مصنوعی شامل توانایی درک زبان، یادگیری، حل مسئله و تولید موجودات هوشمند بود.
در این دوران پروژه های تحقیقاتی زیادی به امید دستیابی به هوش مصنوعی عمومی 
 AGI (Artificial General Intelligence)
شروع به کار کردند

\subsection{دورهٔ طلایی و پیشرفت‌های اولیه}
در دهه 50 و60 میلادی، هوش مصنوعی به عنوان یکی از پرچمداران پژوهش های نوین شناخته می شد.
الگوریتم های اولیه به کمک روش های منطقی و ریاضیاتی برای حل مسئله و بازی های ساده توسعه یافتند مانند انواع الگوریتم جستوجوی درختی که در این دوره به وجود آمدند و زمینه ساز اولین دستاوردهای هوش مصنوعی در بازی های تخته ای همچون شطرنج شدند.
در این دوران پیشرفت های بیشتری در پردازش زبان طبیعی (NLP)
و سیستم های خبره (Expert Systems) نیز صورت گرفت که این امید را در دانشمندان و محققان تقویت کرد که دستیابی به هوش مصنوعی عمومی به زودی ممکن خواهد بود.

\subsection{انتظارات بیش از حد و ظهور عصر تاریک}
با وجود پیشرفت های های هوش مصنوعی، محدودیت های تکنولوژی مثل gpu ها  و محاسباتی در آن زمان و همچنین کمبود داده های کافی برای آموزش مدل های پیچیده تر، باعث شد که بسیاری از پروژه های تحقیقاتی نتوانند به نتایج پیش بینی شده دست یابند. و در نتیجه، هوش مصنوعی در دهه 70 به مرحله ای از رکود وارد شد که به آن عصر تاریک هوش مصنوعی یا (AI Winter) می گویند.
در این دوران بسیاری از پروژه ها تعطیل و سرمایه گذاری ها قطع شدند و دولت ها و سازمان های سرمایه گذار به دلیل عدم دستیابی به نتایج مطلوب از ادامه سرمایه گذاری منصرف شدند.

\subsection{عوامل اصلی عصر تاریک هوش مصنوعی}
\begin{itemize}
	\item \textbf{محدودیت‌های سخت‌افزاری:}
در آن زمان، سیستم‌های اولیه هوش مصنوعی  به محاسبات سنگینی نیاز داشتند که با توان پردازشی محدود آن زمان همخوانی نداشت.
	
	\item \textbf{کمبود داده‌ها:}
 در آن زمان، دسترسی به داده‌های کافی برای آموزش مدل‌های پیچیده ممکن نبود و الگوریتم‌های موجود به داده‌های بیشتری نیاز داشتند تا بتوانند به درستی آموزش ببینند و عملکرد مطلوبی داشته باشند.
	
	\item \textbf{روش‌های محدود یادگیری:} 
الگوریتم‌های اولیه به شدت به برنامه‌ریزی انسانی وابسته بودند و در بسیاری از موارد، مدل‌ها قادر به تعمیم به مسائل جدید نبودند و نمی توانستند تعمیم پذیری خیلی بالایی داشته باشند. \cite{russell2016artificial}.
	\end{itemize}

\subsection{پایان عصر تاریک و بازگشت هوش مصنوعی}
پس از چندین سال رکود و عدم سرمایه گذاری در حوزه هوش مصنوعی، سرانجام در دهه 1980 و 1990  عصر تاریک هوش مصنوعی با تحولات تکنولوژی و از همه مهم تر ظهور سیستم های خبره
(Expert-Systems)
به پایان رسید.
سیستم های خبره به عنوان یکی از اولین تلاش های موفق برای کاربردهای صنعتی در هوش مصنوعی به وجود آمدند.بر خلاف الگوریتم های اولیه، این سیستم هااز پایگاه بزرگ قواعد و قوانین 
(Rule-Based-Systems) 
استفاده میکردند. 
در سیستم های خبره به جای تلاش برای شبیه سازی کلی هوش مصنوعی، بر حل مسائل تخصصی برای صنایع و سازمان ها تمرکز میکردند. برای مثال، سیستم های خبره در پزشکی برای تشخیص بیماری ها و پیشنهاد درمان، در صنعت برای مدیریت و پیش بینی خرابی ماشین آلات، و در امور مالی برای تحلیل و ارزیابی ریسک کاربرد داشتند.
هر چند این سیستم ها نمی توانستند درک عمیق و هوشمندی عمومی را ایجاد کنند. اما برای رفع نیاز های پیچیده مناسب بودند.
همزمان با موفقیت این سیستم ها، بهبودهای زیادی در سخت افزارها و کاهش هزینه های پردازش به وجود آمد. در دهه های 1980  و 1990 کامپیوتر ها به تدریج قوی تر و مقرون به رفه تر شدند و امکان پردازش داده های بیشتر و اجرای الگوریتم های پیچیده تر فراهم شد.
این افزایش توان محاسباتی، نیاز به پردازش داده های بزرگ و پیچیده را فراهم کرد و در نتیجه دسترسی به داده ها و انجام محاسبات سنگین برای توسعه الگوریتم های جدید فراهم شد.
از طرف دیگر، پیشرفت های انجام شده در ذخیره سازی داده و رشد اینترنت باعث دسترسی گسترده تر به داده ها و منابع اطلاعاتی شد.
به این ترتیب، مجموعه ای از عوامل شامل ظهور سیستم های خبره، افزایش قدرت پردازش و دسترسی به داده های بیشتر، منجر به بازگشت هوش مصنوعی شد و این دوره نه تنها پایان عصر هوش مصنوعی بود، بلکه راه را برای الگوریتم های یادگیری ماشین و توسعه شبکه های عصبی هموار کرد. \cite{(McCorduck, 2004; Russell & Norvig, 2016).}


\section{انواع مدل یادگیری ماشین و شبکه های عصبی}

\subsection{یادگیری ماشین: مروری کلی}

یادگیری ماشین (Machine-Learning) شاخه‌ای از هوش مصنوعی است که به مدل های محاسباتی این امکان را می دهد الگو ها  از داده ها را به طور خودکار یاد بگیرند و بتوانند تصمیم گیری کنند در واقع، هدف یادگیری ماشین این است که مدل ها بتوانند  از داده ها الگو ها و روابط پنهان را استخراج کنند و به نتایج و رفتار و تصمیم های قابل اعتماد دست یابند.
\subsection{تقسیم‌بندی‌های اصلی در یادگیری ماشین}

یادگیری ماشین به سه دستهٔ اصلی تقسیم می‌شود:
یادگیری با نظارت(Supervised-Learning)
یادگیری بدون نظارت (Unsupervised-Learning)
یادگیری تقویتی (Reinforcement-Learning)

\subsection{یادگیری نظارت شده (Supervised-Learning)}
یادگیری نظارت شده یکی از رایج ترین روش ها در یادگیری ماشین شناخته می شود. که در آن از مجموعه داده های برچسب گذاری شده برای آموزش مدل استفاده می کنیم.
هدف این الگوریتم تشخیص الگو ها در میان داده های ورودی است که این امکان را می دهد  پیش بینی یا طبقه بندی هایی روی داده جدید انجام دهد.
این نوع شامل دو الگوریتم Regression  و classification  می شود.

\subsubsection{طبقه‌بندی (Classification)}

طبقه بندی یکی از مهم ترین و اصلی ترین وظایف در یادگیری نظارت شده است که هدف آن تخصیص داده ها به یک لیبل مشخص است. در این روش مدل با داده های برچسب دار (label) آموزش میبیند و یاد میگیرد که داده های جدید را بر اساس الگو ها و ویژگی هایی که در داده های آموزشی دیده است، به دسته مناسب اختصاص دهد. از کاربرد های طبقه بندی می توان به تشخصیص اسپم (spam)، تشخیص بیماری که آیا یک فرد مبتلا به بیماری هست یا نه، تشخصیص چهره و ...  استفاده کرد.
\subsubsection{رگرسیون (regression)}

رگرسیون یکی از مهم ترین وظایف یادگیری ماشین است و هدف آن پیش بینی مقادیر پیوسته است.
بر حلاف طبقه بندی که خروجی آن دسته بندی مجزا است، در رگرسیون، خروجی یک مقدار پیوسته است و مدل یاد میگیرد روابط بین متغیر های مستقل و متغیر های هدف را شناسایی کند.
از کاربرد های رگرسیون میتوان به پیش بینی قیمت مسکن، پیش بینی اب و هوا و ... اشاره کرد.
 \subsubsection{یادگیری تقویتی (Reinforcement-Learning)}
 
یادگیری تقویتی نوعی یادگیری بر پایه پاداش و تنبیه است که در آن، مدل با محیط تعامل میکند و بر اساس پاداش یا تنبیه یاد میگیرد.
برخلاف یادگیری نظارت‌شده و بدون نظارت، یادگیری تقویتی به مدل این امکان را می دهد، که از طریق آزمون و خطا بهترین راهکارها را برای انجام یک عمل یاد بگیرد.
در این روش، مدل به جای برچسب، از یک تابع پاداش استفاده میکند که مشخص می کند چه اقداماتی باعث نتیجه بهینه میشود. از کاربرد های یادگیری تقویتی میتوان به بازی ها (games)، کنترل رباتیک (robotic -control) , سیستم های توصیه گر (Recommender-Systems) نام برد.


\subsection{معرفی چند مدل از الگوریتم یادگیری کلاسیک }

\subsubsection{نزدیک ترین همسایه(k-nearest-Neighbors)}

KNN  یکی از الگوریتم های ساده  و کار آمد در یادگیری نظارت شده است  که هم در دسته بندی و هم در رگرسیون کاربرد دارد این الگوریتم برای پیش بینی دسته بندی های یک نمونه جدید، به k نزدیک ترین داده ها در فضای ویژگی نگاه میکند و بر اساس اکثریت نزدیک ترین همسایه دسته بندی را انجام میدهد
\subsubsection{مزایا:}

\begin{itemize}
	
	    \item \textbf{سادگی و قابل فهم بودن}: این الگوریتم به سادگی با اندازه‌گیری فاصله بین نقاط داده کار می‌کند و بدون نیاز به آموزش مدل پیچیده قابل استفاده است.
	\item \textbf{عملکرد خوب در داده‌های با تعداد ویژگی کم}:
	در مسائلی که تعداد ویژگی ها کم است این الگوریتم به خوبی کار میکند.
	\end{itemize}

\subsubsection{معایب:}

\begin{itemize}
	\item \textbf{حساسیت به داده‌های پرت}: نقاط پرت می‌توانند به‌طور قابل توجهی بر نتایج تاثیر بگذارند.
	\item \textbf{کندی در داده‌های بزرگ}: این الگوریتم نیاز به محاسبه فاصله برای هر نقطه جدید دارد که در داده‌های بزرگ بار محاسباتی سنگین می‌شود.
	\item \textbf{عدم کارایی در داده‌های با ابعاد بالا}: در داده‌های با تعداد ویژگی‌های زیاد، کارایی الگوریتم کاهش می‌یابد.
\end{itemize}


\subsection{ماشین بردار پشتیبان:(Support-Vector-Machine)}
الگوریتمی است که با یافتن یک ابر صفحه بهینه، داده ها را به کلاس مختلف تقسیم میکند.
این  الگوریتم یک ابر صفحه به دست می آورد که هدف آن  حداکثر کردن فاصله میان داده های دو کلاس است و به این ترتیب میتواند طبقه بندی دقیفی داشته باشد.

\subsubsection{مزایا:}
\begin{itemize}
	
	    \item \textbf{توانایی مقابله با داده‌های پیچیده و ابعاد بالا}: SVM می‌تواند به خوبی با داده‌های چندبعدی و پیچیده کار کند.
	\item \textbf{مقاومت در برابر بیش‌براش (Overfitting)}: با استفاده از هسته‌ها (kernels) می‌توان داده‌های غیرخطی را نیز به فضای بالاتر برد و جداسازی بهتری انجام داد.
	
\end{itemize}

\subsubsection{معایب:}
\begin{itemize}
	\item \textbf{پیچیدگی محاسباتی}: آموزش SVM به دلیل نیاز به حل مسائل بهینه‌سازی، در حجم‌های بالای داده محاسباتی زمان‌بر است.
	\item \textbf{کارایی پایین در داده‌های پرت}: در صورتی که داده‌ها شامل نقاط پرت زیادی باشند، دقت مدل کاهش می‌یابد.
\end{itemize}

\subsection{ بیز ساده (Naive-Bayes):}
بیز ساده مبتنی بر قضیه بیز است و فرض میکند ویژگی ها به صورت شرطی مستقل از هم هستند. این مدل برای اولین بار در حوزه پردازش متن به کار رفت و هنوز هم در بسیاری از موارد مانند طبقه بندی ایمیل و تحلیل احساسات مورد استفاده قرار میگیرد.
نایو بیز بر اساس احتمالات محاسبه میکند که یک نمونه جدید به کدام دسته تعلق دارد. این الگوریتم بر اساس قضیه بیز، احتمال تعلق یک نمونه ب دسته را به ازای هر ویژگی محاسبه کرده و بیشترین احتمال را بر اساس جواب نهایی در نظر میگبرد.
\subsubsection{مزایا:}

\begin{itemize}
	\item \textbf{سرعت بالا}: به دلیل محاسبات ساده و فرض استقلال ویژگی‌ها، Naive Bayes بسیار سریع و کم‌حجم است.
	\item \textbf{کارایی در داده‌های کوچک}: حتی با داده‌های کم، این الگوریتم عملکرد نسبتاً خوبی دارد.
\end{itemize}

\subsection{معایب:}

\begin{itemize}
	\item \textbf{فرض استقلال ویژگی‌ها}: فرض استقلال ویژگی‌ها ممکن است در بسیاری از مسائل واقعی صادق نباشد و این می‌تواند دقت مدل را کاهش دهد.
	\item \textbf{حساسیت به داده‌های نادرست}: در صورت داده‌های نادرست یا پرت، مدل ممکن است دقت کمتری داشته باشد.
\end{itemize}

\subsection{شبکه‌های عصبی بازگشتی (RNN) و شبکه‌های حافظه بلند مدت کوتاه‌مدت (LSTM)}
شبکه عصبی بازگشتی با RNN  ها و مدل هایی با حافظه بلند مدت تر مانند Lstm  با هدف داده های ترتیبی و وابسته به زمان توسعه یافتند. 
این مدل ها به ویژه در تحلیل زبان طبیعی، صوت و پیش بینی سری های زمانی بسیار موفق عمل کرده اند.
زیرا قادر به حفظ اطلاعات گذشته بودند و از این اطلاعات گذشته برای پیش بینی در لحظه حال و آینده استفاده میکنند.

\subsection{RNN:}
مدل های اولیه شبکه های عصبی، مانند شبکه های چند لایه (Mlp)، قادر به پردازش داده های مستقل و ثابت بودند و نمتوانستند وابستگی های زمانی را یاد بگیرند.
در بسیاری از مباحث دنیای واقعی، مانند تحلیل متن و صدا، داده ها به ترتیب خاصی وابسته هستند. به همین دلیل شبکه های Rnn  معرفی شدند تا از  اطلاعات پیشین در پردازش  داده های بعدی استفاده کنند.

\subsubsection{ساختار و عملکرد RNN:}
شبکه های Rnn داری حلقه بازگشتی هستند که به مدل این امکان را می دهد اطلاعات را در توالی نگه دارند. و در هر گام زمانی ورودی فعلی \( x_t \) و وضعیت قبلی \( h_{t-1} \) به عنوان ورودی به نورون داده می‌شود.


\begin{equation}
	h_t = \sigma(W \cdot x_t + U \cdot h_{t-1} + b)
\end{equation}

در اینجا:

$h_t$ وضعیت مخفی یا حالت در گام زمانی $t$ است.

$W$ وزن‌هایی است که به ورودی $x_t$ اعمال می‌شود.

$U$ وزن‌های اعمال‌شده بر وضعیت قبلی $h_{t-1}$ است.

$b$ بایاس مدل است.

$\sigma$ تابع فعال‌سازی، معمولاً تانژانت هیپربولیک یا سیگموید.

با استفاده از این فرایند، مدل این توانایی را دارد که اطلاعات گذشته را در خود ذخیره کرده و در پردازش های بعدی آن ها را به کار بگیرد.





\subsection{مزایا و معایب Rnn:}
در این قسمت به مزایا و معایب RNN میپردازیم.



\subsubsection{مزایا:}


\begin{itemize}
	\item \textbf{حفظ وابستگی زمانی:} RNN قادر به پردازش توالی‌های طولانی است و می‌تواند اطلاعات را در طول توالی به خاطر بسپارد.
	
	\item \textbf{کاربردهای گسترده در داده‌های ترتیبی:} این مدل در تحلیل زبان طبیعی، پیش‌بینی سری‌های زمانی و پردازش صوتی بسیار موفق عمل می‌کند.
	
\end{itemize}


\subsubsection{معایب:}


\begin{itemize}
	\item \textbf{مشکل ناپدید شدن و انفجار گرادیان (Vanishing and Exploding Gradient):} در فرآیند آموزش با روش پس‌انتشار، اگر توالی داده طولانی باشد، گرادیان‌ها ممکن است به سرعت کوچک یا بزرگ شوند، که منجر به ناپایداری آموزش و کاهش دقت می‌شود.
	
	\item \textbf{محدودیت در پردازش توالی‌های بسیار بلند:} RNN در حفظ اطلاعات طولانی‌مدت دچار مشکل است و برای پردازش وابستگی‌های طولانی، عملکرد ضعیفی دارد.
	
\end{itemize}


\end{document}


